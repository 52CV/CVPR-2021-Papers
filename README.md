# CVPR2021æœ€æ–°ä¿¡æ¯åŠå·²æ¥æ”¶è®ºæ–‡/ä»£ç (æŒç»­æ›´æ–°)


æœ¬è´´æ˜¯å¯¹ CVPR2021 å·²æ¥å—è®ºæ–‡çš„ç²—ç•¥æ±‡æ€»ï¼ŒåæœŸä¼šæœ‰æ›´è¯¦ç»†çš„æ€»ç»“ã€‚æœŸå¾…ing......

å®˜ç½‘é“¾æ¥ï¼šhttp://cvpr2021.thecvf.com<br>
å¼€ä¼šæ—¶é—´ï¼š2021å¹´6æœˆ19æ—¥-6æœˆ25æ—¥<br>
è®ºæ–‡æ¥æ”¶å…¬å¸ƒæ—¶é—´ï¼š2021å¹´2æœˆ28æ—¥<br>

æ¥æ”¶è®ºæ–‡IDsï¼š<br>

* [CVPR 2021 æ¥æ”¶è®ºæ–‡åˆ—è¡¨ï¼27%æ¥å—ç‡ï¼](https://zhuanlan.zhihu.com/p/353686917)

## ğŸ“—ğŸ“—ğŸ“—ä¸‹è½½å·²å…¬å¼€è®ºæ–‡åœ¨ã€æˆ‘çˆ±è®¡ç®—æœºè§†è§‰ã€‘åå°å›å¤â€œCVPR22021â€ï¼Œå³å¯æ”¶åˆ°ã€‚ç›®å‰å·²å…¬å¼€ 495 ç¯‡ã€‚

# CVPR2021æœ€æ–°ä¿¡æ¯åŠå·²æ¥æ”¶è®ºæ–‡/ä»£ç (æŒç»­æ›´æ–°)

### :fireworks::fireworks::fireworks:æ›´æ–°æç¤ºï¼š4æœˆ6æ—¥æ–°å¢27ç¯‡(3åˆ†å‰²+1å…‰æµ+2å›¾åƒè´¨é‡+1åŒ»å­¦+1æ‰‹è¯­+2äººè„¸+1å§¿æ€+1è§†é¢‘+3å°æ ·æœ¬/åŸŸé€‚åº”+2ç›®æ ‡æ£€æµ‹+1åŠ¨ä½œæ£€æµ‹+1æ·±åº¦å­¦ä¹ +2è¯­éŸ³å¤„ç†+1é£æ ¼è¿ç§»+1è‡ªåŠ¨é©¾é©¶+2OCR+1NAS+1æœªåˆ†)

* åˆ†å‰²
  * [Fully Convolutional Networks for Panoptic Segmentation](https://arxiv.org/abs/2012.00720)<br>:open_mouth:oral:star:[code](https://github.com/yanwei-li/PanopticFCN)<br>ç²—è§£ï¼š[11](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
  * [Adaptive Prototype Learning and Allocation for Few-Shot Segmentation](https://arxiv.org/abs/2104.01893)<br>:star:[code](https://github.com/Reagan1311/ASGNet)
  * [Weakly-supervised Instance Segmentation via Class-agnostic Learning with Salient Images](https://arxiv.org/abs/2104.01526)<br>
* å…‰æµ
  * [UPFlow:Upsampling Pyramid for Unsupervised Optical Flow Learning](https://arxiv.org/abs/2012.00212)<br>ç²—è§£ï¼š[8](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* å›¾åƒè´¨é‡
  * [NBNet: Noise Basis Learning for Image Denoising with Subspace Projection](https://arxiv.org/abs/2012.15028)<br>ç²—è§£ï¼š[9](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
  * [Towards Rolling Shutter Correction and Deblurring in Dynamic Scenes](https://arxiv.org/abs/2104.01601)<br>:star:[code](https://github.com/zzh-tech/RSCD)
* åŒ»å­¦
  * [DARCNN: Domain Adaptive Region-based Convolutional Neural Network for Unsupervised Instance Segmentation in Biomedical Images](https://arxiv.org/abs/2104.01325)<br>
* æ‰‹è¯­
  * [Fingerspelling Detection in American Sign Language](https://arxiv.org/abs/2104.01291)
* äººè„¸
  * Practical Wide-Angle Portraits Correction with Deep Structured Models<br>ç²—è§£ï¼š[7](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
  * [HLA-Face: Joint High-Low Adaptation for Low Light Face Detection](https://arxiv.org/abs/2104.01984)<br>:house:[project](https://daooshee.github.io/HLA-Face-Website/)
* å§¿æ€
  * [Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation](https://arxiv.org/abs/2012.15175)<br>:star:[code](https://github.com/greatlog/SWAHR-HumanPose)<br>ç²—è§£ï¼š[2](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* è§†é¢‘
  * [MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection](https://arxiv.org/abs/2104.01633)
* å°æ ·æœ¬/åŸŸé€‚åº”
  * [Generalized Few-Shot Object Detection without Forgetting]<br>ç²—è§£ï¼š[16](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
  * [Instance Level Affinity-Based Transfer for Unsupervised Domain Adaptation](https://arxiv.org/abs/2104.01286)<br>:star:[code](https://github.com/astuti/ILA-DA)
  * [Unsupervised Multi-source Domain Adaptation Without Access to Source Data](https://arxiv.org/abs/2104.01845)
* ç›®æ ‡æ£€æµ‹
  * [Points as Queries: Weakly Semi-supervised Object Detection by Points]<br>ç²—è§£ï¼š[6](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
  * [IQDet: Instance-wise Quality Distribution Sampling for Object Detection]<br>ç²—è§£ï¼š[20](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* åŠ¨ä½œæ£€æµ‹
  * [Beyond Short Clips: End-to-End Video-Level Learning with Collaborative Memories](https://arxiv.org/abs/2104.01198)
* æ·±åº¦å­¦ä¹ 
  * [Activate or Not: Learning Customized Activation](https://arxiv.org/abs/2009.04759)<br>ç²—è§£ï¼š[4](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* è¯­éŸ³å¤„ç†
  * [Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation](https://arxiv.org/abs/2104.02026)<br>:star:[code](https://github.com/YapengTian/CCOL-CVPR21)
  * [Can audio-visual integration strengthen robustness under multimodal attacks?](https://arxiv.org/abs/2104.02000)<br>:star:[code](https://github.com/YapengTian/AV-Robustness-CVPR21)
* é£æ ¼è¿ç§»
  * [Lipstick ain't enough: Beyond Color Matching for In-the-Wild Makeup Transfer](https://arxiv.org/abs/2104.01867)
* è‡ªåŠ¨é©¾é©¶
  * [SGCN:Sparse Graph Convolution Network for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2104.01528)<br>:star:[code](https://github.com/shuaishiliu/SGCN)
* NAS
  * [Neural Architecture Search with Random Labels](https://arxiv.org/abs/2101.11834)<br>ç²—è§£ï¼š[1](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* OCR
  * [MetaHTR: Towards Writer-Adaptive Handwritten Text Recognition](https://arxiv.org/abs/2104.01876)
  * [Scene Text Retrieval via Joint Text Detection and Similarity Learning](https://arxiv.org/abs/2104.01552)<br>:star:[code](https://github.com/lanfeng4659/STR-TDSL)
* æœªåˆ†
  * [Dynamic Region-Aware Convolution](https://arxiv.org/abs/2003.12243)<br>ç²—è§£ï¼š[14](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)

:fireworks::fireworks::fireworks:æ›´æ–°æç¤ºï¼š4æœˆ5æ—¥æ–°å¢15ç¯‡(1äººè„¸+2ä¸‰ç»´+1é‡åŒ–+1åˆ†å‰²+1ç‚¹äº‘+1æ— äººæœº+1åŸŸé€‚åº”+1è§†é¢‘ç†è§£+3ç›®æ ‡æ£€æµ‹+1è¿åŠ¨é¢„æµ‹+1OCR+1æœªåˆ†)
* äººè„¸
  * [Towards High Fidelity Face Relighting with Realistic Shadows](https://arxiv.org/abs/2104.00825)<br>:star:[code](https://github.com/andrewhou1/Shadow-Mask-Face-Relighting)
* ä¸‰ç»´
  * [S2R-DepthNet: Learning a Generalizable Depth-specific Structural Representation](https://arxiv.org/abs/2104.00877)<br>:open_mouth:oral
  * [Fully Understanding Generic Objects: Modeling, Segmentation, and Reconstruction](https://arxiv.org/abs/2104.00858)
* é‡åŒ–
  * [Network Quantization with Element-wise Gradient Scaling](https://arxiv.org/abs/2104.00903)<br>:house:[project](https://cvlab.yonsei.ac.kr/projects/EWGS/)
* åˆ†å‰²
  * [Background-Aware Pooling and Noise-Aware Loss for Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2104.00905)<br>:house:[project](https://cvlab.yonsei.ac.kr/projects/BANA/)
* ç‚¹äº‘
  * [FESTA: Flow Estimation via Spatial-Temporal Attention for Scene Point Clouds](https://arxiv.org/abs/2104.00798)
* æ— äººæœº
  * [UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles](https://arxiv.org/abs/2104.00946)
* åŸŸé€‚åº”
  * [Curriculum Graph Co-Teaching for Multi-Target Domain Adaptation](https://arxiv.org/abs/2104.00808)<br>:star:[code](https://github.com/Evgeneus/Graph-Domain-Adaptaion):house:[project](https://roysubhankar.github.io/graph-coteaching-adaptation/)
* è§†é¢‘ç†è§£
  * [Visual Semantic Role Labeling for Video Understanding](https://arxiv.org/abs/2104.00990)<br>:house:[project](https://vidsitu.org/)
* ç›®æ ‡æ£€æµ‹
  * [Group Collaborative Learning for Co-Salient Object Detection](https://arxiv.org/abs/2104.01108)<br>:star:[code](https://github.com/fanq15/GCoNet)
  * [HVPR: Hybrid Voxel-Point Representation for Single-stage 3D Object Detection](https://arxiv.org/abs/2104.00902)
  * [Adaptive Class Suppression Loss for Long-Tail Object Detection](https://arxiv.org/abs/2104.00885)<br>:star:[code](https://github.com/CASIA-IVA-Lab/ACSL)
* è¿åŠ¨é¢„æµ‹
  * [Video Prediction Recalling Long-term Motion Context via Memory Alignment Learning](https://arxiv.org/abs/2104.00924)<br>:open_mouth:oral:star:[code](https://github.com/sangmin-git/LMC-Memory)
* OCR
  * [MOST: A Multi-Oriented Scene Text Detector with Localization Refinement](https://arxiv.org/abs/2104.01070)
* æœªåˆ†
  * [Self-supervised Video Representation Learning by Context and Motion Decoupling](https://arxiv.org/abs/2104.00862)

# ç›®å½•

|:cat:|:dog:|:mouse:|:hamster:|:tiger:|
|------|------|------|------|------|
|[Workshopå¾ç¨¿](#*)|[67.å…‰æµä¼°è®¡](#67)|[66.é£æ ¼è¿ç§»](#66)|
|[65.è¯­éŸ³å¤„ç†](#65)|[64.VL](#64)[63.å›¾åƒå¤„ç†](#63)|[62.æ‰‹ç»˜è‰å›¾è¯†åˆ«](#62)|[61.ç®—æ³•](#61)|
|[60. SLAM/AR/æœºå™¨äºº](#60)|[59.æ·±åº¦å­¦ä¹ æ¨¡å‹](#59)|[58.åº¦é‡å­¦ä¹ ](#58)|[57.æ‰‹è¯­è¯†åˆ«](#57)|[56.å…‰å­¦ã€å‡ ä½•ã€å…‰åœºæˆåƒ](#56)|
|[55.å›¾åŒ¹é…](#55)|[54.æƒ…æ„Ÿé¢„æµ‹](#54)|[53.æ•°æ®é›†](#53)|[52.å›¾åƒ/è§†é¢‘ç”Ÿæˆ](#52)|[51.å¯¹æ¯”å­¦ä¹ ](#51)|
|[50.OCR](#50)|[49.å¯¹æŠ—å­¦ä¹ ](#49)|[48.å›¾åƒè¡¨ç¤º](#48)|[47.è§†è§‰è¯­è¨€å¯¼èˆªVLN](#47)|[46.äººç‰©äº¤äº’HOI](#46)|
|[45.ç›¸æœºå®šä½](#45)|[44.å›¾åƒ/è§†é¢‘å­—å¹•](#44)|[43.ä¸»åŠ¨å­¦ä¹ ](#43)|[42.åŠ¨ä½œé¢„æµ‹](#42)|[41.è¡¨ç¤ºå­¦ä¹ ï¼ˆå›¾åƒ+å­—å¹•ï¼‰](#41)|
|[40.è¶…åƒç´ ](#40)|[39.æ¨¡å‹åè§æ¶ˆé™¤](#39)|[38.ç±»å¢é‡å­¦ä¹ ](#38)|[37.æŒç»­å­¦ä¹ ](#37)|[36.åŠ¨ä½œæ£€æµ‹ä¸è¯†åˆ«](#36)|
|[35.å›¾åƒèšç±»](#35)|[34.å›¾åƒ/ç»†ç²’åº¦åˆ†ç±»](#34)|[33.6Dä½å§¿ä¼°è®¡](#33)|[32.è§†å›¾åˆæˆ](#32)|[31. å¼€æ”¾é›†è¯†åˆ«](#31)|
|[30.æ–°è§†è§’åˆæˆ](#30)|[29.å§¿æ€ä¼°è®¡](#29)|[28.å¯†é›†é¢„æµ‹](#28)|[27.æ´»ä½“æ£€æµ‹](#27)|[26.è§†é¢‘ç›¸å…³æŠ€æœ¯](#26)|
|[25.ä¸‰ç»´è§†è§‰](#25)|[24.å¼ºåŒ–å­¦ä¹ ](#24)|[23.è‡ªåŠ¨é©¾é©¶](#23)|[22.åŒ»å­¦å½±åƒ](#22)|[21.Transformer/Self-attention](#21)|
|[20.äººå‘˜é‡è¯†åˆ«/äººç¾¤è®¡æ•°](#20)|[19.é‡åŒ–ã€å‰ªæã€è’¸é¦ã€æ¨¡å‹å‹ç¼©ä¸ä¼˜åŒ–](#19)|[18.èˆªç©ºå½±åƒ](#18)|[17.è¶…åˆ†è¾¨ç‡](#17)|[16.è§†è§‰é—®ç­”](#16)|
|[15.GAN](#15)|[14.å°/é›¶æ ·æœ¬å­¦ä¹ ï¼ŒåŸŸé€‚åº”ï¼ŒåŸŸæ³›åŒ–](#14)|[13.å›¾åƒæ£€ç´¢](#13)|[12.å›¾åƒå¢å¹¿](#12)|[11.äººè„¸æŠ€æœ¯](#11)|
|[10.ç¥ç»æ¶æ„æœç´¢](#10)|[9.ç›®æ ‡è·Ÿè¸ª](#9)|[8.å›¾åƒåˆ†å‰²](#8)|[7.ç›®æ ‡æ£€æµ‹](#7)|[6.æ•°æ®å¢å¼º](#6)|
|[5.å¼‚å¸¸æ£€æµ‹](#5)|[4.è‡ª/åŠ/å¼±ç›‘ç£å­¦ä¹ ](#4)|[3.ç‚¹äº‘](#3)|[2.å›¾å·ç§¯ç½‘ç»œGNN](#2)|[1.æœªåˆ†ç±»](#1)|



<a name="67"/>

## 67.å…‰æµä¼°è®¡
  * [UPFlow:Upsampling Pyramid for Unsupervised Optical Flow Learning](https://arxiv.org/abs/2012.00212)<br>ç²—è§£ï¼š[8](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
 

<a name="66"/>

## 66.é£æ ¼è¿ç§»
  * [Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes](https://arxiv.org/abs/2103.17185)<br>:star:[code](https://github.com/CompVis/brushstroke-parameterized-style-transfer)
  * [ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows](https://arxiv.org/abs/2103.16877)<br>:star:[code](https://github.com/pkuanjie/ArtFlow) 
  * [Lipstick ain't enough: Beyond Color Matching for In-the-Wild Makeup Transfer](https://arxiv.org/abs/2104.01867)

<a name="65"/>

## 65.è¯­éŸ³å¤„ç†
  * [Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation](https://arxiv.org/abs/2104.02026)<br>:star:[code](https://github.com/YapengTian/CCOL-CVPR21)
  * [Can audio-visual integration strengthen robustness under multimodal attacks?](https://arxiv.org/abs/2104.02000)<br>:star:[code](https://github.com/YapengTian/AV-Robustness-CVPR21)

<a name="64"/>

## 64.VL
  * [Kaleido-BERT: Vision-Language Pre-training on Fashion Domain](https://arxiv.org/abs/2103.16110)<br>

<a name="63"/>

## 63.å›¾åƒå¤„ç†
* å›¾åƒä¿¡å·å¤„ç†
  * [Invertible Image Signal Processing](https://arxiv.org/abs/2103.15061)<br>:star:[code](https://github.com/yzxing87/Invertible-ISP):house:[project](https://yzxing87.github.io/InvISP/index.html)
* å…‰è°±é‡å»º
  * [Tuning IR-cut Filter for Illumination-aware Spectral Reconstruction from RGB](https://arxiv.org/abs/2103.14708)<br>:open_mouth:oral

<a name="62"/>

## 62.æ‰‹ç»˜è‰å›¾è¯†åˆ«
  * [Cloud2Curve: Generation and Vectorization of Parametric Sketches](https://arxiv.org/abs/2103.15536)

<a name="61"/>

## 61.ç®—æ³•
* å› æœæ¨ç†ç®—æ³•
  * [ACRE: Abstract Causal REasoning Beyond Covariation](https://arxiv.org/abs/2103.14232)<br>:star:[code](https://github.com/WellyZhang/ACRE):house:[project](http://wellyzhang.github.io/project/acre.html)
* æŠ½è±¡æ—¶ç©ºæ¨ç†ç®—æ³•
  * [Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution](https://arxiv.org/abs/2103.14230)<br>:star:[code](https://github.com/WellyZhang/PrAE):house:[project](http://wellyzhang.github.io/project/prae.html)

<a name="60"/>

## 60. SLAM/AR/æœºå™¨äºº
* [Tangent Space Backpropagation for 3D Transformation Groups](https://arxiv.org/abs/2103.12032)<br>:star:[code](https://github.com/princeton-vl/lietorch)
* è§†è§‰é‡Œç¨‹è®¡
  * [Generalizing to the Open World: Deep Visual Odometry with Online Adaptation](https://arxiv.org/abs/2103.15279)
* æœºå™¨äºº
  * [Visual Room Rearrangement](https://arxiv.org/abs/2103.16544)<br>:open_mouth:oral:house:[project](https://ai2thor.allenai.org/rearrangement/):tv:[video](https://www.youtube.com/watch?v=1APxaOC9U-A)
* AR
  * Stay Positive: Non-Negative Image Synthesis for Augmented Reality<br>:open_mouth:oral
  * è™šæ‹Ÿè¯•ç©¿
    * [VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization](https://arxiv.org/abs/2103.16874)

<a name="59"/>

## 59.æ·±åº¦å­¦ä¹ æ¨¡å‹
  * [Dynamic Slimmable Network](https://arxiv.org/abs/2103.13258)<br>:open_mouth:oral:star:[code](https://github.com/changlin31/DS-Net)
  * [Towards Evaluating and Training Verifiably Robust Neural Networks](https://arxiv.org/abs/2104.00447)<br>:open_mouth:oral:star:[code](https://github.com/ZhaoyangLyu/VerifiablyRobustNN)
  * [Activate or Not: Learning Customized Activation](https://arxiv.org/abs/2009.04759)<br>ç²—è§£ï¼š[4](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)

<a name="58"/>

## 58.åº¦é‡å­¦ä¹ ï¼ˆç›¸ä¼¼åº¦å­¦ä¹ ï¼‰
  * [Dynamic Metric Learning: Towards a Scalable Metric Space to Accommodate Multiple Semantic Scales](https://arxiv.org/abs/2103.11781)<br>:star:[code](https://github.com/SupetZYK/DynamicMetricLearning)
  * [Embedding Transfer with Label Relaxation for Improved Metric Learning](https://arxiv.org/abs/2103.14908)
  * [Noise-resistant Deep Metric Learning with Ranking-based Instance Selection](https://arxiv.org/abs/2103.16047)<br>:star:[code](https://github.com/alibaba-edu/Ranking-based-Instance-Selection)
  
<a name="57"/>

## 57.æ‰‹è¯­è¯†åˆ«
  * [Skeleton Based Sign Language Recognition Using Whole-body Keypoints](https://arxiv.org/abs/2103.08833)<br>:star:[code](https://github.com/jackyjsy/CVPR21Chal-SLR)
  * [Read and Attend: Temporal Localisation in Sign Language Videos](https://arxiv.org/abs/2103.16481)<br>:house:[project](https://www.robots.ox.ac.uk/~vgg/research/bslattend/)
  * [Fingerspelling Detection in American Sign Language](https://arxiv.org/abs/2104.01291)

<a name="56"/>

## 56.å…‰å­¦ã€å‡ ä½•ã€å…‰åœºæˆåƒ
  * [Deep Gaussian Scale Mixture Prior for Spectral Compressive Imaging](https://arxiv.org/abs/2103.07152)<br>:star:[code](https://github.com/TaoHuang95/DGSMP):house:[project](https://see.xidian.edu.cn/faculty/wsdong/Projects/DGSM-SCI.htm)
  * [Mask-ToF: Learning Microlens Masks for Flying Pixel Correction in Time-of-Flight Imaging](https://arxiv.org/abs/2103.16693)<br>:house:[project](https://light.princeton.edu/publication/mask-tof/)
  * [Passive Inter-Photon Imaging](https://arxiv.org/abs/2104.00059)<br>:open_mouth:oral

<a name="55"/>

## 55.å›¾åŒ¹é…
  * [Deep Graph Matching under Quadratic Constraint](https://arxiv.org/abs/2103.06643)

<a name="54"/>

## 54.æƒ…æ„Ÿé¢„æµ‹
  * [Affect2MM: Affective Analysis of Multimedia Content Using Emotion Causality](https://arxiv.org/abs/2103.06541)<br>:house:[project](https://gamma.umd.edu/researchdirections/affectivecomputing/affect2mm/)

<a name="53"/>

## 53.æ•°æ®é›†
  * [Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](https://arxiv.org/abs/2102.08981)<br>:sunflower:[dataset](https://github.com/google-research-datasets/conceptual-12m)
  * [Sewer-ML: A Multi-Label Sewer Defect Classification Dataset and Benchmark](https://arxiv.org/abs/2103.10895)<br>:house:[project](https://vap.aau.dk/sewer-ml/)
  * [Benchmarking Representation Learning for Natural World Image Collections](https://arxiv.org/abs/2103.16483)<br>:sunflower:[dataset](https://github.com/visipedia/newt)

<a name="52"/>

## 52.å›¾åƒ/è§†é¢‘ç”ŸæˆImage Generation

- [Spatially-Adaptive Pixelwise Networks for Fast Image Translation](https://arxiv.org/abs/2012.02992)<br>:house:[project](https://tamarott.github.io/ASAPNet_web/)<br>é‡‡ç”¨è¶…ç½‘ç»œå’Œéšå¼å‡½æ•°ï¼Œæå¿«çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘é€Ÿåº¦ï¼ˆæ¯”åŸºçº¿å¿«18å€ï¼‰
- [Image Generators with Conditionally-Independent Pixel Synthesis](https://arxiv.org/abs/2011.13775)<br>:open_mouth:oral:star:[code](https://github.com/saic-mdal/CIPS)
* [Im2Vec: Synthesizing Vector Graphics without Vector Supervision](https://arxiv.org/abs/2102.02798)<br>:open_mouth:oral:star:[code](https://github.com/preddy5/Im2Vec):house:[project](http://geometry.cs.ucl.ac.uk/projects/2021/im2vec/)
* å›¾åƒç”Ÿæˆ
  * [Context-Aware Layout to Image Generation with Enhanced Object Appearance](https://arxiv.org/abs/2103.11897)<br>:star:[code](https://github.com/wtliao/layout2img) 
* è§†é¢‘ç”Ÿæˆ
  * [Playable Video Generation](https://arxiv.org/abs/2101.12195)<br>:open_mouth:oral:star:[code](https://github.com/willi-menapace/PlayableVideoGeneration):house:[project](https://willi-menapace.github.io/playable-video-generation-website/):tv:[video](https://www.youtube.com/watch?v=QtDjSyZERpg)
  * [One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing](https://arxiv.org/abs/2011.15126)<br>:open_mouth:oral:star:[code](https://github.com/NVlabs/imaginaire):house:[project](https://nvlabs.github.io/face-vid2vid/):tv:[video](https://youtu.be/nLYg9Waw72U)<br>è§£è¯»ï¼š[é¢ è¦†è§†é¢‘å‹ç¼©çš„ä¸ä¸€å®šæ˜¯æ–°å‹ç¼©ç®—æ³•ï¼Œè€Œå¯èƒ½æ˜¯GANï¼è‹±ä¼Ÿè¾¾æ–°ç®—æ³•æœ€é«˜å‹ç¼©90%æµé‡](https://mp.weixin.qq.com/s/UpfgxiIaSU4iIjbrkS--zA)<br>Nvidiaçš„æ–°ç ”ç©¶ï¼Œä½¿ç”¨äººè„¸å…³é”®ç‚¹+GANé‡å»ºè§†é¢‘é€šè¯ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„H.264èŠ‚çœ90%æµé‡ã€‚ä»£ç æœªå¼€æºï¼Œä½†è‹±ä¼Ÿè¾¾çš„GANæ¡†æ¶å¼€æºäº†ã€‚

<a name="51"/>

## 51.å¯¹æ¯”å­¦ä¹ 
* [AdCo: Adversarial Contrast for Efficient Learning of Unsupervised Representations from Self-Trained Negative Adversaries](https://arxiv.org/abs/2011.08435)<br>:star:[code](https://arxiv.org/abs/2011.08435)<br>è§£è¯»:[CVPR 2021æ¥æ”¶è®ºæ–‡ï¼šAdCoåŸºäºå¯¹æŠ—çš„å¯¹æ¯”å­¦ä¹ ](https://mp.weixin.qq.com/s/u7Lhzh8uYEEHfWiM32-4yQ)

<a name="50"/>

## 50.OCR

* åœºæ™¯æ–‡æœ¬æ£€æµ‹
  * [What If We Only Use Real Datasets for Scene Text Recognition? Toward Scene Text Recognition With Fewer Labels](https://arxiv.org/abs/2103.04400)
  * [Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition](https://arxiv.org/abs/2103.06495)<br>:open_mouth:oral:star:[code](https://github.com/FangShancheng/ABINet)
  * [MOST: A Multi-Oriented Scene Text Detector with Localization Refinement](https://arxiv.org/abs/2104.01070)
  * [Scene Text Retrieval via Joint Text Detection and Similarity Learning](https://arxiv.org/abs/2104.01552)<br>:star:[code](https://github.com/lanfeng4659/STR-TDSL)
* æ‰‹å†™æ–‡æœ¬è¯†åˆ«
  * [MetaHTR: Towards Writer-Adaptive Handwritten Text Recognition](https://arxiv.org/abs/2104.01876)

<a name="49"/>

## 49.å¯¹æŠ—å­¦ä¹ 

- [Simulating Unknown Target Models for Query-Efficient Black-box Attacks](https://arxiv.org/abs/2009.00960)<br>:star:[code](https://github.com/machanic/MetaSimulator)<br>é»‘ç›’å¯¹æŠ—æ”»å‡»
- Delving into Data: Effectively Substitute Training for Black-box Attack<br>åŸºäºé«˜æ•ˆè®­ç»ƒæ›¿ä»£æ¨¡å‹çš„é»‘ç›’æ”»å‡»æ–¹æ³•<br>è§£è¯»ï¼š[8](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
- [LiBRe: A Practical Bayesian Approach to Adversarial Detection](https://arxiv.org/abs/2103.14835)<br>:star:[code](https://github.com/thudzj/ScalableBDL)

<a name="48"/>

## 48.å›¾åƒè¡¨ç¤ºImage Representation

- [Learning Continuous Image Representation with Local Implicit Image Function](https://arxiv.org/abs/2012.09161)<br>:open_mouth:oral:star:[code](https://github.com/yinboc/liif):house:[project](https://yinboc.github.io/liif/):tv:[video](https://youtu.be/6f2roieSY_8)

<a name="47"/>

## 47.è§†è§‰è¯­è¨€å¯¼èˆªvision-language navigation

- [Structured Scene Memory for Vision-Language Navigation](https://arxiv.org/abs/2103.03454)

<a name="46"/>

## 46.äººç‰©äº¤äº’ï¼ˆhuman-object interactionï¼‰

- [Learning Asynchronous and Sparse Human-Object Interaction in Videos](https://arxiv.org/abs/2103.02758)
- [QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information](https://arxiv.org/abs/2103.05399)<br>:star:[code](https://github.com/hitachi-rd-cv/qpic)
- [Reformulating HOI Detection as Adaptive Set Prediction](https://arxiv.org/abs/2103.05983)<br>:star:[code](https://github.com/yoyomimi/AS-Net)
* [Detecting Human-Object Interaction via Fabricated Compositional Learning](https://arxiv.org/abs/2103.08214)<br>:star:[code](https://github.com/zhihou7/FCL)

<a name="45"/>

## 45.ç›¸æœºå®šä½(Camera Localization)

- [Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments](https://arxiv.org/abs/2012.04746)<br>:open_mouth:oral
- [Back to the Feature: Learning Robust Camera Localization from Pixels to Pose](https://arxiv.org/abs/2103.09213)<br>:star:[code](https://github.com/cvg/pixloc)
- [Learning Camera Localization via Dense Scene Matching](https://arxiv.org/abs/2103.16792)<br>:star:[code](https://github.com/Tangshitao/Dense-Scene-Matching)

<a name="44"/>

## 44.å›¾åƒ/è§†é¢‘å­—å¹•

- [Scan2Cap: Context-aware Dense Captioning in RGB-D Scans](https://arxiv.org/abs/2012.02206)<br>:star:[code](https://github.com/daveredrum/Scan2Cap):house:[project](https://daveredrum.github.io/Scan2Cap/):tv:[video](https://youtu.be/AgmIpDbwTCY)
- [VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs](https://arxiv.org/abs/2101.12059)<br>è§†é¢‘å­—å¹•ã€è§†é¢‘é—®ç­”å’Œè§†é¢‘å¯¹è¯ä»»åŠ¡çš„å¤šæ¨¡å¼æ¡†æ¶
- [Open-book Video Captioning with Retrieve-Copy-Generate Network](https://arxiv.org/abs/2103.05284)
* å›¾åƒå­—å¹•
  * [Human-like Controllable Image Captioning with Verb-specific Semantic Roles](https://arxiv.org/abs/2103.12204)<br>:star:[code](https://github.com/mad-red/VSR-guided-CIC)

<a name="43"/>

## 43.ä¸»åŠ¨å­¦ä¹ 

- [Vab-AL: Incorporating Class Imbalance and Difficulty with Variational Bayes for Active Learning](https://arxiv.org/abs/2003.11249)

<a name="42"/>

## 42.åŠ¨ä½œé¢„æµ‹

- [Learning the Predictability of the Future](https://arxiv.org/abs/2101.01600)<br>é¢„æµ‹æœªæ¥<br>:star:[code](https://github.com/cvlab-columbia/hyperfuture):house:[project](https://hyperfuture.cs.columbia.edu/):tv:[video](https://www.youtube.com/watch?v=-Uy92jvT_90)<br>
* [Video Prediction Recalling Long-term Motion Context via Memory Alignment Learning](https://arxiv.org/abs/2104.00924)<br>:open_mouth:oral:star:[code](https://github.com/sangmin-git/LMC-Memory)

<a name="41"/>

## 41.è¡¨ç¤ºå­¦ä¹ ï¼ˆå›¾åƒ+å­—å¹•ï¼‰

- [VirTex: Learning Visual Representations from Textual Annotations](https://arxiv.org/abs/2006.06666)<br>:star:[code](https://github.com/kdexd/virtex)

<a name="40"/>

## 40.è¶…åƒç´ 

- [Learning the Superpixel in a Non-iterative and Lifelong Manner](https://arxiv.org/abs/2103.10681)<br>è®ºæ–‡å…¬å¼€

<a name="39"/>

## 39.æ¨¡å‹åè§æ¶ˆé™¤

- [Fair Attribute Classification through Latent Space De-biasing](https://arxiv.org/abs/2012.01469)<br>:star:[code](https://github.com/princetonvisualai/gan-debiasing):house:[project](https://princetonvisualai.github.io/gan-debiasing/)<br>

<a name="38"/>

## 38.ç±»å¢é‡å­¦ä¹ ï¼ˆclass-incremental learningï¼‰

- [IIRC: Incremental Implicitly-Refined Classification](https://arxiv.org/abs/2012.12477)<br>:house:[project](https://chandar-lab.github.io/IIRC/)<br>
- [Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2103.04059)
- [DER: Dynamically Expandable Representation for Class Incremental Learning](https://arxiv.org/abs/2103.16788)<br>:star:[code](https://github.com/Rhyssiyan/DER-ClassIL.pytorch)

<a name="37"/>

## 37.æŒç»­å­¦ä¹ 

- Rainbow Memory: Continual Learning with a Memory of Diverse Samples<br>
- [Training Networks in Null Space for Continual Learning]()<br>:open_mouth:oral:star:[code](https://github.com/ShipengWang/Adam-NSCL)
* [Efficient Feature Transformations for Discriminative and Generative Continual Learning](https://arxiv.org/abs/2103.13558)
* [Rainbow Memory: Continual Learning with a Memory of Diverse Samples](https://arxiv.org/abs/2103.17230) 
* [Rectification-based Knowledge Retention for Continual Learning](https://arxiv.org/abs/2103.16597) 

<a name="36"/>

## 36.åŠ¨ä½œæ£€æµ‹ä¸è¯†åˆ«

- [Coarse-Fine Networks for Temporal Activity Detection in Videos](https://arxiv.org/abs/2103.01302)<br>
- [3D CNNs with Adaptive Temporal Feature Resolutions](https://arxiv.org/abs/2011.08652)<br> 
- [Understanding the Robustness of Skeleton-based Action Recognition under Adversarial Attack](https://arxiv.org/abs/2103.05347)
- [BASAR:Black-box Attack on Skeletal Action Recognition](https://arxiv.org/abs/2103.05266)<br>:tv:[video](https://www.youtube.com/watch?v=PjWgwnAkV8g)
- [TDN: Temporal Difference Networks for Efficient Action Recognition]( https://arxiv.org/abs/2012.10071)<br>:star:[code](https://github.com/MCG-NJU/TDN)
- [ACTION-Net: Multipath Excitation for Action Recognition](https://arxiv.org/abs/2103.07372)<br>:star:[code](https://github.com/V-Sense/ACTION-Net)<br>è§£è¯»ï¼š[CVPR 2021 | ç”¨äºåŠ¨ä½œè¯†åˆ«ï¼Œå³æ’å³ç”¨ã€æ··åˆæ³¨æ„åŠ›æœºåˆ¶çš„ ACTION æ¨¡å—](https://mp.weixin.qq.com/s/L2_lkhKbVhW8fjAaDdsyWQ)
- [No frame left behind: Full Video Action Recognition](https://arxiv.org/abs/2103.15395)
* [Recognizing Actions in Videos from Unseen Viewpoints](https://arxiv.org/abs/2103.16516)
* [Beyond Short Clips: End-to-End Video-Level Learning with Collaborative Memories](https://arxiv.org/abs/2104.01198)
* æ—¶åºåŠ¨ä½œå®šä½
  * [Modeling Multi-Label Action Dependencies for Temporal Action Localization](https://arxiv.org/abs/2103.03027)<br>:open_mouth:oral<br>æå‡ºåŸºäºæ³¨æ„åŠ›çš„ç½‘ç»œæ¶æ„æ¥å­¦ä¹ è§†é¢‘ä¸­çš„åŠ¨ä½œä¾èµ–æ€§ï¼Œç”¨äºè§£å†³å¤šæ ‡ç­¾æ—¶é—´åŠ¨ä½œå®šä½ä»»åŠ¡ã€‚
  * Learning Salient Boundary Feature for Anchor-free Temporal Action Localization<br>åŸºäºæ˜¾è‘—è¾¹ç•Œç‰¹å¾å­¦ä¹ çš„æ— é”šæ¡†æ—¶åºåŠ¨ä½œå®šä½<br>è§£è¯»ï¼š[10](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [The Blessings of Unlabeled Background in Untrimmed Videos](https://arxiv.org/abs/2103.13183)
  * [Temporal Context Aggregation Network for Temporal Action Proposal Refinement](https://arxiv.org/abs/2103.13141)
  * [Learning Salient Boundary Feature for Anchor-free Temporal Action Localization](https://arxiv.org/abs/2103.13137)
  * [CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning](https://arxiv.org/abs/2103.16392)

<a name="35"/>

## 35.å›¾åƒèšç±»

- [Improving Unsupervised Image Clustering With Robust Learning](https://arxiv.org/abs/2012.11150)<br>:star:[code](https://github.com/deu30303/RUC)<br>åˆ©ç”¨é²æ£’å­¦ä¹ æ”¹è¿›æ— ç›‘ç£å›¾åƒèšç±»æŠ€æœ¯<br>
- [Jigsaw Clustering for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2104.00323)<br>:open_mouth:oral:star:[code](https://github.com/Jia-Research-Lab/JigsawClustering)

<a name="34"/>

## 34.å›¾åƒåˆ†ç±»

- [Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels](https://arxiv.org/abs/2101.05022)<br>:star:[code](https://github.com/naver-ai/relabel_imagenet)<br>
* ç»†ç²’åº¦åˆ†ç±»
  * [Fine-grained Angular Contrastive Learning with Coarse Labels](https://arxiv.org/abs/2012.03515)<br>:open_mouth:oral<br>ä½¿ç”¨è‡ªç›‘ç£è¿›è¡Œ Coarse Labelsï¼ˆç²—æ ‡ç­¾ï¼‰çš„ç»†ç²’åº¦åˆ†ç±»æ–¹é¢çš„å·¥ä½œã€‚ç²—æ ‡ç­¾ä¸ç»†ç²’åº¦æ ‡ç­¾ç›¸æ¯”ï¼Œæ›´å®¹æ˜“å’Œæ›´ä¾¿å®œï¼Œå› ä¸ºç»†ç²’åº¦æ ‡ç­¾é€šå¸¸éœ€è¦åŸŸä¸“å®¶ã€‚
  * Graph-based High-Order Relation Discovery for Fine-grained Recognition<br>åŸºäºç‰¹å¾é—´é«˜é˜¶å…³ç³»æŒ–æ˜çš„ç»†ç²’åº¦è¯†åˆ«æ–¹æ³•<br>è§£è¯»ï¼š[20](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * Fine-Grained Few-Shot Classification with Feature Map Reconstruction Networks
  * [A Realistic Evaluation of Semi-Supervised Learning for Fine-Grained Classification](https://arxiv.org/abs/2104.00679)<br>:open_mouth:oral
* å›¾åƒåˆ†ç±»
  * [MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition](https://arxiv.org/abs/2103.12579)
  * [PML: Progressive Margin Loss for Long-tailed Age Classification](https://arxiv.org/abs/2103.02140)<br>
  * [Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification](https://arxiv.org/abs/2103.14267)<br>:house:[project]()(https://www.kaihan.org/HybridLT/)
  * [Capsule Network is Not More Robust than Convolutional Network](https://arxiv.org/abs/2103.15459)
  * [Model-Contrastive Federated Learning](https://arxiv.org/abs/2103.16257)
* åŠç›‘ç£å›¾åƒåˆ†ç±»
  * [SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification](https://arxiv.org/abs/2103.16725)<br>:star:[code](https://github.com/zijian-hu/SimPLE)
é•¿å°¾è§†è§‰è¯†åˆ«
  * [Distribution Alignment: A Unified Framework for Long-tail Visual Recognition](https://arxiv.org/abs/2103.16370)<br>:star:[code](https://github.com/Megvii-BaseDetection/DisAlign)
  * [Improving Calibration for Long-Tailed Recognition](https://arxiv.org/abs/2104.00466)<br>:star:[code](https://github.com/Jia-Research-Lab/MiSLAS)

<a name="33"/>

## 33.6Dä½å§¿ä¼°è®¡

- [FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation](https://arxiv.org/abs/2103.02242)<br>:open_mouth:oral:star:[code](https://github.com/ethnhe/FFB6D)<br>
- [GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation](http://arxiv.org/abs/2102.12145)<br>:star:[code](https://github.com/THU-DA-6D-Pose-Group/GDR-Net)
- [FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism](https://arxiv.org/abs/2103.07054)<br>:open_mouth:oral:star:[code](https://github.com/DC1991/FS-Net)
- [Wide-Depth-Range 6D Object Pose Estimation in Space](https://arxiv.org/abs/2104.00337)<br>:star:[code](https://github.com/cvlab-epfl/wide-depth-range-pose)

<a name="32"/>

## 32.è§†å›¾åˆæˆ

- [ID-Unet: Iterative Soft and Hard Deformation for View Synthesis](https://arxiv.org/abs/2103.02264)
- [NeX: Real-time View Synthesis with Neural Basis Expansion](https://arxiv.org/abs/2103.05606)<br>:open_mouth:oral:house:[project](https://nex-mpi.github.io/):tv:[video](https://www.youtube.com/watch?v=HyfkF7Z-ddA)<br>åˆ©ç”¨ç¥ç»åŸºç¡€æ‰©å±•çš„å®æ—¶è§†å›¾åˆæˆæŠ€æœ¯
- [Layout-Guided Novel View Synthesis from a Single Indoor Panorama](https://arxiv.org/abs/2103.17022)<br>:star:[code](https://github.com/bluestyle97/PNVS)

<a name="31"/>

## 31.å¼€æ”¾é›†è¯†åˆ«

- [Counterfactual Zero-Shot and Open-Set Visual Recognition](https://arxiv.org/abs/2103.00887)<br>:star:[code](https://github.com/yue-zhongqi/gcm-cf)<br>
- [Few-shot Open-set Recognition by Transformation Consistency](https://arxiv.org/abs/2103.01537)<br>
- [Learning Placeholders for Open-Set Recognition](https://arxiv.org/abs/2103.15086)<br>:open_mouth:oral

<a name="30"/>

## 30.æ–°è§†è§’åˆæˆ

- [DeRF: Decomposed Radiance Fields](https://arxiv.org/abs/2011.12490)<br>:house:[project](https://ubc-vision.github.io/derf/)<br>
- [D-NeRF: Neural Radiance Fields for Dynamic Scenes](https://arxiv.org/abs/2011.13961)<br>:house:[project](https://www.albertpumarola.com/research/D-NeRF/index.html)<br>
* [Neural Lumigraph Rendering](https://arxiv.org/abs/2103.11571)<br>:sunflower:[dataset](https://drive.google.com/file/d/1BBpIfrqwZNYmG1TiFljlCnwsmL2OUxNT/view):house:[project](http://www.computationalimaging.org/publications/nlr/):tv:[video](https://www.youtube.com/watch?v=maVF-7x9644)<br>æ–¯å¦ç¦å¤§å­¦
* [AutoInt: Automatic Integration for Fast Neural Volume Rendering](https://arxiv.org/abs/2012.01714)<br>:open_mouth:oral:house:[project](http://www.computationalimaging.org/publications/automatic-integration/):tv:[video](https://youtu.be/GYxFYbih0PU)<br>æ–¯å¦ç¦å¤§å­¦
* [pixelNeRF: Neural Radiance Fields from One or Few Images](https://arxiv.org/abs/2012.02190)<br>:star:[code](https://github.com/sxyu/pixel-nerf):house:[project](https://alexyu.net/pixelnerf/):tv:[video](https://youtu.be/voebZx7f32g)
* [IBRNet: Learning Multi-View Image-Based Rendering](https://arxiv.org/abs/2102.13090)<br>:house:[project](https://ibrnet.github.io/)<br>å¤‡æ³¨ï¼šæœ‰å­¦è€…è¯„è®ºpixelNeRFå’ŒIBRNetçš„å·¥ä½œæ€æƒ³ç›¸è¿‘ï¼Œä½†IBRNetä¼¼ä¹æ›´åŠ æˆç†Ÿã€‚
* [Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans](https://arxiv.org/abs/2012.15838)<br>:star:[code](https://github.com/zju3dv/neuralbody):[project](https://zju3dv.github.io/neuralbody/):tv:[video](https://youtu.be/BPCAMeBCE-8)<br>æµ™å¤§ç­‰å­¦è€…å‘æ˜çš„Neural Bodyç®—æ³•ï¼Œè¾“å…¥å¤šè§’åº¦è§†é¢‘å¯è¾“å‡º3Däººä½“å’Œæ–°è§’åº¦è§†å›¾ã€‚
* [NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis](https://arxiv.org/abs/2012.03927)<br>:house:[project](https://pratulsrinivasan.github.io/nerv/):tv:[video](https://youtu.be/4XyDdvhhjVo)<br>åœ¨ä»»æ„ç…§æ˜æ¡ä»¶ä¸‹ï¼Œæ ¹æ®ä¸€ç»„è¾“å…¥å›¾åƒç”Ÿæˆå®Œæ•´çš„3Dåœºæ™¯
* [Self-Supervised Visibility Learning for Novel View Synthesis](https://arxiv.org/abs/2103.15407)<br>:star:[code](https://github.com/shiyujiao/SVNVS)

<a name="29"/>

## 29.å§¿æ€ä¼°è®¡

- [Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration](https://arxiv.org/abs/2103.02845)<br>:star:[code](https://github.com/SeanChenxy/HandMesh)<br>
- [Monocular Real-time Full Body Capture with Inter-part Correlations](https://arxiv.org/abs/2012.06087)<br>:tv:[video](https://www.youtube.com/watch?v=pAcywTUTv-E)<br>åœ¨ç”µå½±åŠ¨ä½œç‰¹æ•ˆä¸­ï¼Œäººä½“è¿åŠ¨æ•æ‰æ˜¯å…³é”®æŠ€æœ¯ï¼Œé«˜è´¨é‡çš„æ•æ‰å¾€å¾€éœ€è¦ç‰¹æ®Šè®¾å¤‡ï¼Œè€Œå¦‚æœèƒ½ä½¿ç”¨æ™®é€šRGBç›¸æœºè¿›è¡Œè¿åŠ¨æ•æ‰ï¼Œå°†ä¼šä½¿äººäººéƒ½æ˜¯ç‰¹æ•ˆå¸ˆã€‚è¯¥è§†é¢‘æ¥è‡ªæ¸…åã€é©¬æ™®æ‰€ç­‰å•ä½çš„å­¦è€…å‘è¡¨äºCVPR2021çš„è®ºæ–‡ç»“æœï¼Œä½¿ç”¨å•ç›®RGBç›¸æœºçš„åŠ¨ä½œæ•æ‰ã€‚
- [Behavior-Driven Synthesis of Human Dynamics](https://arxiv.org/abs/2103.04677)<br>:star:[code](https://github.com/CompVis/behavior-driven-video-synthesis):house:[project](https://compvis.github.io/behavior-driven-video-synthesis/)
- [Learning Compositional Representation for 4D Captures with Neural ODE](https://arxiv.org/abs/2103.08271)
- [Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation](https://arxiv.org/abs/2012.15175)<br>:star:[code](https://github.com/greatlog/SWAHR-HumanPose)<br>ç²—è§£ï¼š[2](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* 3Dæ‰‹éƒ¨é‡å»º
  * [Model-based 3D Hand Reconstruction via Self-Supervised Learning](https://arxiv.org/abs/2103.11703)
* äººä½“è¿åŠ¨è¿ç§»
  * [Few-Shot Human Motion Transfer by Personalized Geometry and Texture Modeling](https://arxiv.org/abs/2103.14338)<br>:star:[code](https://github.com/HuangZhiChao95/FewShotMotionTransfer):tv:[video](https://www.youtube.com/watch?v=ZJ15X-sdKSU)
* Human Volumetric Capture
  * [POSEFusion: Pose-guided Selective Fusion for Single-view Human Volumetric Capture](https://arxiv.org/abs/2103.15331)<br>:open_mouth:oral:house:[project](http://www.liuyebin.com/posefusion/posefusion.html)
* 3Däººä½“å§¿æ€ä¼°è®¡
  * [CanonPose: Self-supervised Monocular 3D Human Pose Estimation in the Wild](https://arxiv.org/abs/2011.14679)
  * Context Modeling in 3D Human Pose Estimation: A Unified Perspective
  * [PCLs: Geometry-aware Neural Reconstruction of 3D Pose with Perspective Crop Layers](https://arxiv.org/abs/2011.13607)<br>:tv:[video](https://twitter.com/i/status/1334395954644930560)<br>é€šè¿‡æ¶ˆé™¤ location-dependent é€è§†æ•ˆæœæ¥æ”¹è¿›3Däººä½“å§¿åŠ¿ä¼°è®¡æŠ€æœ¯å·¥ä½œã€‚<br>
  * [Graph Stacked Hourglass Networks for 3D Human Pose Estimation](https://arxiv.org/abs/2103.16385)
  * [Human POSEitioning System (HPS): 3D Human Pose Estimation and Self-localization in Large Scenes from Body-Mounted Sensors](https://arxiv.org/abs/2103.17265)<br>:house:[project](http://virtualhumans.mpi-inf.mpg.de/hps/)
  * [SimPoE: Simulated Character Control for 3D Human Pose Estimation](https://arxiv.org/abs/2104.00683)<br>:open_mouth:oral:house:[project](https://www.ye-yuan.com/simpoe/)
  * [Reconstructing 3D Human Pose by Watching Humans in the Mirror](https://arxiv.org/abs/2104.00340)<br>:open_mouth:oral:star:[code](https://github.com/zju3dv/Mirrored-Human):house:[project](https://zju3dv.github.io/Mirrored-Human/)
* åŠ¨ç‰©å§¿æ€ä¼°è®¡
  * [From Synthetic to Real: Unsupervised Domain Adaptation for Animal Pose Estimation](https://arxiv.org/abs/2103.14843)<br>:star:[code](https://github.com/chaneyddtt/UDA-Animal-Pose)

<a name="28"/>

## 28.å¯†é›†é¢„æµ‹

- [Densely connected multidilated convolutional networks for dense prediction tasks](https://arxiv.org/abs/2011.11844)<br>æå‡ºçš„D3Netåœ¨è¯­ä¹‰åˆ†å‰²&éŸ³ä¹æºåˆ†ç¦»ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºSOTAç½‘ç»œ<br>
- [Dense Contrastive Learning for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2011.09157)<br>:star:[code](https://github.com/WXinlong/DenseCL)
* [Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2011.10043)<br>:star:[code](https://github.com/zdaxie/PixPro)

<a name="27"/>

## 27.æ´»ä½“æ£€æµ‹

- [Cross Modal Focal Loss for RGBD Face Anti-Spoofing](https://arxiv.org/abs/2103.00948)<br>

<a name="26"/>

## 26.è§†é¢‘ç›¸å…³æŠ€æœ¯

* [VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples](https://arxiv.org/abs/2103.05905)<br>:star:[code](https://github.com/tinapan-pt/VideoMoCo)
* [Learning by Aligning Videos in Time](https://arxiv.org/abs/2103.17260)
* è§†é¢‘æ‘˜è¦
  * [Learning Discriminative Prototypes with Dynamic Time Warping](https://arxiv.org/abs/2103.09458)<br>:star:[code](https://github.com/BorealisAI/TSC-Disc-Proto)
* è§†é¢‘ç¼–è§£ç 
  * [MetaSCI: Scalable and Adaptive Reconstruction for Video Compressive Sensing](https://arxiv.org/abs/2103.01786)<br>:star:[code](https://github.com/xyvirtualgroup/MetaSCI-CVPR2021)
* è§†é¢‘æ’å¸§
  * [FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation](https://arxiv.org/pdf/2012.08512.pdf)<br>:star:[code](https://tarun005.github.io/FLAVR/Code):house:[project](https://tarun005.github.io/FLAVR/)<br>
* è§†é¢‘è¯­è¨€å­¦ä¹ ï¼ˆvideo-and-language learningï¼‰
  * [Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling](https://arxiv.org/pdf/2102.06183.pdf)<br>:open_mouth:oral:star:[code](https://github.com/jayleicn/ClipBERT)<br>
* è§†é¢‘é¢„æµ‹
  * [Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction](https://arxiv.org/abs/2103.04174)<br>:house:[project](https://sites.google.com/view/ghvae):tv:[video](https://youtu.be/C8_-z8SEGOU)
* è§†é¢‘ç†è§£
  * [Context-aware Biaffine Localizing Network for Temporal Sentence Grounding](https://arxiv.org/abs/2103.11555)<br>:star:[code](https://github.com/liudaizong/CBLN)
  * [Co-Grounding Networks with Semantic Attention for Referring Expression Comprehension in Videos](https://arxiv.org/abs/2103.12346)<br>:house:[project](https://sijiesong.github.io/co-grounding/)
  * [Visual Semantic Role Labeling for Video Understanding](https://arxiv.org/abs/2104.00990)<br>:house:[project](https://vidsitu.org/)
* è§†é¢‘ç¼©æ”¾
  * [Video Rescaling Networks with Joint Optimization Strategies for Downscaling and Upscaling](https://arxiv.org/abs/2103.14858)<br>:star:[code](https://github.com/ding3820/MIMO-VRN):house:[project](https://ding3820.github.io/MIMO-VRN/)
* è§†é¢‘å¼‚å¸¸æ£€æµ‹
  * [MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection](https://arxiv.org/abs/2104.01633)

<a name="25"/>

## 25.ä¸‰ç»´è§†è§‰

- [A Deep Emulator for Secondary Motion of 3D Characters](https://arxiv.org/abs/2103.01261)<br>
- [Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction](https://arxiv.org/abs/2012.01451)<br>:open_mouth:oral:house:[project](https://aljazbozic.github.io/neural_deformation_graphs/):tv:[video](https://www.youtube.com/watch?v=vyq36eFkdWo)<br>
- [Deep Implicit Templates for 3D Shape Representation](https://arxiv.org/abs/2011.14565)<br>:open_mouth:oral:star:[code](https://github.com/ZhengZerong/DeepImplicitTemplates):house:[project](http://www.liuyebin.com/dit/dit.html):tv:[video](http://www.liuyebin.com/dit/assets/supp_vid.mp4)<br>[CVPR 2021 Oralï¼Œæ¸…åå­¦è€…æå‡ºDeep Implicit Templatesï¼Œæå¤§æ‰©å±•DIFèƒ½åŠ›](https://zhuanlan.zhihu.com/p/354737798)<br>
- [SMPLicit: Topology-aware Generative Model for Clothed People](https://arxiv.org/abs/2103.06871)<br>:house:[project](http://www.iri.upc.edu/people/ecorona/smplicit/)
- [Picasso: A CUDA-based Library for Deep Learning over 3D Meshes](https://arxiv.org/abs/2103.15076)<br>:star:[code](https://github.com/hlei-ziyan/Picasso)
- [Semi-supervised Synthesis of High-Resolution Editable Textures for 3D Humans](https://arxiv.org/abs/2103.17266)
* [RGB-D Local Implicit Function for Depth Completion of Transparent Objects](https://arxiv.org/abs/2104.00622)<br>:house:[project](https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit)
* [Deep Two-View Structure-from-Motion Revisited](https://arxiv.org/abs/2104.00556)
* æ·±åº¦ä¼°è®¡
  * [PLADE-Net: Towards Pixel-Level Accuracy for Self-Supervised Single-View Depth Estimation with Neural Positional Encoding and Distilled Matting Loss](https://arxiv.org/abs/2103.07362)
  * [Beyond Image to Depth: Improving Depth Prediction using Echoes](https://arxiv.org/abs/2103.08468)<br>:star:[code](https://github.com/krantiparida/beyond-image-to-depth):house:[project](https://krantiparida.github.io/projects/bimgdepth.html)
  * [Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos](https://arxiv.org/abs/2103.03319)<br>:open_mouth:oral:star:[code](https://github.com/yasaminjafarian/HDNet_TikTok):house:[project](https://www.yasamin.page/hdnet_tiktok):tv:[video](https://youtu.be/EFJ8WXdKghs)
  * [3D Packing for Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/1905.02693)<br>:open_mouth:oral:star:[code](https://github.com/TRI-ML/packnet-sfm)
  * [LED2-Net: Monocular 360 Layout Estimation via Differentiable Depth Rendering](https://arxiv.org/abs/2104.00568)<br>:open_mouth:oral:star:[code](https://github.com/fuenwang/LED2-Net):house:[project](https://fuenwang.ml/project/led2net/)
  * [S2R-DepthNet: Learning a Generalizable Depth-specific Structural Representation](https://arxiv.org/abs/2104.00877)<br>:open_mouth:oral
* ä¸‰ç»´é‡å»º
  * [Deep Implicit Moving Least-Squares Functions for 3D Reconstruction](https://arxiv.org/abs/2103.12266)<br>:star:[code](https://github.com/Andy97/DeepMLS)
  * [Bilevel Online Adaptation for Out-of-Domain Human Mesh Reconstruction](https://arxiv.org/abs/2103.16449)<br>:house:[project](https://sites.google.com/view/humanmeshboa)
  * [Learning Parallel Dense Correspondence from Spatio-Temporal Descriptors for Efficient and Robust 4D Reconstruction](https://arxiv.org/abs/2103.16341)<br>:star:[code](https://github.com/tangjiapeng/LPDC-Net)
  * [Fostering Generalization in Single-view 3D Reconstruction by Learning a Hierarchy of Local and Global Shape Priors](https://arxiv.org/abs/2104.00476)
  * [NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video](https://arxiv.org/abs/2104.00681)<br>:open_mouth:oral:star:[code](https://github.com/zju3dv/NeuralRecon):house:[project](https://zju3dv.github.io/neuralrecon/)
  * [Fully Understanding Generic Objects: Modeling, Segmentation, and Reconstruction](https://arxiv.org/abs/2104.00858)

<a name="24"/> 

## 24.å¼ºåŒ–å­¦ä¹ 
- [Hierarchical and Partially Observable Goal-driven Policy Learning with Goals Relational Graph](https://arxiv.org/abs/2103.01350)<br>:star:[code](https://github.com/Xin-Ye-1/HRL-GRG):house:[project](https://xin-ye-1.github.io/HRL-GRG/)
- [Unsupervised Learning for Robust Fitting:A Reinforcement Learning Approach](https://arxiv.org/abs/2103.03501)

<a name="23"/> 

## 23.è‡ªåŠ¨é©¾é©¶

- [Patch-NetVLAD: Multi-Scale Fusion of Locally-Global Descriptors for Place Recognition](https://arxiv.org/abs/2103.01486)<br>:star:[code](https://github.com/QVPR/Patch-NetVLAD)<br>ECCV 2020 Facebook Mapillary Visual Place Recognition Challenge å† å†›æ–¹æ¡ˆ
* [Convex Online Video Frame Subset Selection using Multiple Criteria for Data Efficient Autonomous Driving](https://arxiv.org/abs/2103.13021)
* è½¦é“çº¿é¢„æµ‹
  * [LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of Dynamic Agents](https://arxiv.org/abs/2104.00249)
* è½¨è¿¹é¢„æµ‹
  * [SGCN:Sparse Graph Convolution Network for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2104.01528)<br>:star:[code](https://github.com/shuaishiliu/SGCN)

<a name="22"/> 

## 22.åŒ»å­¦å½±åƒ

- [3D Graph Anatomy Geometry-Integrated Network for Pancreatic Mass Segmentation, Diagnosis, and Quantitative Patient Management](https://arxiv.org/abs/2012.04701)<br>ç”¨çº¯å¤šæ¨¡æ€ CT å½±åƒå¯æ›¿ä»£ç›®å‰ JHMI çš„éœ€è¦åšè‚¿ç˜¤åŒ–å­¦æ£€æµ‹å’Œ DNA æµ‹åº+åŒ»å­¦å½±åƒçš„ç»¼åˆå¤šæ¨¡æ€è¯Šæ–­æµç¨‹ï¼Œä»è¯Šæ–­å‡†ç¡®åº¦ä¸Šæœ‰å¯æ¯”è¾ƒæ€§ï¼Œå®šé‡è¯Šæ–­ç²¾åº¦æ›´ä¼˜<br>
- [Deep Lesion Tracker: Monitoring Lesions in 4D Longitudinal Imaging Studies](https://arxiv.org/abs/2012.04872)<br>è‚¿ç˜¤å½±åƒé‡Œé¢æ™ºèƒ½ PACS è¾…åŠ©åŒ»ç”Ÿè¯»ç‰‡çš„é‡è¦åŠŸèƒ½<br>
- [Automatic Vertebra Localization and Identification in CT by Spine Rectification and Anatomically-constrained Optimization](https://arxiv.org/abs/2012.07947)<br>åŸºäºCT å½±åƒçš„éª¨æŠ˜/éª¨è´¨ç–æ¾ç³»ç»Ÿ<br>
- [Multi-institutional Collaborations for Improving Deep Learning-based Magnetic Resonance Image Reconstruction Using Federated Learning](https://arxiv.org/abs/2103.02148)<br>:star:[code](https://github.com/guopengf/FL-MRCM)<br>å¤šæœºæ„åˆä½œï¼Œåˆ©ç”¨è”åˆå­¦ä¹ æ”¹è¿›åŸºäºæ·±åº¦å­¦ä¹ çš„ç£å…±æŒ¯å›¾åƒé‡å»ºæŠ€æœ¯<br>
- [DeepTag: An Unsupervised Deep Learning Method for Motion Tracking on Cardiac Tagging Magnetic Resonance Images](https://arxiv.org/abs/2103.02772)<br>:open_mouth:oral:star:[code](https://github.com/DeepTag/cardiac_tagging_motion_estimation)<br>DeepTag: ä¸€ç§æ— ç›‘ç£çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¿ƒè„æ ‡è®°ç£å…±æŒ¯å›¾åƒçš„è¿åŠ¨è·Ÿè¸ª<br>
- [Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles](https://arxiv.org/abs/2103.05121)
* [XProtoNet: Diagnosis in Chest Radiography with Global and Local Explanations](https://arxiv.org/abs/2103.10663)
* åŒ»å­¦å›¾åƒåˆ†å‰²
  * [FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space](https://arxiv.org/abs/2103.06030)<br>:star:[code](https://github.com/liuquande/FedDG-ELCFS)
  * [DoDNet: Learning to segment multi-organ and tumors from multiple partially labeled datasets](https://arxiv.org/abs/2011.10217)<br>:star:[code](https://github.com/jianpengz/DoDNet)
  * [DiNTS: Differentiable Neural Network Topology Search for 3D Medical Image Segmentation](https://arxiv.org/abs/2103.15954)
  * [DARCNN: Domain Adaptive Region-based Convolutional Neural Network for Unsupervised Instance Segmentation in Biomedical Images](https://arxiv.org/abs/2104.01325)<br>
* åŒ»å­¦å›¾åƒåˆæˆ
  * [Brain Image Synthesis with Unsupervised Multivariate Canonical CSCâ„“4Net](https://arxiv.org/abs/2103.11587)<br>:open_mouth:oral

<a name="21"/> 

## 21.Transformer 

- [Transformer Interpretability Beyond Attention Visualization](https://arxiv.org/pdf/2012.09838.pdf)<br>:star:[code](https://github.com/hila-chefer/Transformer-Explainability)<br> 
- [MIST: Multiple Instance Spatial Transformer Network](https://arxiv.org/abs/1811.10725)<br>è¯•å›¾ä»çƒ­å›¾ä¸­è¿›è¡Œå¯å¾®çš„top-Ké€‰æ‹©(MIST)ï¼ˆç›®å‰åœ¨è‡ªç„¶å›¾åƒä¸Šä¹Ÿæœ‰äº†ä¸€äº›ç»“æœï¼›) ç”¨å®ƒå¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•å®šä½ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œæ£€æµ‹å’Œåˆ†ç±»ï¼ˆå¹¶ä¸æ˜¯å®ƒå”¯ä¸€èƒ½åšçš„äº‹æƒ…!ï¼‰
* åŠ¨ä½œè¯†åˆ«æ£€æµ‹
  * 3D Vision Transformers for Action Recognition<br>ç”¨äºåŠ¨ä½œè¯†åˆ«çš„3Dè§†è§‰Transformer
* ç›®æ ‡æ£€æµ‹
  * [UP-DETR: Unsupervised Pre-training for Object Detection with Transformers](https://arxiv.org/pdf/2011.09094.pdf)<br>:open_mouth:oral:star:[code](https://github.com/dddzg/up-detr)
* å›¾åƒå¤„ç†
  * [Pre-Trained Image Processing Transformer](https://arxiv.org/pdf/2012.00364.pdf)<br>
* äººæœºäº¤äº’
  * [End-to-End Human Object Interaction Detection with HOI Transformer](https://arxiv.org/abs/2103.04503)<br>:star:[code](https://github.com/bbepoch/HoiTransformer)
* å›¾åƒåˆ†å‰²
  * [Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers](https://arxiv.org/abs/2012.15840)<br>:star:[code](https://github.com/fudan-zvg/SETR):house:[project](https://fudan-zvg.github.io/SETR/)<br>åŸºäºTransformersä»åºåˆ—åˆ°åºåˆ—çš„è§’åº¦é‡æ–°æ€è€ƒè¯­ä¹‰åˆ†å‰²<br>è§£è¯»ï¼š[16](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)<br>è§£è¯»ï¼š[Transformer åœ¨è¯­ä¹‰åˆ†å‰²ä¸­çš„åº”ç”¨ï¼Œæ›¾ä½ADE20K æ¦œé¦–ï¼ˆ44.42% mIoUï¼‰](https://zhuanlan.zhihu.com/p/341768446)
  * [VisTR: End-to-End Video Instance Segmentation with Transformers](https://arxiv.org/abs/2011.14503)<br>:open_mouth:oral:star:[code](https://github.com/Epiphqny/VisTR)
* è·Ÿè¸ª
  * [Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking](https://arxiv.org/abs/2103.11681)<br>:open_mouth:oral:star:[code](https://github.com/594422814/TransformerTrack)<br>more:[Transformerå†è“„åŠ›ï¼Œè·Ÿè¸ªä»»åŠ¡ä¸­åˆ›æ–°é«˜ï¼Œæ¡¥æ¥ç‹¬ç«‹å¸§ï¼Œè·¨å¸§ä¼ é€’æ—¶åŸŸä¿¡æ¯ï¼ŒCVPR 2021 Oral](https://zhuanlan.zhihu.com/p/359237554)
  * [Transformer Tracking](https://arxiv.org/abs/2103.15436)<br>:star:[code](https://github.com/chenxin-dlut/TransT)
* åŠ¨ä½œé¢„æµ‹
  * [Multimodal Motion Prediction with Stacked Transformers](https://arxiv.org/abs/2103.11624)<br>:star:[code](https://github.com/Mrmoore98/mmTransformer-Multimodal-Motion-Prediction-with-Stacked-Transformers):house:[project](https://decisionforce.github.io/mmTransformer/):tv:[video](https://youtu.be/ytqS8dgVcx0)  
* Self-attentionè‡ªæ³¨æ„åŠ›æœºåˆ¶
  * [Scaling Local Self-Attention For Parameter Efficient Visual Backbones](https://arxiv.org/abs/2103.12731)<br>:open_mouth:oral
* æ£€ç´¢
  * [Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning](https://arxiv.org/abs/2103.13061)<br>:star:[code](https://github.com/amzn/image-to-recipe-transformers)
* ç‰¹å¾åŒ¹é…
  * [LoFTR: Detector-Free Local Feature Matching with Transformers](https://arxiv.org/abs/2104.00680)<br>:star:[code](https://github.com/zju3dv/LoFTR):house:[project](https://zju3dv.github.io/loftr/)

<a name="20"/> 

## 20.äººå‘˜é‡è¯†åˆ«

- [Meta Batch-Instance Normalization for Generalizable Person Re-Identification](https://arxiv.org/abs/2011.14670)<br>
- [Watching You: Global-guided Reciprocal Learning for Video-based Person Re-identification](https://arxiv.org/abs/2103.04337)
- [Joint Noise-Tolerant Learning and Meta Camera Shift Adaptation for Unsupervised Person Re-Identification](https://arxiv.org/abs/2103.04618)<br>:star:[code](https://github.com/FlyingRoastDuck/MetaCam_DSCE)
- [Self-supervised 3D Reconstruction and Re-Projection for Texture Insensitive Person Re-identification]<br>åŸºäºè‡ªç›‘ç£ä¸‰ç»´é‡å»ºå’Œé‡æŠ•å½±çš„çº¹ç†ä¸æ•æ„Ÿè¡Œäººé‡è¯†åˆ«<br>è§£è¯»ï¼š[12](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
- [Intra-Inter Camera Similarity for Unsupervised Person Re-Identification](https://arxiv.org/abs/2103.11658)<br>:star:[code](https://github.com/SY-Xuan/IICS)<br>è®ºæ–‡å…¬å¼€
- [Anchor-Free Person Search](https://arxiv.org/abs/2103.11617)<br>:star:[code](https://github.com/daodaofr/AlignPS)
* [Lifelong Person Re-Identification via Adaptive Knowledge Accumulation](https://arxiv.org/abs/2103.12462)<br>:star:[code](https://github.com/TPCD/LifelongReID)
* [Group-aware Label Transfer for Domain Adaptive Person Re-identification](https://arxiv.org/abs/2103.12366)<br>:star:[code](https://github.com/zkcys001/UDAStrongBaseline)|[code](https://github.com/JDAI-CV/fast-reid)
* æ‹¥æŒ¤äººç¾¤è®¡æ•°
  * [Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT Benchmark for Crowd Counting](https://arxiv.org/abs/2012.04529)

<a name="19"/> 


## 19.é‡åŒ–ã€å‰ªæã€è’¸é¦ã€æ¨¡å‹å‹ç¼©/æ‰©å±•ä¸ä¼˜åŒ–

- Learning Student Networks in the Wild<br>
- [ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network](https://arxiv.org/abs/2007.00992)<br>:star:[code](https://github.com/clovaai/rexnet)<br>
- [RepVGG: Making VGG-style ConvNets Great Again](https://arxiv.org/abs/2101.03697)<br>:star:[code](https://github.com/megvii-model/RepVGG)<br>
- [Coordinate Attention for Efficient Mobile Network Design](https://arxiv.org/abs/2103.02907)<br>:star:[code](https://github.com/Andrew-Qibin/CoordAttention)
* å‰ªæ
  * [Manifold Regularized Dynamic Network Pruning](https://arxiv.org/abs/2103.05861)
  * [Neural Response Interpretation through the Lens of Critical Pathways](https://arxiv.org/abs/2103.16886)<br>:star:[code](https://github.com/CAMP-eXplain-AI/PathwayGrad)|[code](https://github.com/CAMP-eXplain-AI/RoarTorch)
* æ¨¡å‹æ‰©å±•
  * [Fast and Accurate Model Scaling](https://arxiv.org/abs/2103.06877)<br>:star:[code](https://github.com/facebookresearch/pycls)
* é‡åŒ–  
  * [Learnable Companding Quantization for Accurate Low-bit Neural Networks](https://arxiv.org/abs/2103.07156)
  * [Diversifying Sample Generation for Accurate Data-Free Quantization](https://arxiv.org/abs/2103.01049)
  * [Zero-shot Adversarial Quantization](https://arxiv.org/abs/2103.15263)<br>:open_mouth:oral:star:[code](https://github.com/FLHonker/ZAQ-code)
  * [Network Quantization with Element-wise Gradient Scaling](https://arxiv.org/abs/2104.00903)<br>:house:[project](https://cvlab.yonsei.ac.kr/projects/EWGS/) 
* çŸ¥è¯†è’¸é¦
  * [Refine Myself by Teaching Myself: Feature Refinement via Self-Knowledge Distillation](https://arxiv.org/abs/2103.08273)<br>:star:[code](https://github.com/MingiJi/FRSKD)
  * [Complementary Relation Contrastive Distillation](https://arxiv.org/abs/2103.16367)
* å¯é€†ç¥ç»ç½‘ç»œ
  * [Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks](https://arxiv.org/abs/2103.10429)<br>:house:[project](https://paschalidoud.github.io/neural_parts)
* æ¨¡å‹å‹ç¼©
  * [CDFI: Compression-Driven Network Design for Frame Interpolation](https://arxiv.org/abs/2103.10559)<br>:star:[code](https://github.com/tding1/CDFI)


<a name="18"/> 

## 18.èˆªç©ºå½±åƒ/æ— äººæœº

- Dogfight: Detecting Drones from Drone Videos<br>
- [UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles](https://arxiv.org/abs/2104.00946)
* èˆªç©ºå½±åƒåˆ†å‰²
  * [PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation](https://arxiv.org/abs/2103.06564)<br>:star:[code](https://github.com/lxtGH/PFSegNets)
* èˆªç©ºå½±åƒæ£€æµ‹
  * [ReDet: A Rotation-equivariant Detector for Aerial Object Detection](https://arxiv.org/abs/2103.07733)<br>:star:[code](https://github.com/csuhan/ReDet)
* æ— äººæœºæ£€æµ‹
  * [Dogfight: Detecting Drones from Drones Videos](https://arxiv.org/abs/2103.17242)

<a name="17"/> 

## 17.è¶…åˆ†è¾¨ç‡

- Data-Free Knowledge Distillation For Image Super-Resolution<br>
- [AdderSR: Towards Energy Efficient Image Super-Resolution](https://arxiv.org/pdf/2009.08891.pdf)<br>:star:[code](https://github.com/huawei-noah/AdderNet)<br>
- [Cross-MPI: Cross-scale Stereo for Image Super-Resolution using Multiplane Images](https://arxiv.org/abs/2011.14631)<br>:house:[project](http://www.liuyebin.com/crossMPI/crossMPI.html):tv:[video](http://www.liuyebin.com/crossMPI/assets/supp_vid.mp4)<br>[CVPR 2021ï¼ŒCross-MPIä»¥åº•å±‚åœºæ™¯ç»“æ„ä¸ºçº¿ç´¢çš„ç«¯åˆ°ç«¯ç½‘ç»œï¼Œåœ¨å¤§åˆ†è¾¨ç‡ï¼ˆx8ï¼‰å·®è·ä¸‹ä¹Ÿå¯å®Œæˆé«˜ä¿çœŸçš„è¶…åˆ†è¾¨ç‡](https://zhuanlan.zhihu.com/p/354752197)
- [ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic](https://arxiv.org/abs/2103.04039)<br>:star:[code](https://github.com/Xiangtaokong/ClassSR)
* Robust Reference-based Super-Resolution via CÂ²-Matching
* [GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution](https://arxiv.org/abs/2012.00739)<br>:open_mouth:oral:house:[project](https://ckkelvinchan.github.io/projects/GLEAN/)
* [BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond](https://arxiv.org/abs/2012.02181)<br>:star:[code](https://github.com/ckkelvinchan/BasicVSR-IconVSR):house:[project](https://ckkelvinchan.github.io/projects/BasicVSR/)
* [Temporal Modulation Network for Controllable Space-Time Video Super-Resolution]<br>[ä½œè€…ä¸»é¡µ](https://csjunxu.github.io/)<br>åŸºäºæ—¶ç©ºç‰¹å¾å¯æ§æ’å€¼çš„è§†é¢‘è¶…åˆ†è¾¨ç‡ç½‘ç»œ<br>è§£è¯»ï¼š[18](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* [Flow-based Kernel Prior with Application to Blind Super-Resolution](https://arxiv.org/abs/2103.15977)
* [Unsupervised Degradation Representation Learning for Blind Super-Resolution](https://arxiv.org/abs/2104.00416)<br>:star:[code](https://github.com/LongguangWang/DASR)

<a name="16"/> 


## 16.è§†è§‰é—®ç­”

- Weakly-supervised Grounded Visual Question Answering using Capsules<br>
* [Counterfactual VQA: A Cause-Effect Look at Language Bias](https://arxiv.org/abs/2006.04315)<br>:star:[code](https://github.com/yuleiniu/cfvqa)
* [AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning](https://arxiv.org/abs/2103.16002)
* [Domain-robust VQA with diverse datasets and methods but no target labels](https://arxiv.org/abs/2103.15974)
* è§†é¢‘é—®ç­”
  * [TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events](https://arxiv.org/abs/2103.15538)<br>:star:[code](https://github.com/SUTDCV/SUTD-TrafficQA)

<a name="15"/> 

## 15.GAN
- Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing<br>
- [Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation](https://arxiv.org/abs/2008.00951)<br>:star:[code](https://github.com/eladrich/pixel2style2pixel):house:[project](https://eladrich.github.io/pixel2style2pixel/)<br>
- [Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs](https://arxiv.org/pdf/2011.14107.pdf)<br>
- [Image-to-image Translation via Hierarchical Style Disentanglement](https://arxiv.org/abs/2103.01456)<br>:star:[code](https://github.com/imlixinyang/HiSD)
- [Efficient Conditional GAN Transfer with Knowledge Propagation across Classes](https://arxiv.org/abs/2102.06696)<br>:star:[code](https://github.com/mshahbazi72/cGANTransfer)
- [Anycost GANs for Interactive Image Synthesis and Editing](https://arxiv.org/abs/2103.03243)<br>:star:[code](https://github.com/mit-han-lab/anycost-gan):house:[project](https://hanlab.mit.edu/projects/anycost-gan/):tv:[video](https://www.youtube.com/watch?v=_yEziPl9AkM&t=90s)<br>Anycost GANï¼Œå¯é€‚åº”å¹¿æ³›çš„ç¡¬ä»¶å’Œå»¶è¿Ÿè¦æ±‚ï¼Œä»¥åŠå®ç°äº¤äº’å¼å›¾åƒç¼–è¾‘
- [TediGAN: Text-Guided Diverse Image Generation and Manipulation](https://arxiv.org/abs/2012.03308)<br>:star:[code](https://github.com/weihaox/TediGAN):house:[project](https://xiaweihao.com/projects/tedigan/):tv:[video](https://www.youtube.com/watch?v=L8Na2f5viAM)
- [Generative Hierarchical Features from Synthesizing Images](https://arxiv.org/abs/2007.10379)<br>:open_mouth:oral:star:[code](https://github.com/genforce/ghfeat):house:[project](https://genforce.github.io/ghfeat/)<br>ä½œè€…ç§°é¢„è®­ç»ƒ GAN ç”Ÿæˆå™¨å¯ä»¥å½“ä½œæ˜¯ä¸€ç§å­¦ä¹ çš„å¤šå°ºåº¦æŸå¤±ã€‚ç”¨å®ƒè¿›è¡Œè®­ç»ƒå¯ä»¥å¸¦æ¥é«˜åº¦ç«äº‰çš„å±‚æ¬¡åŒ–å’Œåˆ†ç¦»çš„è§†è§‰ç‰¹å¾ï¼Œç§°ä¹‹ä¸ºç”Ÿæˆå±‚æ¬¡åŒ–ç‰¹å¾ï¼ˆGH-Featï¼‰ã€‚å¹¶è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒGH-Featä¸ä»…æœ‰åˆ©äºç”Ÿæˆæ€§ä»»åŠ¡ï¼Œæ›´é‡è¦çš„æ˜¯æœ‰åˆ©äºåˆ†è¾¨æ€§ä»»åŠ¡ï¼ŒåŒ…æ‹¬äººè„¸éªŒè¯ã€å…³é”®ç‚¹æ£€æµ‹ã€layout predictionã€è¿ç§»å­¦ä¹ ã€style mixingã€å›¾åƒç¼–è¾‘ç­‰ã€‚
- [Teachers Do More Than Teach: Compressing Image-to-Image Models](https://arxiv.org/abs/2103.03467)
- [PISE: Person Image Synthesis and Editing with Decoupled GAN](https://arxiv.org/abs/2103.04023)<br>:star:[code](https://github.com/Zhangjinso/PISE)
- [LOHO: Latent Optimization of Hairstyles via Orthogonalization](https://arxiv.org/abs/2103.03891)
- [Image-to-image Translation via Hierarchical Style Disentanglement](https://arxiv.org/abs/2103.01456)<br>:open_mouth:oral:star:[code](https://github.com/imlixinyang/HiSD)<br>åœ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸Šå®ç°å±‚æ¬¡é£æ ¼è§£è€¦
- [CoMoGAN: continuous model-guided image-to-image translation](https://arxiv.org/abs/2103.06879)<br>:open_mouth:oral:star:[code](https://github.com/cv-rits/CoMoGAN)
- [HumanGAN: A Generative Model of Humans Images](https://arxiv.org/abs/2103.06902)
- [HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms](https://arxiv.org/abs/2011.11731)<br>:star:[code](https://github.com/mahmoudnafifi/HistoGAN)
- [DivCo: Diverse Conditional Image Synthesis via Contrastive Generative Adversarial Network](https://arxiv.org/abs/2103.07893)<br>:star:[code](https://github.com/ruiliu-ai/DivCo)
* [pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis](https://arxiv.org/abs/2012.00926)<br>:open_mouth:oral:house:[project](https://marcoamonteiro.github.io/pi-GAN-website/):tv:[video](https://youtu.be/0HCdof9BGtw)<br>æ›´å¤šï¼š[æ–¯å¦ç¦å­¦è€…æå‡ºå‘¨æœŸæ€§éšå¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆÏ€-GANæˆ–pi-GANï¼‰ï¼Œç”¨äºé«˜è´¨é‡çš„3Dæ„ŸçŸ¥å›¾åƒåˆæˆ](https://zhuanlan.zhihu.com/p/336155077)<br>æ–¯å¦ç¦å¤§å­¦
* [ReMix: Towards Image-to-Image Translation with Limited Data](https://arxiv.org/abs/2103.16835)
* [Unsupervised Disentanglement of Linear-Encoded Facial Semantics](https://arxiv.org/abs/2103.16605)
  
<a name="14"/> 

## 14.å°/é›¶æ ·æœ¬å­¦ä¹ ï¼ŒåŸŸé€‚åº”ï¼ŒåŸŸæ³›åŒ–

* å°æ ·æœ¬å­¦ä¹ 
  * Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning<br>
  * [Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning](https://arxiv.org/abs/2103.01315)<br>
  * [Learning Dynamic Alignment via Meta-filter for Few-shot Learning](https://arxiv.org/abs/2103.13582)<br>[ä½œè€…ä¸»é¡µ](https://yanweifu.github.io/page3.html)<br>é€šè¿‡å…ƒå·ç§¯æ ¸å®ç°åŸºäºåŠ¨æ€å¯¹é½çš„å°æ ·æœ¬å­¦ä¹ <br>è§£è¯»ï¼š[17](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* åŸŸæ³›åŒ–
  * [FSDR: Frequency Space Domain Randomization for Domain Generalization](https://arxiv.org/abs/2103.02370)<br>å— JPEG å°†ç©ºé—´å›¾åƒè½¬æ¢ä¸ºå¤šä¸ªé¢‘ç‡åˆ†é‡(FCs)çš„å¯å‘ï¼Œæå‡ºé¢‘ç‡ç©ºé—´åŸŸéšæœºåŒ–(FSDR)ï¼Œé€šè¿‡ä¿ç•™åŸŸå˜é‡FCs(DIFs)å’ŒåªéšæœºåŒ–åŸŸå˜é‡FCs(DVFs)æ¥éšæœºåŒ–é¢‘ç‡ç©ºé—´çš„å›¾åƒã€‚
  * [Domain Generalization via Inference-time Label-Preserving Target Projections](https://arxiv.org/abs/2103.01134)<br>:open_mouth: Oral
  * [Adaptive Methods for Real-World Domain Generalization](https://arxiv.org/abs/2103.15796)
  * [Progressive Domain Expansion Network for Single Domain Generalization](https://arxiv.org/abs/2103.16050)<br>:star:[code](https://github.com/lileicv/PDEN)
* é›¶æ ·æœ¬å­¦ä¹ 
  * [Goal-Oriented Gaze Estimation for Zero-Shot Learning](https://arxiv.org/abs/2103.03433):star:[code](https://github.com/osierboy/GEM-ZSL)
  * [Contrastive Embedding for Generalized Zero-Shot Learning](https://arxiv.org/abs/2103.16173)<br>:star:[code](https://github.com/Hanzy1996/CE-GZSL)
* åŸŸé€‚åº”
  * [Dynamic Transfer for Multi-Source Domain Adaptation](https://arxiv.org/abs/2103.10583)<br>:star:[code](https://github.com/liyunsheng13/DRT)  
  * [Transferable Semantic Augmentation for Domain Adaptation](https://arxiv.org/abs/2103.12562)
  * [MetaAlign: Coordinating Domain Alignment and Classification for Unsupervised Domain Adaptation](https://arxiv.org/abs/2103.13575)
  * [DRANet: Disentangling Representation and Adaptation Networks for Unsupervised Cross-Domain Adaptation](https://arxiv.org/abs/2103.13447)
  * [Dynamic Domain Adaptation for Efficient Inference](https://arxiv.org/abs/2103.16403)<br>:star:[code](https://github.com/BIT-DA/DDA)
  * [Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation](https://arxiv.org/abs/2103.16765)<br>:house:[project](http://xyue.io/pcs-fuda/)
  * [Domain Consensus Clustering for Universal Domain Adaptation](http://reler.net/papers/guangrui_cvpr2021.pdf)<br>:star:[code](https://github.com/Solacex/Domain-Consensus-Clustering)
  * [Divergence Optimization for Noisy Universal Domain Adaptation](https://arxiv.org/abs/2104.00246)
  * [Curriculum Graph Co-Teaching for Multi-Target Domain Adaptation](https://arxiv.org/abs/2104.00808)<br>:star:[code](https://github.com/Evgeneus/Graph-Domain-Adaptaion):house:[project](https://roysubhankar.github.io/graph-coteaching-adaptation/)
  * [Instance Level Affinity-Based Transfer for Unsupervised Domain Adaptation](https://arxiv.org/abs/2104.01286)<br>:star:[code](https://github.com/astuti/ILA-DA)
  * [Unsupervised Multi-source Domain Adaptation Without Access to Source Data](https://arxiv.org/abs/2104.01845)
 
<a name="13"/> 


## 13.å›¾åƒ/è§†é¢‘æ£€ç´¢
* [Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers](https://arxiv.org/abs/2103.16553)
* [Convolutional Hough Matching](https://arxiv.org/abs/2103.16831)<br>:open_mouth:oral:house:[project](http://cvlab.postech.ac.kr/research/CHM/)
* å›¾åƒæ£€ç´¢
  * [Probabilistic Embeddings for Cross-Modal Retrieval](https://arxiv.org/abs/2101.05068)<br>
  * [QAIR: Practical Query-efficient Black-Box Attacks for Image Retrieval](https://arxiv.org/abs/2103.02927)
  * [More Photos are All You Need: Semi-Supervised Learning for Fine-Grained Sketch Based Image Retrieval](https://arxiv.org/abs/2103.13990)<br>:star:[code](https://github.com/AyanKumarBhunia/semisupervised-FGSBIR)
  * [StyleMeUp: Towards Style-Agnostic Sketch-Based Image Retrieval](https://arxiv.org/abs/2103.15706)
* è§†é¢‘æ£€ç´¢
  * [On Semantic Similarity in Video Retrieval](https://arxiv.org/abs/2103.10095)<br>:star:[code](https://github.com/mwray/Semantic-Video-Retrieval):house:[project](https://mwray.github.io/SSVR/):tv:[video](https://youtu.be/pS9qa_B771I)

<a name="12"/> 

## 12.å›¾åƒå¢å¼º

* å›¾åƒæ¢å¤Image Restoration
  * [Multi-Stage Progressive Image Restoration](https://arxiv.org/abs/2102.02808)<br>:star:[code](https://github.com/swz30/MPRNet)<br>
* å»é˜´å½±Shadow Removal
  * [Auto-Exposure Fusion for Single-Image Shadow Removal](https://arxiv.org/abs/2103.01255)<br>:star:[code](https://github.com/tsingqguo/exposure-fusion-shadow-removal)<br>
  * [From Shadow Generation to Shadow Removal](https://arxiv.org/abs/2103.12997)<br>:star:[code](https://github.com/hhqweasd/G2R-ShadowNet)
* å»æ¨¡ç³ŠDeblurring
  * [DeFMO: Deblurring and Shape Recovery of Fast Moving Objects](https://arxiv.org/abs/2012.00595)<br>:star:[code](https://github.com/rozumden/DeFMO):tv:[video](https://www.youtube.com/watch?v=pmAynZvaaQ4)<br>
  * [ARVo: Learning All-Range Volumetric Correspondence for Video Deblurring](https://arxiv.org/abs/2103.04260)
  * [Explore Image Deblurring via Blur Kernel Space](https://arxiv.org/abs/2104.00317)
  * [Towards Rolling Shutter Correction and Deblurring in Dynamic Scenes](https://arxiv.org/abs/2104.01601)<br>:star:[code](https://github.com/zzh-tech/RSCD)
* å»åå°„Reflection Removal
  * [Robust Reflection Removal with Reflection-free Flash-only Cues](https://arxiv.org/abs/2103.04273)<br>:star:[ccode](https://github.com/ChenyangLEI/flash-reflection-removal)
* å»é›¾
  * Learning to Restore Hazy Video: A New Real-World Dataset and A New Method<br>å­¦ä¹ å¤åŸæœ‰é›¾è§†é¢‘ï¼šä¸€ç§æ–°çš„çœŸå®æ•°æ®é›†åŠç®—æ³•<br>è§£è¯»ï¼š[9](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * Contrastive Learning for Compact Single Image Dehazing<br>åŸºäºå¯¹æ¯”å­¦ä¹ çš„ç´§å‡‘å›¾åƒå»é›¾æ–¹æ³•<br>è§£è¯»ï¼š[5](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* å»å™ªDenoising
  * [Neighbor2Neighbor: Self-Supervised Denoising from Single Noisy Images](https://arxiv.org/abs/2101.02824)<br>è§£è¯»ï¼š[CVPR 2021 | Neighbor2Neighborï¼šä»…éœ€å™ªå£°å›¾åƒå³å¯è®­ç»ƒä»»æ„é™å™ªç½‘ç»œçš„æ–¹æ³•](https://mp.weixin.qq.com/s/Eg7vbjTILSd1Si3HSyz3CA)
  * [NBNet: Noise Basis Learning for Image Denoising with Subspace Projection](https://arxiv.org/abs/2012.15028)<br>ç²—è§£ï¼š[9](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* å»é›¨Deraining
  * [Semi-Supervised Video Deraining with Dynamic Rain Generator](https://arxiv.org/abs/2103.07939)
  * [Closing the Loop: Joint Rain Generation and Removal via Disentangled Image Translation](https://arxiv.org/abs/2103.13660)
* æ›å…‰æ ¡æ­£
  * [Learning Multi-Scale Photo Exposure Correction](https://arxiv.org/abs/2003.11596)<br>:star:[code](https://github.com/mahmoudnafifi/Exposure_Correction)
* å›¾åƒä¿®å¤Image Inpainting
  * [Generating Diverse Structure for Image Inpainting With Hierarchical VQ-VAE](https://arxiv.org/abs/2103.10022)<br>:star:[code](https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting)
  * [TransFill: Reference-guided Image Inpainting by Merging Multiple Color and Spatial Transformations](https://arxiv.org/abs/2103.15982)<br>:house:[project](https://yzhouas.github.io/projects/TransFill/index.html)
* å›¾åƒç¼–è¾‘
  * [DeFLOCNet: Deep Image Editing via Flexible Low-level Controls](https://arxiv.org/abs/2103.12723)<br>:star:[code](https://github.com/KumapowerLIU/DeFLOCNet)
* å›¾åƒå‹ç¼©
  * [Attention-guided Image Compression by Deep Reconstruction of Compressive Sensed Saliency Skeleton](https://arxiv.org/abs/2103.15368)
  * [Slimmable Compressive Autoencoders for Practical Neural Image Compression](https://arxiv.org/abs/2103.15726)<br>:star:[code](https://github.com/FireFYF/SlimCAE)
  * [Checkerboard Context Model for Efficient Learned Image Compression](https://arxiv.org/abs/2103.15306)
  * [Learning Scalable â„“âˆ-constrained Near-lossless Image Compression via Joint Lossy Image and Residual Compression](https://arxiv.org/abs/2103.17015)<br>:star:[code](https://github.com/BYchao100/Scalable-Near-lossless-Image-Compression)

<a name="11"/> 

## 11. äººè„¸æŠ€æœ¯

* [Towards High Fidelity Face Relighting with Realistic Shadows](https://arxiv.org/abs/2104.00825)<br>:star:[code](https://github.com/andrewhou1/Shadow-Mask-Face-Relighting)
* äººè„¸è¯†åˆ«
  * [A 3D GAN for Improved Large-pose Facial Recognition](https://arxiv.org/pdf/2012.10545.pdf)<br>
  * [When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework](https://arxiv.org/abs/2103.01520)<br>:open_mouth:oral:star:[code](https://github.com/Hzzone/MTLFace)<br>
  * [MagFace: A Universal Representation for Face Recognition and Quality Assessment](https://arxiv.org/abs/2103.06627)<br>:open_mouth:oral:star:[code](https://github.com/IrvingMeng/MagFace)<br>äººè„¸è¯†åˆ«+è´¨é‡ï¼Œä»Šå¹´çš„Oral presentationã€‚ ä»£ç å¾…æ•´ç†
  * [WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition](https://arxiv.org/abs/2103.04098)<br>:house:[project](https://www.face-benchmark.org/)
  * [ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis](https://arxiv.org/abs/2103.05630)<br>:open_mouth:oral:house:[project](https://yinanhe.github.io/projects/forgerynet.html):tv:[video](https://youtu.be/e8XIL3Di2Y8) 
  * Spherical Confidence Learning for Face Recognition<br>:open_mouth:oral<br>åŸºäºè¶…çƒæµå½¢ç½®ä¿¡åº¦å­¦ä¹ çš„äººè„¸è¯†åˆ«
  * Consistent Instance False Positive Improves Fairness in Face Recognition<br>åŸºäºå®ä¾‹è¯¯æŠ¥ä¸€è‡´æ€§çš„äººè„¸è¯†åˆ«å…¬å¹³æ€§æå‡æ–¹æ³•<br>è§£è¯»ï¼š[7](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [CRFace: Confidence Ranker for Model-Agnostic Face Detection Refinement](https://arxiv.org/abs/2103.07017)
  * [Cross-Domain Similarity Learning for Face Recognition in Unseen Domains](https://arxiv.org/abs/2103.07503)
  * [HLA-Face: Joint High-Low Adaptation for Low Light Face Detection](https://arxiv.org/abs/2104.01984)<br>:house:[project](https://daooshee.github.io/HLA-Face-Website/)
* åˆæˆäººè„¸ï¼ˆDeepfake/Face Forgeryï¼‰æ£€æµ‹
  * [Multi-attentional Deepfake Detection](https://arxiv.org/abs/2103.02406)<br>
  * [Frequency-aware Discriminative Feature Learning Supervised by Single-Center Loss for Face Forgery Detection](https://arxiv.org/abs/2103.09096)
  * [MagDR: Mask-guided Detection and Reconstruction for Defending Deepfakes](https://arxiv.org/abs/2103.14211)
  * [Face Forensics in the Wild](https://arxiv.org/abs/2103.16076)<br>:open_mouth:oral:star:[code](https://github.com/tfzhou/FFIW)
* äººè„¸è´¨é‡è¯„ä¼°
  * [SDD-FIQA: Unsupervised Face Image Quality Assessment with Similarity Distribution Distance](https://arxiv.org/abs/2103.05977)<br>åŸºäºç›¸ä¼¼åº¦åˆ†å¸ƒè·ç¦»çš„æ— ç›‘ç£äººè„¸è´¨é‡è¯„ä¼°<br>è§£è¯»ï¼š[6](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* 3Däººè„¸é‡å»º
  * Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection<br>:open_mouth:oral<br>åœ¨å¼€æ”¾çš„äººåƒé›†åˆä¸­å­¦ä¹ 3Däººè„¸çš„èšåˆä¸ç‰¹å¼‚åŒ–é‡å»º
  * [3DCaricShop: A Dataset and A Baseline Method for Single-view 3D Caricature Face Reconstruction](https://arxiv.org/abs/2103.08204)<br>:star:[code](https://github.com/qiuyuda/3DCaricShop):house:[project](https://qiuyuda.github.io/3DCaricShop/)
* äººè„¸è¡¨æƒ…è¯†åˆ«
  * [Affective Processes: stochastic modelling of temporal context for emotion and facial expression recognition](https://arxiv.org/abs/2103.13372)<br>
  * [Dive into Ambiguity: Latent Distribution Mining and Pairwise Uncertainty Estimation for Facial Expression Recognition](https://arxiv.org/abs/2104.00232)
* äººè„¸èšç±» 
  * [Structure-Aware Face Clustering on a Large-Scale Graph with 10^7 Nodes](https://arxiv.org/abs/2103.13225)<br>:star:[code](https://github.com/sstzal/STAR-FC):house:[project](https://sstzal.github.io/STAR-FC/)
* äººè„¸ç¼–è¾‘
  * [High-Fidelity and Arbitrary Face Editing](https://arxiv.org/abs/2103.15814)
* äººè„¸è·Ÿè¸ª
  * [High-fidelity Face Tracking for AR/VR via Deep Lighting Adaptation](https://arxiv.org/abs/2103.15876)<br>:house:[project](https://www.cs.rochester.edu/u/lchen63/):tv:[video](https://www.youtube.com/watch?v=dtz1LgZR8cc)
* å¹¿è§’äººè„¸çŸ«æ­£
  * Practical Wide-Angle Portraits Correction with Deep Structured Models<br>ç²—è§£ï¼š[7](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)

<a name="10"/> 

## 10.ç¥ç»æ¶æ„æœç´¢

- [AttentiveNAS: Improving Neural Architecture Search via Attentive](https://arxiv.org/pdf/2011.09011.pdf)<br>
- [HourNAS: Extremely Fast Neural Architecture Search Through an Hourglass Lens](https://arxiv.org/pdf/2005.14446.pdf)<br>
- [ReNAS: Relativistic Evaluation of Neural Architecture Search](https://arxiv.org/pdf/1910.01523.pdf)<br>
- [OPANAS: One-Shot Path Aggregation Network Architecture Search for Object](https://arxiv.org/abs/2103.04507)
- Towards Improving the Consistency, Efficiency, and Flexibility of Differentiable Neural Architecture Search<br>åŒ—äº¬å¤§å­¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æœºå™¨å­¦ä¹ ç ”ç©¶ä¸­å¿ƒ
- [Contrastive Neural Architecture Search with Neural Architecture Comparators](https://arxiv.org/abs/2103.05471)<br>:star:[code](https://arxiv.org/abs/2103.05471)
- [Searching by Generating: Flexible and Efficient One-Shot NAS with Architecture Generator](https://arxiv.org/abs/2103.07289)<br>:star:[code](https://github.com/eric8607242/SGNAS)
- [Prioritized Architecture Sampling with Monto-Carlo Tree Search](https://arxiv.org/abs/2103.11922)<br>:star:[code](https://github.com/xiusu/NAS-Bench-Macro)
* [One-Shot Neural Ensemble Architecture Search by Diversity-Guided Search Space Shrinking](https://arxiv.org/abs/2104.00597)<br>:star:[code](https://github.com/researchmm/NEAS)
* [NetAdaptV2: Efficient Neural Architecture Search with Fast Super-Network Training and Architecture Optimization](https://arxiv.org/abs/2104.00031)<br>:house:[project](http://web.mit.edu/netadapt/)
* [Neural Architecture Search with Random Labels](https://arxiv.org/abs/2101.11834)<br>ç²—è§£ï¼š[1](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)

<a name="9"/> 

## 9.ç›®æ ‡è·Ÿè¸ª

- [Rotation Equivariant Siamese Networks for Tracking](https://arxiv.org/abs/2012.13078)<br>
* å¤šç›®æ ‡è·Ÿè¸ª
  * [Probabilistic Tracklet Scoring and Inpainting for Multiple Object Tracking](https://arxiv.org/abs/2012.02337)<br>
  * [Track to Detect and Segment: An Online Multi-Object Tracker](https://arxiv.org/abs/2103.08808)<br>:star:[code](https://github.com/JialianW/TraDeS):house:[project](https://jialianwu.com/projects/TraDeS.html):tv:[video](https://youtu.be/oGNtSFHRZJA)<br>TraDeS ï¼šCVPR 2021å¤šç›®æ ‡è·Ÿè¸ªç®—æ³•ï¼Œæ”¹è¿›äº†ç›®å‰è”åˆæ£€æµ‹ä¸è·Ÿè¸ªçš„åœ¨çº¿æ–¹æ³•ï¼Œä½¿ç”¨è·Ÿè¸ªçº¿ç´¢è¾…åŠ©æ£€æµ‹ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†å®ç°äº†å¤§å¹…ç²¾åº¦æå‡ï¼Œä½œè€…æ¥è‡ªçº½çº¦å·ç«‹å¤§å­¦ã€‚ä»£ç å·²å¼€æºã€‚
  * Multiple Object Tracking with Correlation Learning
  * [Learning a Proposal Classifier for Multiple Object Tracking](https://arxiv.org/abs/2103.07889)<br>:star:[code](https://github.com/daip13/LPC_MOT)
  * [Learnable Graph Matching: Incorporating Graph Partitioning with Deep Feature Learning for Multiple Object Tracking](https://arxiv.org/abs/2103.16178)<br>:star:[code](https://github.com/jiaweihe1996/GMTracker)
  * [Online Multiple Object Tracking with Cross-Task Synergy](https://arxiv.org/abs/2104.00380)<br>:star:[code](https://github.com/songguocode/TADAM)
* è§†è§‰ç›®æ ‡è·Ÿè¸ª
  * [IoU Attack: Towards Temporally Coherent Black-Box Adversarial Attack for Visual Object Tracking](https://arxiv.org/abs/2103.14938)<br>:star:[code](https://github.com/VISION-SJTU/IoUattack)
  * [Learning to Track Instances without Video Annotations](https://arxiv.org/abs/2104.00287)<br>:open_mouth:oral:house:[project](https://oasisyang.github.io/projects/semi-track/index.html):tv:[video](https://youtu.be/-S7xtk-7pGk)
* å•ç›®æ ‡è·Ÿè¸ª
  * [Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark](https://arxiv.org/abs/2103.16746)<br>:house:[project](https://sites.google.com/view/langtrackbenchmark/):tv:[video](https://www.youtube.com/watch?v=7lvVDlkkff0)
  * [SiamGAT: Graph Attention Tracking](https://arxiv.org/abs/2011.11204)<br>:star:[code](https://github.com/ohhhyeahhh/SiamGAT)

<a name="8"/> 

## 8.å›¾åƒåˆ†å‰²

- [Information-Theoretic Segmentation by Inpainting Error Maximization](https://arxiv.org/abs/2012.07287)<br>
- [Simultaneously Localize, Segment and Rank the Camouflaged Objects](https://arxiv.org/abs/2103.04011)<br>:star:[code](https://github.com/JingZhang617/COD-Rank-Localize-and-Segment)
- [Capturing Omni-Range Context for Omnidirectional Segmentation](https://arxiv.org/abs/2103.05687)<br>:star:[code](https://github.com/elnino9ykl/WildPASS)
- [Boundary IoU: Improving Object-Centric Image Segmentation Evaluation](https://arxiv.org/abs/2103.16562)<br>:star:[code](https://github.com/bowenc0221/boundary-iou-api):house:[project](https://bowenc0221.github.io/boundary-iou/)
* [Locate then Segment: A Strong Pipeline for Referring Image Segmentation](https://arxiv.org/abs/2103.16284)
* å®ä¾‹åˆ†å‰²
  * [Zero-Shot Instance Segmentation]<br>åˆ›æ–°å¥‡æ™ºé¦–æ¬¡æå‡ºé›¶æ ·æœ¬å®ä¾‹åˆ†å‰²ï¼ŒåŠ©åŠ›è§£å†³å·¥ä¸šåœºæ™¯æ•°æ®ç“¶é¢ˆéš¾é¢˜
  * [Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers](https://arxiv.org/abs/2103.12340)<br>:star:[code](https://github.com/lkeab/BCNet)
  * [Weakly Supervised Instance Segmentation for Videos with Temporal Mask Consistency](https://arxiv.org/abs/2103.12886)
  * [FAPIS: A Few-shot Anchor-free Part-based Instance Segmenter](https://arxiv.org/abs/2104.00073)
  * [Weakly-supervised Instance Segmentation via Class-agnostic Learning with Salient Images](https://arxiv.org/abs/2104.01526)<br>
* å…¨æ™¯åˆ†å‰²
  * [4D Panoptic LiDAR Segmentation](https://arxiv.org/abs/2102.12472)<br>
  * [Cross-View Regularization for Domain Adaptive Panoptic Segmentation](https://arxiv.org/abs/2103.02584)<br>:open_mouth:oral<br>ç”¨äºåŸŸè‡ªé€‚åº”å…¨æ™¯åˆ†å‰²çš„è·¨è§†å›¾æ­£åˆ™åŒ–æ–¹æ³•<br>
  * Part-aware Panoptic Segmentation<br>
  * Toward Joint Thing-and-Stuff Mining for Weakly Supervised Panoptic Segmentation<br>è”åˆç‰©ä½“å’Œç‰©è´¨æŒ–æ˜çš„å¼±ç›‘ç£å…¨æ™¯åˆ†å‰²<br>è§£è¯»ï¼š[15](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [Panoptic-PolarNet: Proposal-free LiDAR Point Cloud Panoptic Segmentation](https://arxiv.org/abs/2103.14962)<br>:star:[code](https://github.com/edwardzhou130/Panoptic-PolarNet)
  * [Fully Convolutional Networks for Panoptic Segmentation](https://arxiv.org/abs/2012.00720)<br>:open_mouth:oral:star:[code](https://github.com/yanwei-li/PanopticFCN)<br>ç²—è§£ï¼š[11](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* è¯­ä¹‰åˆ†å‰²
  * [PLOP: Learning without Forgetting for Continual Semantic Segmentation](https://arxiv.org/abs/2011.11390)<br>:star:[code](https://github.com/arthurdouillard/CVPR2021_PLOP)
  * [Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges](https://arxiv.org/abs/2009.03137)<br>:sunflower:[dataset](https://github.com/QingyongHu/SensatUrban):tv:[video](https://www.youtube.com/watch?v=IG0tTdqB3L8)<br>
  * [Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation](https://arxiv.org/abs/2103.04717)
  * [Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation](https://arxiv.org/abs/2103.04705)
  * [Differentiable Multi-Granularity Human Representation Learning for Instance-Aware Human Semantic Parsing](https://arxiv.org/abs/2103.04570)<br>:open_mouth:oral:star:[code](https://github.com/tfzhou/MG-HumanParsing)
  * [Learning Statistical Texture for Semantic Segmentation](https://arxiv.org/abs/2103.04133)
  * [MetaCorrection: Domain-aware Meta Loss Correction for Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2103.05254)<br>:star:[code](https://github.com/cyang-cityu/MetaCorrection)<br>è¯­ä¹‰åˆ†å‰²ä¸­çš„æ— ç›‘ç£åŸŸé€‚åº”çš„åŸŸæ„ŸçŸ¥å…ƒæŸå¤±æ ¡æ­£
  * [Continual Semantic Segmentation via Repulsion-Attraction of Sparse and Disentangled Latent Representations](https://arxiv.org/abs/2103.06342)
  * [Semantic Segmentation for Real Point Cloud Scenes via Bilateral Augmentation and Adaptive Fusion](https://arxiv.org/abs/2103.07074)<br>:star:[code](https://github.com/ShiQiu0419/BAAF-Net)
  * [Rethinking BiSeNet For Real-time Semantic Segmentation]<br>:star:[code](https://github.com/MichaelFan01/STDC-Seg)
  * [BBAM: Bounding Box Attribution Map for Weakly Supervised Semantic and Instance Segmentation](https://arxiv.org/abs/2103.08907)<br>:star:[code](https://github.com/jbeomlee93/BBAM)
  * [Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2103.08896)<br>:star:[code](https://github.com/jbeomlee93/AdvCAM)
  * [Cross-Dataset Collaborative Learning for Semantic Segmentation](https://arxiv.org/abs/2103.11351)
  * [Coarse-to-Fine Domain Adaptive Semantic Segmentation with Photometric Alignment and Category-Center Regularization](https://arxiv.org/abs/2103.13041)
  * [Non-Salient Region Object Mining for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2103.14581)<br>:star:[code](https://github.com/NUST-Machine-Intelligence-Laboratory/nsrom)
  * [Source-Free Domain Adaptation for Semantic Segmentation](https://arxiv.org/abs/2103.16372)
  * [PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering](https://arxiv.org/abs/2103.17070)
  * [Background-Aware Pooling and Noise-Aware Loss for Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2104.00905)<br>:house:[project](https://cvlab.yonsei.ac.kr/projects/BANA/)
* åœºæ™¯ç†è§£/åœºæ™¯è§£æ
  * [Exploring Data Efficient 3D Scene Understanding with Contrastive Scene Contexts](https://arxiv.org/abs/2012.09165)<br>:open_mouth:oral:house:[project](https://sekunde.github.io/project_efficient/):tv:[video](https://youtu.be/E70xToZLgs4)
  * [Monte Carlo Scene Search for 3D Scene Understanding](https://arxiv.org/abs/2103.07969)
  * [Bidirectional Projection Network for Cross Dimension Scene Understanding](https://arxiv.org/abs/2103.14326)<br>:open_mouth:oral:star:[code](https://github.com/wbhu/BPNet)
  * [RobustNet: Improving Domain Generalization in Urban-Scene Segmentation via Instance Selective Whitening](https://arxiv.org/abs/2103.15597)<br>:open_mouth:oral:star:[code](https://github.com/shachoi/RobustNet)
  * åœºæ™¯å›¾åˆæˆ/åˆ†æ
    * [SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences](https://arxiv.org/abs/2103.14898)<br>:house:[project](https://shunchengwu.github.io/SceneGraphFusion)
    * [Probabilistic Modeling of Semantic Ambiguity for Scene Graph Generation](https://arxiv.org/abs/2103.05271)<br>åœºæ™¯å›¾ç”Ÿæˆ---åœºæ™¯è§£æ
    * [Exploiting Edge-Oriented Reasoning for 3D Point-based Scene Graph Analysis](https://arxiv.org/abs/2103.05558)<br>:house:[project](https://sggpoint.github.io/)<br>åˆ©ç”¨é¢å‘è¾¹ç¼˜çš„æ¨ç†è¿›è¡ŒåŸºäº3Dç‚¹çš„åœºæ™¯å›¾åˆ†æ---åœºæ™¯ç†è§£
    * [Fully Convolutional Scene Graph Generation](https://arxiv.org/abs/2103.16083)<br>:open_mouth:oral
    * [Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene Graph Generation](https://arxiv.org/abs/2104.00308)<br>:star:[code](https://github.com/Scarecrow0/BGNN-SGG)
* æŠ å›¾
  * [Real-Time High Resolution Background Matting](https://arxiv.org/abs/2012.07810)<br>:open_mouth:oral:star:[code](https://github.com/PeterL1n/BackgroundMattingV2):house:[project](https://grail.cs.washington.edu/projects/background-matting-v2/):tv:[video](https://youtu.be/oMfPTeYDF9g)<br>æœ€æ–°å¼€æºæŠ å›¾æŠ€æœ¯ï¼Œå®æ—¶å¿«é€Ÿé«˜åˆ†è¾¨ç‡ï¼Œ4k(30fps)ã€ç°ä»£GPUï¼ˆ60fpsï¼‰<br>è§£è¯»ï¼š[å•å—GPUå®ç°4Kåˆ†è¾¨ç‡æ¯ç§’30å¸§ï¼Œåç››é¡¿å¤§å­¦å®æ—¶è§†é¢‘æŠ å›¾å†å‡çº§ï¼Œæ¯›å‘ç»†èŠ‚åˆ°ä½](https://mp.weixin.qq.com/s/0OJR3Y5cPfeHhdTdI3BgEA)<br>[æœ€æ–°å¼€æºæŠ å›¾æŠ€æœ¯ï¼Œå®æ—¶å¿«é€Ÿé«˜åˆ†è¾¨ç‡ï¼Œ4k(30fps)ã€ç°ä»£GPUï¼ˆ60fpsï¼‰](https://zhuanlan.zhihu.com/p/337028483)
* è§†é¢‘åŠ¨ä½œåˆ†å‰²
  * [Global2Local: Efficient Structure Search for Video Action Segmentation](https://arxiv.org/abs/2101.00910)<br>ä»å…¨å±€åˆ°å±€éƒ¨ï¼šé¢å‘è§†é¢‘åŠ¨ä½œåˆ†å‰²çš„é«˜æ•ˆç½‘ç»œç»“æ„æœç´¢<br>è§£è¯»ï¼š[19](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* æ—¶åºåŠ¨ä½œåˆ†å‰²
  * [Temporal Action Segmentation from Timestamp Supervision](https://arxiv.org/abs/2103.06669)<br>:star:[code](https://github.com/ZheLi2020/TimestampActionSeg)
  * [Temporally-Weighted Hierarchical Clustering for Unsupervised Action Segmentation](https://arxiv.org/abs/2103.11264)<br>:star:[code](https://github.com/ssarfraz/FINCH-Clustering/tree/master/TW-FINCH)
* é›·è¾¾åˆ†å‰²
  * [Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR Segmentation](https://arxiv.org/abs/2011.10033)<br>:open_mouth:oral:star:[code](https://github.com/xinge008/Cylinder3D)<br>åœ¨ SemanticKITTI æ¦œå•æ’åç¬¬ä¸€ï¼ˆuntil CVPR DDLï¼‰ï¼Œåœ¨ nuScenes ä¸­è·å¾— SOTAï¼Œå¹¶å¯¹å…¶ä»–åŸºäºæ¿€å…‰é›·è¾¾çš„ä»»åŠ¡ä¿æŒäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¿€å…‰é›·è¾¾å…¨æ™¯åˆ†å‰²å’Œæ¿€å…‰é›·è¾¾ä¸‰ç»´æ£€æµ‹ï¼Œå…¶ä¸­å°±åŸºäºæ­¤å·¥ä½œï¼Œåœ¨ SemanticKITTI å…¨æ™¯åˆ†å‰²æ¦œå•ä¹Ÿæ’åç¬¬ä¸€ã€‚
* è§†é¢‘ç›®æ ‡åˆ†å‰²
  * [Modular Interactive Video Object Segmentation:Interaction-to-Mask, Propagation and Difference-Aware Fusion](https://arxiv.org/abs/2103.07941)<br>:open_mouth:oral:star:[code](https://github.com/hkchengrex/MiVOS):house:[project](https://hkchengrex.github.io/MiVOS/):tv:[video](https://hkchengrex.github.io/MiVOS/video.html)
  * [Learning to Recommend Frame for Interactive Video Object Segmentation in the Wild](https://arxiv.org/abs/2103.10391)<br>:star:[code](https://github.com/svip-lab/IVOS-W)
  * [Efficient Regional Memory Network for Video Object Segmentation](https://arxiv.org/abs/2103.12934)<br>:star:[code](https://github.com/hzxie/RMNet):house:[project](https://infinitescript.com/project/rmnet)
* è§†é¢‘å®ä¾‹åˆ†å‰²
  * [SG-Net: Spatial Granularity Network for One-Stage Video Instance Segmentation](https://arxiv.org/abs/2103.10284)<br>:star:[code](https://github.com/goodproj13/SG-Net):tv:[video](https://www.youtube.com/watch?v=zft0T3YUgpM)<br>æ–‡ç« ä»‹ç»ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å•é˜¶æ®µæ¡†æ¶ï¼šSG-Netï¼Œä¸ä¼ ç»Ÿçš„ä¸¤é˜¶æ®µæ¡†æ¶ç›¸æ¯”ï¼Œå¯ä»¥æœ‰æ•ˆæé«˜æ©ç è´¨é‡å’Œæ¨ç†é€Ÿåº¦ã€‚
* å°æ ·æœ¬åˆ†å‰²
  * [Self-Guided and Cross-Guided Learning for Few-Shot Segmentation](https://arxiv.org/abs/2103.16129)<br>:star:[code](https://github.com/zbf1991/SCL)
  * [Adaptive Prototype Learning and Allocation for Few-Shot Segmentation](https://arxiv.org/abs/2104.01893)<br>:star:[code](https://github.com/Reagan1311/ASGNet)

<a name="7"/> 

## 7.ç›®æ ‡æ£€æµ‹

- [Multiple Instance Active Learning for Object Detection](https://github.com/yuantn/MIAL/raw/master/paper.pdf)<br>:star:[code](https://github.com/yuantn/MIAL)<br>
- Positive-Unlabeled Data Purification in the Wild for Object Detection<br>
- [Depth from Camera Motion and Object Detection](https://arxiv.org/abs/2103.01468)<br>:star:[github](https://github.com/griffbr/ODMD):tv:[video](https://www.youtube.com/watch?v=GruhbdJ2l7k)<br>é€šè¿‡ä½¿ç”¨â€œæ™®é€šæ‰‹æœºæ‘„åƒå¤´è¿åŠ¨+ç›®æ ‡æ£€æµ‹çš„åŒ…å›´æ¡†â€æ•°æ®ï¼Œè®¾è®¡RNNç½‘ç»œå®ç°äº†è¾¾åˆ°æœ€å…ˆè¿›ç²¾åº¦çš„ç›®æ ‡æ·±åº¦ä¼°è®¡ã€‚<br>
- [Towards Open World Object Detection](https://arxiv.org/abs/2103.02603)<br>:open_mouth:oral:star:[code](https://github.com/JosephKJ/OWOD)<br>
- [General Instance Distillation for Object Detection](https://arxiv.org/abs/2103.02340)<br>è¿‘å¹´æ¥ï¼ŒçŸ¥è¯†è’¸é¦å·²è¢«è¯æ˜æ˜¯æ¨¡å‹å‹ç¼©çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚å¯ä»¥ä½¿è½»é‡çº§çš„å­¦ç”Ÿæ¨¡å‹è·å¾—ä»ç¹ççš„æ•™å¸ˆæ¨¡å‹ä¸­æå–çš„çŸ¥è¯†ï¼Œä½†ä»¥å¾€çš„æ£€æµ‹è’¸é¦æ–¹æ³•å¯¹äºä¸åŒçš„æ£€æµ‹æ¡†æ¶çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ï¼Œè€Œä¸”ä¸¥é‡ä¾èµ–ground truthï¼ˆGTï¼‰ï¼Œå¿½ç•¥äº†å®ä¾‹ä¹‹é—´æœ‰ä»·å€¼çš„å…³ç³»ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œä½œè€…åœ¨æœ¬æ–‡ä¸­æå‡ºæ–°çš„åŸºäºåˆ¤åˆ«æ€§å®ä¾‹çš„æ£€æµ‹ä»»åŠ¡è’¸é¦æ–¹æ³•ï¼Œä¸è€ƒè™‘ GT åŒºåˆ†çš„æ­£è´Ÿï¼Œå‘½åä¸ºé€šç”¨å®ä¾‹è’¸é¦ï¼ˆGIDï¼‰ã€‚è¯¥æ–¹æ³•åŒ…å«ä¸€ä¸ªé€šç”¨å®ä¾‹é€‰æ‹©æ¨¡å—(GISM)ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨åŸºäºç‰¹å¾ã€åŸºäºå…³ç³»å’ŒåŸºäºå“åº”çš„çŸ¥è¯†è¿›è¡Œè’¸é¦ã€‚å®éªŒéªŒè¯ï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨å„ç§æ£€æµ‹æ¡†æ¶ä¸­å¯ä»¥å®ç°æ˜¾è‘—çš„ AP æ”¹è¿›ï¼Œç”šè‡³ä¼˜äºæ•™å¸ˆæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒRetinaNet ä¸ ResNet-50 åœ¨ COCO æ•°æ®é›†ä¸Šç”¨ GID å®ç°äº†39.1% çš„ mAPï¼Œæ¯”åŸºçº¿ 36.2% è¶…å‡ºäº† 2.9%ï¼Œç”šè‡³ä¼˜äºåŸºäº ResNet-101 çš„æ•™å¸ˆæ¨¡å‹ 38.1% çš„ APã€‚<br>
- Distilling Object Detectors via Decoupled Features<br>
- [MeGA-CDA: Memory Guided Attention for Category-Aware Unsupervised Domain Adaptive Object Detection](https://arxiv.org/abs/2103.04224)<br>
- Informative and Consistent Correspondence Mining for Cross-Domain Weakly Supervised Object Detection<br>:open_mouth:oral
* [You Only Look One-level Feature](https://arxiv.org/abs/2103.09460)<br>:star:[code](https://github.com/megvii-model/YOLOF)<br>è®ºæ–‡å…¬å¼€<br>[å¼€æº YOLOFï¼Œæ— éœ€ FPNï¼Œé€Ÿåº¦æ¯” YOLOv4 å¿«13%](https://zhuanlan.zhihu.com/p/357986047)<br>è§£è¯»ï¼š[ç›®æ ‡æ£€æµ‹ç®—æ³•YOLOFï¼šYou Only Look One-level Feature](https://mp.weixin.qq.com/s/0ChHOljrqrXk8yWi_S6ITg)
- [Sparse R-CNN: End-to-End Object Detection with Learnable Proposals](https://arxiv.org/abs/2011.12450)<br>:star:[code](https://github.com/PeizeSun/SparseR-CNN)
- [End-to-End Object Detection with Fully Convolutional Network](https://arxiv.org/abs/2012.03544)<br>:star:[code](https://github.com/Megvii-BaseDetection/DeFCN)<br>è§£è¯»ï¼š[ä¸¢å¼ƒTransformerï¼ŒFCNä¹Ÿå¯ä»¥å®ç°E2Eæ£€æµ‹](https://zhuanlan.zhihu.com/p/332281368)
- [Robust and Accurate Object Detection via Adversarial Learning](https://arxiv.org/abs/2103.13886)
* [I^3Net: Implicit Instance-Invariant Network for Adapting One-Stage Object Detectors](https://arxiv.org/abs/2103.13757)
* [Distilling Object Detectors via Decoupled Features](https://arxiv.org/abs/2103.14475)<br>:star:[code](https://github.com/ggjy/DeFeat.pytorch)
* [OTA: Optimal Transport Assignment for Object Detection](https://arxiv.org/abs/2103.14259)<br>:star:[code](https://github.com/Megvii-BaseDetection/OTA)
* [Scale-aware Automatic Augmentation for Object Detection](https://arxiv.org/abs/2103.17220)<br>:star:[code](https://github.com/Jia-Research-Lab/SA-AutoAug) 
* [A Closer Look at Fourier Spectrum Discrepancies for CNN-generated Images Detection](https://arxiv.org/abs/2103.17195)<br>:open_mouth:oral:house:[project](https://keshik6.github.io/Fourier-Discrepancies-CNN-Detection/)
* [Group Collaborative Learning for Co-Salient Object Detection](https://arxiv.org/abs/2104.01108)<br>:star:[code](https://github.com/fanq15/GCoNet)
* [IQDet: Instance-wise Quality Distribution Sampling for Object Detection]<br>ç²—è§£ï¼š[20](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* å°æ ·æœ¬ç›®æ ‡æ£€æµ‹
  * [Semantic Relation Reasoning for Shot-Stable Few-Shot Object Detection](https://arxiv.org/abs/2103.01903)<br>é¦–ä¸ªç ”ç©¶å°‘æ ·æœ¬æ£€æµ‹ä»»åŠ¡çš„è¯­ä¹‰å…³ç³»æ¨ç†ï¼Œå¹¶è¯æ˜å®ƒå¯æå‡å¼ºåŸºçº¿çš„æ½œã€‚ <br> 
  * [Dense Relation Distillation with Context-aware Aggregation for Few-Shot Object Detection](https://arxiv.org/abs/2103.17115)<br>:star:[code](https://github.com/hzhupku/DCNet)<br>åŒ—äº¬å¤§å­¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æœºå™¨å­¦ä¹ ç ”ç©¶ä¸­å¿ƒ<br>
  * [FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding](https://arxiv.org/abs/2103.05950)<br>:star:[code](https://github.com/bsun0802/FSCE)
  * [Generalized Few-Shot Object Detection without Forgetting]<br>ç²—è§£ï¼š[16](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* å¤šç›®æ ‡æ£€æµ‹
  * [There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge](https://arxiv.org/abs/2103.01353)<br>:house:[project](https://rl.uni-freiburg.de/)<br>
* 3Dç›®æ ‡æ£€æµ‹
  * [Categorical Depth Distribution Network for Monocular 3D Object Detection](https://arxiv.org/abs/2103.01100)<br>:open_mouth:oral<br>
  * [3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection](https://arxiv.org/abs/2012.04355)<br>:star:[code](https://github.com/thu17cyz/3DIoUMatch):house:[project](https://thu17cyz.github.io/3DIoUMatch/):tv:[video](https://youtu.be/nuARjhkQN2U)<br>æ›´å¤šï¼š[CVPR 2021|åˆ©ç”¨IoUé¢„æµ‹è¿›è¡ŒåŠç›‘ç£å¼3Dç›®æ ‡æ£€æµ‹](https://zhuanlan.zhihu.com/p/354618636)
  * [ST3D: Self-training for Unsupervised Domain Adaptation on 3D ObjectDetection](https://arxiv.org/abs/2103.05346)<br>:star:[code](https://github.com/CVMI-Lab/ST3D)
  * [Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection](https://arxiv.org/abs/2103.16470)<br>:star:[code](https://github.com/fudan-zvg/DDMP)
  * [MonoRUn: Monocular 3D Object Detection by Self-Supervised Reconstruction and Uncertainty Propagation](https://arxiv.org/abs/2103.12605)<br>:star:[code](https://github.com/tjiiv-cprg/MonoRUn)
  * [M3DSSD: Monocular 3D Single Stage Object Detector](https://arxiv.org/abs/2103.13164)<br>:star:[code](https://github.com/mumianyuxin/M3DSSD)
  * [GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection](https://arxiv.org/abs/2103.17202)<br>:star:[code](https://github.com/abhi1kumar/groomed_nms):tv:[video](https://www.youtube.com/watch?v=PWctKkyWrno)<br>è®ºæ–‡å…¬å¼€
  * [LiDAR R-CNN: An Efficient and Universal 3D Object Detector](https://arxiv.org/abs/2103.15297)<br>:star:[code](https://github.com/tusimple/LiDAR_RCNN)
  * [Exploring intermediate representation for monocular vehicle pose estimation](https://arxiv.org/abs/2011.08464)<br>:star:[code](https://github.com/Nicholasli1995/EgoNet)
  * [Delving into Localization Errors for Monocular 3D Object Detection](https://arxiv.org/abs/2103.16237)<br>:star:[code](https://github.com/xinzhuma/monodle)
  * [HVPR: Hybrid Voxel-Point Representation for Single-stage 3D Object Detection](https://arxiv.org/abs/2104.00902)
* æ—‹è½¬ç›®æ ‡æ£€æµ‹
  * [Dense Label Encoding for Boundary Discontinuity Free Rotation Detection](https://arxiv.org/abs/2011.09670)<br>:star:[code](https://github.com/Thinklab-SJTU/DCL_RetinaNet_Tensorflow)
* ç›®æ ‡å®šä½
  * [Unveiling the Potential of Structure-Preserving for Weakly Supervised Object Localization](https://arxiv.org/abs/2103.04523v1)<br>åŸºäºç»“æ„ä¿¡æ¯ä¿æŒçš„å¼±ç›‘ç£ç›®æ ‡å®šä½<br>è§£è¯»ï¼š[13](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* å¯†é›†ç›®æ ‡æ£€æµ‹
  * [Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection]()<br>:star:[code](https://github.com/implus/GFocalV2)<br>è§£è¯»ï¼š[ç›®æ ‡æ£€æµ‹æ— ç—›æ¶¨ç‚¹ä¹‹ Generalized Focal Loss V2](https://mp.weixin.qq.com/s/H3LuCuqKCUNFldzqiPWQXg)
* RGB-D æ˜¾è‘—ç›®æ ‡æ£€æµ‹
  * [Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion](https://arxiv.org/abs/2103.11832)<br>:open_mouth:oral
* åŠç›‘ç£ç›®æ ‡æ£€æµ‹
 * [Data-Uncertainty Guided Multi-Phase Learning for Semi-Supervised Object Detection](https://arxiv.org/abs/2103.16368)
 * [Points as Queries: Weakly Semi-supervised Object Detection by Points]<br>ç²—è§£ï¼š[6](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* å¼±ç›‘ç£ç›®æ ‡æ£€æµ‹
  * [DAP: Detection-Aware Pre-training with Weak Supervision](https://arxiv.org/abs/2103.16651)
* é•¿å°¾ç›®æ ‡æ£€æµ‹ 
  * [Adaptive Class Suppression Loss for Long-Tail Object Detection](https://arxiv.org/abs/2104.00885)<br>:star:[code](https://github.com/CASIA-IVA-Lab/ACSL)

<a name="6"/> 

## 6.æ•°æ®å¢å¹¿

- [KeepAugment: A Simple Information-Preserving Data Augmentation](https://arxiv.org/pdf/2011.11778.pdf)<br>

<a name="5"/> 

## 5.å¼‚å¸¸æ£€æµ‹

- [Multiresolution Knowledge Distillation for Anomaly Detection](https://arxiv.org/abs/2011.11108)<br>:star:[code](https://github.com/Niousha12/Knowledge_Distillation_AD)

<a name="4"/> 

## 4.è‡ª/åŠ/å¼±ç›‘ç£å­¦ä¹ 

* å¼±ç›‘ç£
  * [Weakly Supervised Learning of Rigid 3D Scene Flow](https://arxiv.org/pdf/2102.08945.pdf)<br>:star:[code](https://arxiv.org/pdf/2102.08945.pdf):house:[project](https://3dsceneflow.github.io/)<br>
  * [Relation-aware Instance Refinement for Weakly Supervised Visual Grounding](https://arxiv.org/abs/2103.12989)<br>:star:[code](https://github.com/youngfly11/ReIR-WeaklyGrounding.pytorch)
* åŠç›‘ç£
  * [Adaptive Consistency Regularization for Semi-Supervised Transfer Learning](https://arxiv.org/abs/2103.02193)<br>:star:[code](https://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning)<br>
  * [SSLayout360: Semi-Supervised Indoor Layout Estimation from 360âˆ˜ Panorama](https://arxiv.org/abs/2103.13696)
* è‡ªç›‘ç£
  * [Self-supervised Geometric Perception](https://arxiv.org/abs/2103.03114)<br>:open_mouth:oral:star:[code](https://github.com/theNded/SGP)<br>ä½œè€…ç§° SGP æ˜¯ç¬¬ä¸€ä¸ªåœ¨å‡ ä½•æ„ŸçŸ¥ä¸­è¿›è¡Œç‰¹å¾å­¦ä¹ çš„é€šç”¨æ¡†æ¶ï¼Œä¸éœ€è¦ä»»ä½•æ¥è‡ª ground-truth å‡ ä½•æ ‡ç­¾çš„ç›‘ç£ã€‚SGPä»¥EMæ–¹å¼è¿è¡Œï¼Œå®ƒè¿­ä»£æ‰§è¡Œå‡ ä½•æ¨¡å‹çš„é²æ£’ä¼°è®¡ä»¥ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶åœ¨å™ªå£°ä¼ªæ ‡ç­¾çš„ç›‘ç£ä¸‹è¿›è¡Œç‰¹å¾å­¦ä¹ ã€‚å°† SGP åº”ç”¨äºç›¸æœºå§¿åŠ¿ä¼°è®¡å’Œç‚¹äº‘é…å‡†ï¼Œå¹¶è¯æ˜åœ¨å¤§è§„æ¨¡çœŸå®æ•°æ®é›†ä¸­ï¼ŒSGP çš„æ€§èƒ½ç­‰åŒäºç”šè‡³ä¼˜äºç›‘ç£çš„æƒå¨ã€‚
  * [Vectorization and Rasterization: Self-Supervised Learning for Sketch and Handwriting](https://arxiv.org/abs/2103.13716)<br>:star:[code](https://github.com/AyanKumarBhunia/Self-Supervised-Learning-for-Sketch)
  * [Self-supervised Motion Learning from Static Images](https://arxiv.org/abs/2104.00240)

<a name="3"/> 

## 3.ç‚¹äº‘

- [Style-based Point Generator with Adversarial Rendering for Point Cloud Completion](https://arxiv.org/abs/2103.02535)<br>
- [MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan Synchronization](https://arxiv.org/abs/2101.06605)<br>:open_mouth:oral:star:[code](https://github.com/huangjh-pub/multibody-sync)
- [TPCN: Temporal Point Cloud Networks for Motion Forecasting](https://arxiv.org/abs/2103.03067)<br>ç”¨äºè¿åŠ¨é¢„æµ‹çš„æ—¶ç©ºç‚¹äº‘ç½‘ç»œ<br>
- [How Privacy-Preserving are Line Clouds? Recovering Scene Details from 3D Lines](https://arxiv.org/abs/2103.05086)<br>:star:[code](https://github.com/kunalchelani/Line2Point)
- [PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds](https://arxiv.org/abs/2103.14635)<br>:star:[code](https://github.com/CVMI-Lab/PAConv)
- [Point2Skeleton: Learning Skeletal Representations from Point Clouds](https://arxiv.org/abs/2012.00230)<br>:open_mouth:oral:star:[code](https://github.com/clinplayer/Point2Skeleton):house:[project](https://enigma-li.github.io/projects/point2skeleton/point2skeleton.html)
- [FESTA: Flow Estimation via Spatial-Temporal Attention for Scene Point Clouds](https://arxiv.org/abs/2104.00798)
* ç‚¹äº‘é…å‡†
  * [PREDATOR: Registration of 3D Point Clouds with Low Overlap](https://arxiv.org/pdf/2011.13005.pdf)<br>:open_mouth:oral:star:[code](https://github.com/ShengyuH/OverlapPredator):house:[project](https://overlappredator.github.io/)<br>
  * [SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration](https://arxiv.org/abs/2011.12149)<br>:star:[code](https://github.com/QingyongHu/SpinNet)
  * [Robust Point Cloud Registration Framework Based on Deep Graph Matching](https://arxiv.org/abs/2103.04256)<br>:star:[code](https://github.com/fukexue/RGM)
  * [PointDSC: Robust Point Cloud Registration using Deep Spatial Consistency](https://arxiv.org/abs/2103.05465)<br>:star:[code](https://github.com/XuyangBai/PointDSC)
  * [ReAgent: Point Cloud Registration using Imitation and Reinforcement Learning](https://arxiv.org/abs/2103.15231)<br>:star:[code](https://github.com/dornik/reagent)
* ç‚¹äº‘è¡¥å…¨
  * [Cycle4Completion: Unpaired Point Cloud Completion using Cycle Transformation with Missing Region Coding](https://arxiv.org/abs/2103.07838)
  * [Denoise and Contrast for Category Agnostic Shape Completion](https://arxiv.org/abs/2103.16671)<br>:star:[code](https://github.com/antoalli/Deco)
* ç‚¹äº‘å…³é”®ç‚¹æ£€æµ‹
  * [Skeleton Merger: an Unsupervised Aligned Keypoint Detector](https://arxiv.org/abs/2103.10814)<br>:star:[code](https://github.com/eliphatfs/SkeletonMerger)
* 3Dç‚¹äº‘
  * [Diffusion Probabilistic Models for 3D Point Cloud Generation](https://arxiv.org/abs/2103.01458)<br>:open_mouth:oral:star:[code](https://github.com/luost26/diffusion-point-cloud)<br>
  * [PointGuard: Provably Robust 3D Point Cloud Classification](https://arxiv.org/abs/2103.03046)
  * [Equivariant Point Network for 3D Point Cloud Analysis](https://arxiv.org/abs/2103.14147)<br>:star:[code](https://github.com/nintendops/EPN_PointCloud)

<a name="2"/> 

## 2.å›¾å·ç§¯ç½‘ç»œGNN

- [Sequential Graph Convolutional Network for Active Learning](https://arxiv.org/pdf/2006.10219.pdf)<br>
- [Quantifying Explainers of Graph Neural Networks in Computational Pathology](https://arxiv.org/abs/2011.12646)<br>
- [Binary Graph Neural Networks](https://arxiv.org/abs/2012.15823)

<a name="1"/> 

## 1.æœªåˆ†ç±»

- Inverting the Inherence of Convolution for Visual Recognition<br>
- Representative Batch Normalization with Feature Calibration<br>
- UC2: Universal Cross-lingual Cross-modal Vision-and-Language Pretraining<br>
- Reconsidering Representation Alignment for Multi-view Clustering<br>
- Self-supervised Simultaneous Multi-Step Prediction of Road Dynamics and Cost Map<br>
- [Instance Localization for Self-supervised Detection Pretraining](https://arxiv.org/pdf/2102.08318.pdf)<br>:star:[code](https://github.com/limbo0000/InstanceLoc)<br>
- Model-Contrastive Federated Learning<br>æå‡ºæ¨¡å‹å¯¹æ¯”å­¦ä¹ æ¥è§£å†³è”åˆå­¦ä¹ ä¸­çš„éIIDæ•°æ®é—®é¢˜<br>
- [Neural Geometric Level of Detail:Real-time Rendering with Implicit 3D Surfaces](https://arxiv.org/abs/2101.10994)<br>:open_mouth:Oral:star:[code](https://github.com/nv-tlabs/nglod):house:[project](https://nv-tlabs.github.io/nglod/)<br>
- [Data-Free Model Extraction](https://arxiv.org/abs/2011.14779)<br>:star:[code](https://github.com/cake-lab/datafree-model-extraction)<br>
- Single-Stage Instance Shadow Detection with Bidirectional Relation Learning<br>:open_mouth:oral<br>:star:[code](https://github.com/stevewongv/SSIS)
- [Continual Adaptation of Visual Representations via Domain Randomization and Meta-learning](https://arxiv.org/abs/2012.04324)<br>:open_mouth:oral
- [PatchmatchNet: Learned Multi-View Patchmatch Stereo](https://arxiv.org/abs/2012.01411)<br>:open_mouth:oral:star:[code](https://github.com/FangjinhuaWang/PatchmatchNet)
- [Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning]
- [Semantic Palette: Guiding Scene Generation with Class Proportions]
- Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer RGBD Sensors<br>:open_mouth:oral
- POSEFusion:Pose-guided Selective Fusion for Single-view Human Volumetric Capture<br>:open_mouth:oral
- [Multi-Objective Interpolation Training for Robustness to Label Noise](https://arxiv.org/abs/2012.04462)<br>:star:[code](https://github.com/DiegoOrtego/LabelNoiseMOIT)
- [Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations](https://arxiv.org/abs/2011.12854)<br>:star:[code](https://github.com/ml-research/NeSyXIL)
- Simpler Certified Radius Maximization by Propagating Covariances<br>:open_mouth:oral:tv:[video](https://www.youtube.com/watch?v=1V9sBzlfuwY)
- [Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food](https://arxiv.org/abs/2103.03375)
- [Discovering Hidden Physics Behind Transport Dynamics](https://arxiv.org/abs/2011.12222)<br>:open_mouth:oral
- [Soft-IntroVAE: Analyzing and Improving the Introspective Variational Autoencoder](https://arxiv.org/abs/2012.13253)<br>:open_mouth:oral:star:[code](https://github.com/taldatech/soft-intro-vae-pytorch):house:[project](https://taldatech.github.io/soft-intro-vae-web/)
- [Deep Gradient Projection Networks for Pan-sharpening](https://arxiv.org/abs/2103.04584)<br>:star:[code](https://github.com/xsxjtu/GPPNN)
- [Consensus Maximisation Using Influences of Monotone Boolean Functions](https://arxiv.org/abs/2103.04200)<br>:open_mouth:oral
* Forecasting Irreversible Disease via Progression Learning
* Causal Hidden Markov Model for Time Series Disease Forecasting
* Towards Unified Surgical Skill Assessment
- [Knowledge Evolution in Neural Networks](https://arxiv.org/abs/2103.05152)<br>:open_mouth:oral:star:[code](https://github.com/ahmdtaha/knowledge_evolution)
* RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words<br>RSTNet: åŸºäºå¯åŒºåˆ†è§†è§‰è¯å’Œéè§†è§‰è¯çš„è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶çš„å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹<br>è§£è¯»ï¼š[14](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* [Removing the Background by Adding the Background: Towards a Background Robust Self-supervised Video Representation Learning](https://arxiv.org/abs/2009.05769)<br>é€šè¿‡æ·»åŠ èƒŒæ™¯æ¥å»é™¤èƒŒæ™¯å½±å“ï¼šèƒŒæ™¯é²æ£’çš„è‡ªç›‘ç£è§†é¢‘è¡¨å¾å­¦ä¹ <br>è§£è¯»ï¼š[11](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* Representative Batch Normalization with Feature Calibration<br>:open_mouth:oral<br>[ä½œè€…ä¸»é¡µ](https://duoli.org/)<br>åŸºäºç‰¹å¾æ ¡å‡†çš„è¡¨å¾æ‰¹è§„èŒƒåŒ–æ–¹æ³•è§£è¯»ï¼š[4](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* Learning Compositional Representation for 4D Captures with Neural ODE
* [Involution: Inverting the Inherence of Convolution for Visual Recognition](https://arxiv.org/abs/2103.06255)<br>:star:[code](https://github.com/d-li14/involution)<br>è§£è¯»ï¼š[CVPR'21 | involutionï¼šè¶…è¶Šconvolutionå’Œself-attentionçš„ç¥ç»ç½‘ç»œæ–°ç®—å­](https://mp.weixin.qq.com/s/Kn7QJdldLhyBfYS1KiCdcA)
* [Spatially Consistent Representation Learning](https://arxiv.org/abs/2103.06122)
* [Limitations of Post-Hoc Feature Alignment for Robustness](https://arxiv.org/abs/2103.05898)
* [AutoDO: Robust AutoAugment for Biased Data with Label Noise via Scalable Probabilistic Implicit Differentiation](https://arxiv.org/abs/2103.05863)<br>:star:[code](https://github.com/gudovskiy/autodo)
* CFNet: Cascade and Fused Cost Volume for Robust Stereo Matching<br>:star:[code](https://github.com/gallenszl/CFNet)
* [Augmentation Strategies for Learning with Noisy Labels](https://arxiv.org/abs/2103.02130)<br>:star:[code](https://github.com/KentoNishi/Augmentation-for-LNL)
* CFNet: Cascade and Fused Cost Volume for Robust Stereo Matching<br>:star:[code](https://github.com/gallenszl/CFNet)
* [Augmentation Strategies for Learning with Noisy Labels](https://arxiv.org/abs/2103.02130)<br>:star:[code](https://github.com/KentoNishi/Augmentation-for-LNL)
* [PGT: A Progressive Method for Training Models on Long Videos](https://arxiv.org/abs/2103.11313)<br>:open_mouth:oral:star:[code](https://github.com/BoPang1996/PGT)
* [Generic Perceptual Loss for Modeling Structured Output Dependencies](https://arxiv.org/abs/2103.10571)
* [Masksembles for Uncertainty Estimation](https://arxiv.org/abs/2012.08334)<br>:star:[code](https://github.com/nikitadurasov/masksembles):house:[project](https://nikitadurasov.github.io/projects/masksembles/)
* [Student-Teacher Learning from Clean Inputs to Noisy Inputs](https://arxiv.org/abs/2103.07600)
* [Scene-Intuitive Agent for Remote Embodied Visual Grounding](https://arxiv.org/abs/2103.12944)
* [Meta-Mining Discriminative Samples for Kinship Verification](https://arxiv.org/abs/2103.15108)<br>
* [Learning Probabilistic Ordinal Embeddings for Uncertainty-Aware Regression](https://arxiv.org/abs/2103.13629)<br>:star:[code](https://github.com/Li-Wanhua/POEs):tv:[video](https://www.youtube.com/watch?v=zCTPRxxlZsI&t=427s)<br>è®ºæ–‡å…¬å¼€
* [Diverse Branch Block: Building a Convolution as an Inception-like Unit](https://arxiv.org/abs/2103.13425)<br>:star:[code](https://github.com/DingXiaoH/DiverseBranchBlock)
* [OTCE: A Transferability Metric for Cross-Domain Cross-Task Representations](https://arxiv.org/abs/2103.13843)
* [Disentangled Cycle Consistency for Highly-realistic Virtual Try-On](https://arxiv.org/abs/2103.09479)<br>:star:[code](https://github.com/ChongjianGE/DCTON)
* [Stylized Neural Painting](https://arxiv.org/abs/2011.08114)<br>:star:[code](https://github.com/jiupinjia/stylized-neural-painting):house:[project](https://jiupinjia.github.io/neuralpainter/):tv:[video](https://youtu.be/oerb-nwrXhk)<br>é£æ ¼åŒ–çš„ç¥ç»ç»˜ç”»,Stylized Neural Painting,æå‡º image-to-painting ç¿»è¯‘æ–¹æ³•ï¼Œç”Ÿæˆç”ŸåŠ¨é€¼çœŸã€é£æ ¼å¯æ§çš„ç»˜ç”»è‰ºæœ¯ä½œå“ 
* [Confluent Vessel Trees with Accurate Bifurcations](https://arxiv.org/abs/2103.14268)<br>:star:[code](https://vision.cs.uwaterloo.ca/code/)
* [Repopulating Street Scenes](https://arxiv.org/abs/2103.16183)
* Extreme Rotation Estimation using Dense Correlation Volumes
* Can We Characterize Tasks Without Labels or Features?
* [Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding](https://arxiv.org/abs/2103.16848)
* [Online Learning of a Probabilistic and Adaptive Scene Representation](https://arxiv.org/abs/2103.16832)
* [Generative Modelling of BRDF Textures from Flash Images](https://arxiv.org/abs/2102.11861)<br>:star:[code](https://github.com/henzler/neuralmaterial):house:[project](https://henzler.github.io/publication/neuralmaterial/)
* [PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting](https://arxiv.org/abs/2104.00674)<br>:house:[project](https://kai-46.github.io/PhySG-website/)
* [Self-supervised Video Representation Learning by Context and Motion Decoupling](https://arxiv.org/abs/2104.00862)
* [Dynamic Region-Aware Convolution](https://arxiv.org/abs/2003.12243)<br>ç²—è§£ï¼š[14](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)

<a name="*"/>

## Workshop å¾ç¨¿ing

 * [Transport Challenge with ThreeDWorld](http://tdw-transport.csail.mit.edu/)<br>:tv:[video](https://youtu.be/WRx41xsB_wo):warning:6æœˆ1æˆªæ­¢
   * è®ºæ–‡ï¼š[The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion Planning Benchmark for Physically Realistic Embodied AI](https://arxiv.org/abs/2103.14025)

- [Visual Perception for Navigation in Human Environments](https://jrdb.stanford.edu/workshops/jrdb-cvpr21)<br>ç¬¬äºŒå±Šäººç±»ç¯å¢ƒå¯¼èˆªè§†è§‰æ„ŸçŸ¥å¾ç¨¿ :warning:4æœˆ15æˆªæ­¢
- [UG 2 + Challenge](http://cvpr2021.ug2challenge.org/index.html)<br>æ—¨åœ¨é€šè¿‡åº”ç”¨å›¾åƒæ¢å¤å’Œå¢å¼ºç®—æ³•æé«˜åˆ†ææ€§èƒ½ï¼Œæ¨åŠ¨å¯¹ "difficult"å›¾åƒçš„åˆ†æã€‚å‚ä¸è€…ä»»åŠ¡æ˜¯å¼€å‘æ–°çš„ç®—æ³•ï¼Œä»¥æ”¹è¿›å¯¹åœ¨é—®é¢˜æ¡ä»¶ä¸‹æ‹æ‘„çš„å›¾åƒåˆ†æã€‚<br>:crown:10Kç¾å…ƒå¥–é‡‘<br>
   * [ä½èƒ½è§åº¦ç¯å¢ƒä¸‹çš„ç›®æ ‡æ£€æµ‹](https://www.deepl.com/translator#en/zh/OBJECT%20DETECTION%20IN%20POOR%20VISIBILITY%20ENVIRONMENTS)
      * é›¾éœ¾æ¡ä»¶ä¸‹çš„(åŠ)ç›‘ç£ç›®æ ‡æ£€æµ‹
      * (åŠ)ä½å…‰æ¡ä»¶ä¸‹çš„äººè„¸æ£€æµ‹
   * [é»‘æš—è§†é¢‘ä¸­çš„åŠ¨ä½œè¯†åˆ«](http://cvpr2021.ug2challenge.org/track2.html)
      * é»‘æš—ä¸­è¿›è¡Œå®Œå…¨ç›‘ç£åŠ¨ä½œè¯†åˆ«
      * é»‘æš—ä¸­è¿›è¡ŒåŠç›‘ç£åŠ¨ä½œè¯†åˆ«

- [Continual Learning in Computer Vision å¾ç¨¿ä¸­](https://sites.google.com/view/clvision2021/overview?authuser=0)<br>æ—¨åœ¨èšé›†å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆï¼Œè®¨è®ºæŒç»­å­¦ä¹ çš„æœ€æ–°è¿›å±•ã€‚<br>
  * Best paper award: 					500 USD + 500 USD worth of Huawei cloud credits (HUAWEI)
  * Overall Challenge winner: 				1,000 USD  + 500 USD worth of Huawei cloud credits (HUAWEI)
  * Supervised-Learning track winner: 		500 USD (HUAWEI)
  * Reinforcement-Learning track winner: 	500 USD (ServiceNow)

- [ç¬¬å››å±ŠUG2ç ”è®¨ä¼šå’Œç«èµ›ï¼šå¼¥åˆè®¡ç®—æˆåƒä¸è§†è§‰è¯†åˆ«ä¹‹é—´çš„é¸¿æ²Ÿ](https://mp.weixin.qq.com/s/u7YDi7tAsDn77P0hb-dmFg)

- [10ä¸‡ç¾å…ƒå¥–é‡‘ï¼CVPR 2021 é‡ç£…èµ›äº‹ï¼Œå®‰å…¨AIæŒ‘æˆ˜è€…è®¡åˆ’](https://mp.weixin.qq.com/s/1d8RL9_YqKp9stVf2M1N-Q)
  * [CVPR 2021å¤§èµ›ï¼Œ å®‰å…¨AI ä¹‹é˜²å¾¡æ¨¡å‹çš„ã€Œç™½ç›’å¯¹æŠ—æ”»å‡»ã€è§£æ](https://mp.weixin.qq.com/s/OJ9_xaQ6GyuCUrph8boxYA)
  * [è¿˜åœ¨åˆ·æ¦œImageNetï¼Ÿæ‰¾å‡ºæ¨¡å‹çš„è„†å¼±ä¹‹å¤„æ›´æœ‰ä»·å€¼ï¼](https://mp.weixin.qq.com/s/nbAudiGJX_l69zaSEiWJFA)

- [Responsible Computer Vision](https://sites.google.com/view/rcv-cvpr2021/home)<br>:warning:3æœˆ25æ—¥æˆªæ­¢<br>æœ¬æ¬¡ç ”è®¨ä¼šå°†å¹¿æ³›è®¨è®ºè®¡ç®—æœºè§†è§‰èƒŒæ™¯ä¸‹è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½çš„ä¸‰ä¸ªä¸»è¦æ–¹é¢ï¼šå…¬å¹³æ€§ï¼›å¯è§£é‡Šæ€§å’Œé€æ˜åº¦ï¼›ä»¥åŠéšç§ã€‚
- [Holistic Video Understanding](https://holistic-video-understanding.github.io/workshops/cvpr2021.html)<br>ç›®çš„æ˜¯å»ºç«‹ä¸€ä¸ªæ•´åˆæ‰€æœ‰è¯­ä¹‰æ¦‚å¿µè”åˆè¯†åˆ«çš„è§†é¢‘åŸºå‡†ï¼Œå› ä¸ºæ¯ä¸ªä»»åŠ¡çš„å•ä¸€ç±»æ ‡ç­¾å¾€å¾€ä¸è¶³ä»¥æè¿°è§†é¢‘çš„æ•´ä½“å†…å®¹ã€‚
- [ThreeDWorld Transport Challenge](http://tdw-transport.csail.mit.edu/)<br>:warning:6æœˆ1æˆªæ­¢<br>:tv:[video](https://www.youtube.com/watch?v=WcJTpiRC4Zo)
- [FGVC 8](https://sites.google.com/view/fgvc8)<br>ç¬¬å…«å±Šç»†ç²’åº¦è§†è§‰åˆ†ç±»ç ”è®¨ä¼šï¼ˆFGVC8ï¼‰å°†é€šè¿‡ç»†ç²’åº¦è§†è§‰ç†è§£çš„è§†è§’ï¼Œæ¢è®¨ç»†ç²’åº¦å­¦ä¹ ã€è‡ªç›‘ç£å­¦ä¹ ã€åŠç›‘ç£å­¦ä¹ ã€matching(åŒ¹é…)ã€localization(å®šä½)ã€åŸŸé€‚åº”ã€è¿ç§»å­¦ä¹ ã€å°æ ·æœ¬å­¦ä¹ ã€æœºå™¨æ•™å­¦ã€å¤šæ¨¡æ€å­¦ä¹ ï¼ˆå¦‚éŸ³é¢‘å’Œè§†é¢‘ï¼‰ã€ä¼—åŒ…å’Œåˆ†ç±»å­¦é¢„æµ‹ç­‰ç›¸å…³è¯é¢˜ã€‚<br>:warning:[è®ºæ–‡æˆªç¨¿æ—¥æœŸ](https://sites.google.com/view/fgvc8/submission)ä¸º4æœˆ2æ—¥<br>[å¾ç¨¿ä¸»é¢˜åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ–¹é¢](https://sites.google.com/view/fgvc8/submission)
  * Fine-grained categorizationç»†ç²’åº¦åˆ†ç±»
    * Novel datasets and data collection strategies for fine-grained categorizationç”¨äºç»†ç²’åº¦åˆ†ç±»çš„æ–°å‹æ•°æ®é›†å’Œæ•°æ®æ”¶é›†ç­–ç•¥
    * Appropriate error metrics for fine-grained categorizationç»†ç²’åº¦åˆ†ç±»çš„é€‚å½“è¯¯å·®æŒ‡æ ‡
    * Low/few shot learningå°‘/å°æ ·æœ¬å­¦ä¹ 
    * Self-supervised learningè‡ªç›‘ç£å­¦ä¹ 
    * Semi-supervised learningåŠç›‘ç£å­¦ä¹ 
    * Transfer-learning from known to novel subcategories
    * Attribute and part based approaches
    * Taxonomic predictions
    * Addressing long-tailed distributions
  * Human-in-the-loop
    * Fine-grained categorization with humans in the loop
    * Embedding human expertsâ€™ knowledge into computational models
    * Machine teaching
    * Interpretable fine-grained models
  * Multi-modal learning
    * Using audio and video data
    * Using geographical priors
    * Learning shape
  * Fine-grained applications
    * Product recognition 
    * Animal biometrics and camera traps 
    * Museum collections
    * Agricultural 
    * Medical 
    * Fashion
  * ç›¸å…³æŒ‘æˆ˜èµ›å¦‚ä¸‹ï¼ˆéƒ¨åˆ†å·²åœ¨Kaggleç½‘ç«™å¼€å§‹ï¼‰
    * [GeoLifeCLEF2021](https://www.kaggle.com/c/geolifeclef-2021)<br>åˆ©ç”¨è§‚æµ‹ç»“æœä¸èˆªç©ºå›¾åƒå’Œç¯å¢ƒç‰¹å¾é…å¯¹ï¼Œé¢„æµ‹ç‰©ç§çš„å­˜åœ¨
    * [Semi-iNat2021](https://www.kaggle.com/c/semi-inat-2021)<br>ç”±iNaturalistçš„æ•°æ®ç»„æˆçš„åŠç›‘ç£ç»†ç²’åº¦å›¾åƒåˆ†ç±»
    * [iNatChallenge2021](https://www.kaggle.com/c/inaturalist-2021)<br>å¯¹1ä¸‡ç±»åŠ¨æ¤ç‰©è¿›è¡Œå›¾åƒåˆ†ç±»æŒ‘æˆ˜èµ›
    * [iMet2021](https://www.kaggle.com/c/imet-2021-fgvc8)<br>å¯¹è‰ºæœ¯å“è¿›è¡Œç»†ç²’åº¦å±æ€§åˆ†ç±»
    * [iMat-Fashion2021](æœªå¼€å§‹)æœªå¼€å§‹<br>æœè£…å®ä¾‹åˆ†å‰²å’Œç»†ç²’åº¦å±æ€§åˆ†ç±»
    * [Hotel-ID 2021](https://www.kaggle.com/c/hotel-id-2021-fgvc8)<br>ä»å›¾åƒä¸­è¯†åˆ«é…’åº—æˆ¿é—´
    * [HerbariumChallenge2021](https://www.kaggle.com/c/herbarium-2021-fgvc8)<br>ä»æ•°æ®é›†ä¸­è¯†åˆ«æ ‡æœ¬ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªç¾æ´²ã€å¤§æ´‹æ´²å’Œå¤ªå¹³æ´‹åœ°åŒºçš„è¿‘66,000ç§ vascular plant speciesï¼ˆç»´ç®¡æŸæ¤ç‰©ï¼‰çš„ 2.5M å›¾åƒ
    * [iWildCam2021](https://www.kaggle.com/c/iwildcam2021-fgvc8)<br>å¯¹å›¾åƒåºåˆ—ä¸­æ¯ä¸ªç‰©ç§çš„åŠ¨ç‰©æ•°é‡è®¡æ•°
    * [PlantPathologyChallenge2021](æœªå¼€å§‹)æœªå¼€å§‹<br>å¯¹ç—…å®³æ¤ç‰©çš„å›¾åƒè¿›è¡Œåˆ†ç±»
    


## æ‰«ç CVå›å¾®ä¿¡ï¼ˆæ³¨æ˜ï¼šCVPRï¼‰å…¥å¾®ä¿¡äº¤æµç¾¤ï¼š

![image](https://user-images.githubusercontent.com/62801906/109789529-655e4380-7c4b-11eb-9f1a-58c5cb097428.png)
