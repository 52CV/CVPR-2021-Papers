# CVPR2021æœ€æ–°ä¿¡æ¯åŠå·²æ¥æ”¶è®ºæ–‡/ä»£ç (æŒç»­æ›´æ–°)


å®˜ç½‘é“¾æ¥ï¼šhttp://cvpr2021.thecvf.com<br>
å¼€ä¼šæ—¶é—´ï¼š2021å¹´6æœˆ19æ—¥-6æœˆ25æ—¥<br>
è®ºæ–‡æ¥æ”¶å…¬å¸ƒæ—¶é—´ï¼š2021å¹´2æœˆ28æ—¥<br>

æ¥æ”¶è®ºæ–‡IDsï¼š<br>

* [CVPR 2021 æ¥æ”¶è®ºæ–‡åˆ—è¡¨ï¼27%æ¥å—ç‡ï¼](https://zhuanlan.zhihu.com/p/353686917)

# :exclamation::exclamation::exclamation:ğŸŒŸğŸŒŸğŸŒŸ CVPR 2021 æ”¶å½•è®ºæ–‡å·²å…¨éƒ¨å…¬å¸ƒï¼Œä¸‹è½½å¯åœ¨ã€æˆ‘çˆ±è®¡ç®—æœºè§†è§‰ã€‘åå°å›å¤â€œCVPR2021â€ï¼Œå³å¯æ”¶åˆ°ã€‚å…±è®¡ 1660 ç¯‡ã€‚

# :exclamation::exclamation::exclamation:ğŸŒŸğŸŒŸğŸŒŸ å…¨éƒ¨è®ºæ–‡å·²ç²—ç•¥åˆ†ç±»å®Œæ¯•ï¼Œè¯·æŸ¥é˜…ã€‚

### :exclamation::exclamation::exclamation:æ³¨ï¼šåç»­è®ºæ–‡çš„ç»†è‡´åˆ†ç±»æ±‡æ€»å°†å‘å¸ƒåœ¨å…¬ä¼—å·ã€OpenCVä¸­æ–‡ç½‘ã€‘ï¼Œæ•¬è¯·å…³æ³¨ã€‚


# ç›®å½•

|:dog:|:mouse:|:hamster:|:tiger:|
|------|------|------|------|
|[73.Object Re-identification(ç‰©ä½“é‡è¯†åˆ«)](#73)|[72.Gaze Estimation(è§†çº¿ä¼°è®¡)](#72)|[71.Image-to-Image Translation(å›¾åƒåˆ°å›¾åƒç¿»è¯‘)](#71)|[70.NLP(è‡ªç„¶è¯­è¨€å¤„ç†)](#70)|[69.Transfer learning(è¿ç§»å­¦ä¹ )](#69)|
|[68.Crowd Counting(è®¡æ•°)](#68)|[67.Defect Detection(ç¼ºé™·æ£€æµ‹)](#67)|[66.Optical Flow Estimation(å…‰æµä¼°è®¡)](#66)|[65.Style Transfer(é£æ ¼è¿ç§»)](#65)
|[64.Speech processing(è¯­éŸ³å¤„ç†)](#64)|[63.Image Processing(å›¾åƒå¤„ç†)](#63)|[62.Free-Hand Sketches(æ‰‹ç»˜è‰å›¾è¯†åˆ«)](#62)|[61.ç®—æ³•](#61)|
|[60. SLAM/AR/æœºå™¨äºº](#60)|[59.æ·±åº¦å­¦ä¹ æ¨¡å‹](#59)|[58.Metric Learning(åº¦é‡å­¦ä¹ /ç›¸ä¼¼åº¦å­¦ä¹ )](#58)|[57.Sign Language Recognition(æ‰‹è¯­è¯†åˆ«)](#57)|
|[56.Computational Photography(å…‰å­¦ã€å‡ ä½•ã€å…‰åœºæˆåƒã€è®¡ç®—æ‘„å½±)](#56)|[55.Graph Matching(å›¾åŒ¹é…)](#55)|[54.Emotion Perception(æƒ…ç»ªæ„ŸçŸ¥/æƒ…æ„Ÿé¢„æµ‹)](#54)|[53.Dataset(æ•°æ®é›†)](#53)|
|[52. Image Generation/Synthesis(å›¾åƒç”Ÿæˆ)](#52)|[51.Contrastive Learning(å¯¹æ¯”å­¦ä¹ )](#51)|[50.OCR](#50)|[49.Adversarial Learning(å¯¹æŠ—å­¦ä¹ )](#49)|
|[48.Image Representation(å›¾åƒè¡¨ç¤º)](#48)|[47.Vision-Language(è§†è§‰è¯­è¨€)](#47)|[46.Human-Object Interaction(äººç‰©äº¤äº’)](#46)|[45.Camera Localization(ç›¸æœºå®šä½)](#45)|
|[44. Image/video Captioning(å›¾åƒ/è§†é¢‘å­—å¹•)](#44)|[43.Active Learning(ä¸»åŠ¨å­¦ä¹ )](#43)|[42.Scene Flow Estimation(åœºæ™¯æµä¼°è®¡)](#42)|[41. Representation Learning(è¡¨ç¤ºå­¦ä¹ ï¼ˆå›¾åƒ+å­—å¹•ï¼‰)](#41)|
|[40.Superpixel (è¶…åƒç´ )](#40)|[39.Debiasing(å»åè§)](#39)|[38.Class-Incremental learning(ç±»å¢é‡å­¦ä¹ )](#38)|[37.Continual Learning(æŒç»­å­¦ä¹ )](#37)|
|[36.Action Detection and Recognition(åŠ¨ä½œæ£€æµ‹ä¸è¯†åˆ«)](#36)|[35.Image Clustering(å›¾åƒèšç±») ](#35)|[34.Image/Fine-Grained Classification(å›¾åƒåˆ†ç±»/ç»†ç²’åº¦åˆ†ç±»)](#34)|[33.6D Pose Estimation(6Dä½å§¿ä¼°è®¡)](#33)|
|[32.View Synthesis(è§†å›¾åˆæˆ)](#32)|[31.Open-Set Recognition(å¼€æ”¾é›†è¯†åˆ«)](#31)|[30.Neural rendering(ç¥ç»æ¸²æŸ“)](#30)|[29.Human Pose Estimation(äººä½“å§¿æ€ä¼°è®¡)](#29)|
|[28.Dense prediction(å¯†é›†é¢„æµ‹)](#28)|[27.Semantic Line Detection(è¯­ä¹‰çº¿æ£€æµ‹)](#27)|[26.Video Processing(è§†é¢‘ç›¸å…³æŠ€æœ¯)](#26)|[25.3D(ä¸‰ç»´è§†è§‰)](#25)|
|[24.Reinforcement Learning(å¼ºåŒ–å­¦ä¹ )](#24)|[23.Autonomous Driving(è‡ªåŠ¨é©¾é©¶)](#23)|[22.Medical Imaging(åŒ»å­¦å½±åƒ)](#22)|[21.Transformer/Self-attention](#21)|
|[20.Person Re-Identification(äººå‘˜é‡è¯†åˆ«)](#20)|[19.Quantization/Pruning/Knowledge Distillation/Model Compression(é‡åŒ–ã€å‰ªæã€è’¸é¦ã€æ¨¡å‹å‹ç¼©/æ‰©å±•ä¸ä¼˜åŒ–)](#19)|[18.Aeria/Drones/Satellite/RS Image(èˆªç©ºå½±åƒ/æ— äººæœº)](#18)|[17.Super-Resolution(è¶…åˆ†è¾¨ç‡)](#17)|
|[16.Visual Question Answering(è§†è§‰é—®ç­”)](#16)|[15.GAN](#15)|[14.Few-Shot/Zero-Shot Learning,Domain Generalization/Adaptation(å°/é›¶æ ·æœ¬å­¦ä¹ ï¼ŒåŸŸé€‚åº”ï¼ŒåŸŸæ³›åŒ–)](#14)|[13.Image/Video Retrieval(å›¾åƒ/è§†é¢‘æ£€ç´¢)](#13)|
|[12.Image Quality Assessment(å›¾åƒè´¨é‡è¯„ä¼°)](#12)|[11. Face(äººè„¸æŠ€æœ¯)](#11)|[10.Neural Architecture Search(ç¥ç»æ¶æ„æœç´¢)](#10)|[9.Object Tracking(ç›®æ ‡è·Ÿè¸ª)](#9)
|[8.Image Segmentation(å›¾åƒåˆ†å‰²)](#8)|[7.Object Detection(ç›®æ ‡æ£€æµ‹)](#7)|[6.Data Augmentation(æ•°æ®å¢å¹¿)](#6)|[5.Anomaly Detection(å¼‚å¸¸æ£€æµ‹)](#5)|
|[4.Weakly Supervised/Semi-Supervised/Self-supervised/Unsupervised Learning(è‡ª/åŠ/å¼±ç›‘ç£å­¦ä¹ )](#4)|[3.Point Cloud(ç‚¹äº‘)](#3)|[2.Graph Neural Networks(å›¾å·ç§¯ç½‘ç»œGNN)](#2)|[1.Unkown(æœªåˆ†ç±»)](#1)|


<a name="74"/>

## 74.Place Recognition(ä½ç½®è¯†åˆ«)
  * [SOE-Net: A Self-Attention and Orientation Encoding Network for Point Cloud Based Place Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Xia_SOE-Net_A_Self-Attention_and_Orientation_Encoding_Network_for_Point_Cloud_CVPR_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/Yan-Xia/SOE-Net)

<a name="73"/>

## 73.Object Re-identification(ç‰©ä½“é‡è¯†åˆ«)
  * [Refining Pseudo Labels with Clustering Consensus over Generations for Unsupervised Object Re-identification](https://arxiv.org/abs/2106.06133)

<a name="72"/>

## 72.Gaze Estimation(è§†çº¿ä¼°è®¡)
* [Weakly-Supervised Physically Unconstrained Gaze Estimation](https://arxiv.org/abs/2105.09803)<br>:open_mouth:oral:star:[code](https://github.com/NVlabs/weakly-supervised-gaze)
* Gaze ç›®æ ‡æ£€æµ‹
  * [Dual Attention Guided Gaze Target Detection in the Wild](https://openaccess.thecvf.com/content/CVPR2021/papers/Fang_Dual_Attention_Guided_Gaze_Target_Detection_in_the_Wild_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/Crystal2333/DAM)
 
<a name="71"/>

## 71.Image-to-Image Translation(å›¾åƒåˆ°å›¾åƒç¿»è¯‘)
* [High-Resolution Photorealistic Image Translation in Real-Time: A Laplacian Pyramid Translation Network](https://arxiv.org/abs/2105.09188)<br>:star:[code](https://github.com/csjliang/LPTN)
* [CoCosNet v2: Full-Resolution Correspondence Learning for Image Translation](https://arxiv.org/abs/2012.02047)<br>:open_mouth:oral:house:[project](https://tmux.top/publication/geosim/)<br>è§£è¯»ï¼š[CoCosNet v2è§£é”â€œé«˜é…ç‰ˆâ€å›¾åƒç¿»è¯‘](https://mp.weixin.qq.com/s/UIxdBXGN7sO8m01Q83PLew)
* [Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation](https://arxiv.org/abs/2106.09016)
* [Saliency-Guided Image Translation](https://openaccess.thecvf.com/content/CVPR2021/papers/Jiang_Saliency-Guided_Image_Translation_CVPR_2021_paper.pdf)
* [Not Just Compete, but Collaborate: Local Image-to-Image Translation via Cooperative Mask Prediction](https://openaccess.thecvf.com/content/CVPR2021/papers/Kim_Not_Just_Compete_but_Collaborate_Local_Image-to-Image_Translation_via_Cooperative_CVPR_2021_paper.pdf) 
* [Unpaired Image-to-Image Translation via Latent Energy Transport](https://arxiv.org/abs/2012.00649)<br>:star:[code](https://github.com/YangNaruto/latent-energy-transport)
* å›¾åƒç¿»è¯‘
  * [Unbalanced Feature Transport for Exemplar-Based Image Translation](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhan_Unbalanced_Feature_Transport_for_Exemplar-Based_Image_Translation_CVPR_2021_paper.pdf)
  * [The Spatially-Correlative Loss for Various Image Translation Tasks](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_The_Spatially-Correlative_Loss_for_Various_Image_Translation_Tasks_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/lyndonzheng/F-LSeSim):house:[project](http://www.chuanxiaz.com/publication/flsesim/):tv:[video](https://www.youtube.com/watch?v=pu6PT1om2r0)


<a name="70"/>

## 70.NLP(è‡ªç„¶è¯­è¨€å¤„ç†)

  * [Learning Graphs for Knowledge Transfer With Limited Labels](https://openaccess.thecvf.com/content/CVPR2021/papers/Ghosh_Learning_Graphs_for_Knowledge_Transfer_With_Limited_Labels_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/pallabig/LearningGraphsForGCN):house:[project](https://pallabig.github.io/LearningGraphsForGCN/)


<a name="69"/>

## 69.Transfer learning(è¿ç§»å­¦ä¹ )
* åŸŸè¿ç§»
  * [Visualizing Adapted Knowledge in Domain Transfer](https://arxiv.org/abs/2104.10602)<br>:star:[code](https://github.com/hou-yz/DA_visualization) 


<a name="68"/>

## 68.Crowd Counting(è®¡æ•°)
  * [Learning To Count Everything](https://arxiv.org/abs/2104.08391)<br>:star:[code](https://github.com/cvlab-stonybrook)

<a name="67"/>

## 67.Defect Detection(ç¼ºé™·æ£€æµ‹)
  * [CutPaste: Self-Supervised Learning for Anomaly Detection and Localization](https://arxiv.org/abs/2104.04015)

<a name="66"/>

## 66.Optical Flow Estimation(å…‰æµä¼°è®¡)
* [UPFlow:Upsampling Pyramid for Unsupervised Optical Flow Learning](https://arxiv.org/abs/2012.00212)<br>ç²—è§£ï¼š[8](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* [Learning Optical Flow from a Few Matches](https://arxiv.org/abs/2104.02166)<br>:star:[code](https://github.com/zacjiang/scv)
* [Learning optical flow from still images](https://arxiv.org/abs/2104.03965)<br>:star:[code](https://github.com/mattpoggi/depthstillation):house:[project](https://mattpoggi.github.io/projects/cvpr2021aleotti/)
* [AutoFlow: Learning a Better Training Set for Optical Flow](https://arxiv.org/abs/2104.14544)<br>:house:[project](https://autoflow-google.github.io/)<br>AutoFlow ï¼šCVPR 2021 Oral ,ä½œè€…å‘æ˜äº†ä¸€ç§ä¸“ä¸ºå…‰æµç®—æ³•è®­ç»ƒè€Œè®¾è®¡çš„æ•°æ®æ¸²æŸ“æ–¹æ³•ï¼Œæ‰€è®­ç»ƒå¾—åˆ°çš„PWC-Net ä¸ RAFTå…‰æµç®—æ³•è¾¾åˆ°äº†SOTA,ä»£ç å’Œæ•°æ®å°†å¼€æºã€‚
* [UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning](https://arxiv.org/abs/2012.00212)<br>:star:[code](https://github.com/coolbeam/UPFlow_pytorch)

<a name="65"/>

## 65.Style Transfer(é£æ ¼è¿ç§»)
* [Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes](https://arxiv.org/abs/2103.17185)<br>:star:[code](https://github.com/CompVis/brushstroke-parameterized-style-transfer)
* [ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows](https://arxiv.org/abs/2103.16877)<br>:star:[code](https://github.com/pkuanjie/ArtFlow) 
* [Lipstick ain't enough: Beyond Color Matching for In-the-Wild Makeup Transfer](https://arxiv.org/abs/2104.01867)<br>:star:[code](https://github.com/VinAIResearch/CPM)
* [Rethinking and Improving the Robustness of Image Style Transfer](https://arxiv.org/abs/2104.05623)<br>:open_mouth:oral<br>è§£è¯»ï¼š[CVPR2021 æœ€ä½³è®ºæ–‡å€™é€‰â€”æé«˜å›¾åƒé£æ ¼è¿ç§»çš„é²æ£’æ€§](https://mp.weixin.qq.com/s/OMu941IynGtY9GU8dh4Icg)
* [Drafting and Revision: Laplacian Pyramid Network for Fast High-Quality Artistic Style Transfer](https://arxiv.org/abs/2104.05376)<br>:star:[code](https://github.com/PaddlePaddle/PaddleGAN/)
* [Style-Aware Normalized Loss for Improving Arbitrary Style Transfer](https://arxiv.org/abs/2104.10064)<br>:open_mouth:oral
* [In the Light of Feature Distributions: Moment Matching for Neural Style Transfer](https://arxiv.org/abs/2103.07208)<br>:star:[code](https://github.com/D1noFuzi/cmd_styletransfer):house:[project](https://cmdnst.github.io/)
* [ArtCoder: An End-to-End Method for Generating Scanning-Robust Stylized QR Codes](https://openaccess.thecvf.com/content/CVPR2021/papers/Su_ArtCoder_An_End-to-End_Method_for_Generating_Scanning-Robust_Stylized_QR_Codes_CVPR_2021_paper.pdf)
* [Adaptive Convolutions for Structure-Aware Style Transfer](https://openaccess.thecvf.com/content/CVPR2021/papers/Chandran_Adaptive_Convolutions_for_Structure-Aware_Style_Transfer_CVPR_2021_paper.pdf) 
* [Learning To Warp for Style Transfer](https://openaccess.thecvf.com/content/CVPR2021/papers/Cai_Deep_Lesion_Tracker_Monitoring_Lesions_in_4D_Longitudinal_Imaging_Studies_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/xch-liu/learning-warp-st)
* [Single-Shot Freestyle Dance Reenactment](https://arxiv.org/abs/2012.01158)
* [CT-Net: Complementary Transfering Network for Garment Transfer With Arbitrary Geometric Changes](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_CT-Net_Complementary_Transfering_Network_for_Garment_Transfer_With_Arbitrary_Geometric_CVPR_2021_paper.pdf)
* [DualAST: Dual Style-Learning Networks for Artistic Style Transfer](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_DualAST_Dual_Style-Learning_Networks_for_Artistic_Style_Transfer_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/HalbertCH/DualAST)
* [What Can Style Transfer and Paintings Do For Model Robustness?](https://arxiv.org/abs/2011.14477)<br>:star:[code](https://github.com/hubertsgithub/style_painting_robustness)
* è¿åŠ¨è¿ç§»
  * [Autoregressive Stylized Motion Synthesis with Generative Flow](https://openaccess.thecvf.com/content/CVPR2021/papers/Wen_Autoregressive_Stylized_Motion_Synthesis_With_Generative_Flow_CVPR_2021_paper.pdf)

  
<a name="64"/>

## 64.Speech processing(è¯­éŸ³å¤„ç†)
  
* [Can audio-visual integration strengthen robustness under multimodal attacks?](https://arxiv.org/abs/2104.02000)<br>:star:[code](https://github.com/YapengTian/AV-Robustness-CVPR21)
* [Robust Audio-Visual Instance Discrimination](https://arxiv.org/abs/2103.15916)
* ç«‹ä½“éŸ³é¢‘ç”Ÿæˆ
  * [Visually Informed Binaural Audio Generation without Binaural Audios](https://arxiv.org/abs/2104.06162)<br>:star:[code](https://github.com/SheldonTsui/PseudoBinaural_CVPR2021):house:[project](https://sheldontsui.github.io/projects/PseudoBinaural):tv:[video](https://youtu.be/r-uC2MyAWQc)
* è§†å¬åˆ†ç¦»
  * [Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation](https://arxiv.org/abs/2104.02775)<br>:house:[project](https://caffnet.github.io/):tv:[video](https://youtu.be/9R2qQ7dGTp8)
  * [Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation](https://arxiv.org/abs/2104.02026)<br>:star:[code](https://github.com/YapengTian/CCOL-CVPR21)
  * [VisualVoice: Audio-Visual Speech Separation With Cross-Modal Consistency](https://arxiv.org/abs/2101.03149)<br>:star:[code](https://github.com/facebookresearch/VisualVoice):house:[project](http://vision.cs.utexas.edu/projects/VisualVoice/)
* å£°éŸ³-è§†é¢‘è§£æ
  * [Exploring Heterogeneous Clues for Weakly-Supervised Audio-Visual Video Parsing](https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Exploring_Heterogeneous_Clues_for_Weakly-Supervised_Audio-Visual_Video_Parsing_CVPR_2021_paper.pdf) 
* A-V
  * [Positive Sample Propagation Along the Audio-Visual Event Line](https://arxiv.org/abs/2104.00239)<br>:star:[code](https://github.com/jasongief/PSP_CVPR_2021)
* è¯­éŸ³äººè„¸å…³è”
  * [Seeking the Shape of Sound: An Adaptive Framework for Learning Voice-Face Association](https://arxiv.org/abs/2103.07293)



<a name="63"/>

## 63.Image Processing(å›¾åƒå¤„ç†)
* å›¾åƒä¿¡å·å¤„ç†
  * [Invertible Image Signal Processing](https://arxiv.org/abs/2103.15061)<br>:star:[code](https://github.com/yzxing87/Invertible-ISP):house:[project](https://yzxing87.github.io/InvISP/index.html)
* å…‰è°±é‡å»º
  * [Tuning IR-cut Filter for Illumination-aware Spectral Reconstruction from RGB](https://arxiv.org/abs/2103.14708)<br>:open_mouth:oral

<a name="62"/>

## 62.Free-Hand Sketches(æ‰‹ç»˜è‰å›¾è¯†åˆ«)
  * [Cloud2Curve: Generation and Vectorization of Parametric Sketches](https://arxiv.org/abs/2103.15536)

<a name="61"/>

## 61.ç®—æ³•
* å› æœæ¨ç†ç®—æ³•
  * [ACRE: Abstract Causal REasoning Beyond Covariation](https://arxiv.org/abs/2103.14232)<br>:star:[code](https://github.com/WellyZhang/ACRE):house:[project](http://wellyzhang.github.io/project/acre.html)
* æŠ½è±¡æ—¶ç©ºæ¨ç†ç®—æ³•
  * [Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution](https://arxiv.org/abs/2103.14230)<br>:star:[code](https://github.com/WellyZhang/PrAE):house:[project](http://wellyzhang.github.io/project/prae.html)


<a name="60"/>

## 60. SLAM/AR/æœºå™¨äºº
* [Tangent Space Backpropagation for 3D Transformation Groups](https://arxiv.org/abs/2103.12032)<br>:star:[code](https://github.com/princeton-vl/lietorch)
* è§†è§‰é‡Œç¨‹è®¡
  * [Generalizing to the Open World: Deep Visual Odometry with Online Adaptation](https://arxiv.org/abs/2103.15279)
* æœºå™¨äºº
  * [Visual Room Rearrangement](https://arxiv.org/abs/2103.16544)<br>:open_mouth:oral:house:[project](https://ai2thor.allenai.org/rearrangement/):tv:[video](https://www.youtube.com/watch?v=1APxaOC9U-A)
  * [GATSBI: Generative Agent-centric Spatio-temporal Object Interaction](https://arxiv.org/abs/2104.04275)<br>:open_mouth:oral:star:[code](https://github.com/mch5048/gatsbi):tv:[video](https://www.youtube.com/watch?v=3urXFiU9kao)
  * [DexYCB: A Benchmark for Capturing Hand Grasping of Objects](https://arxiv.org/abs/2104.04631)<br>:star:[code](https://github.com/NVlabs/dex-ycb-toolkit):house:[project](https://dex-ycb.github.io/):tv:[video](https://youtu.be/Q4wyBaZeBw0)
  * [ContactOpt: Optimizing Contact to Improve Grasps](https://arxiv.org/abs/2104.07267)<br>:star:[code](https://github.com/facebookresearch/contactopt)<br>æœºå™¨äººæ‰‹æŠ“å–
  * [ManipulaTHOR: A Framework for Visual Object Manipulation](https://arxiv.org/abs/2104.11213)<br>:open_mouth:oral:star:[code](https://github.com/allenai/manipulathor/):house:[project](https://ai2thor.allenai.org/manipulathor/):tv:[video](https://www.youtube.com/watch?v=nINZ52nlzX0)
  * è§†è§‰å¯¼èˆª
    * [Pushing it out of the Way: Interactive Visual Navigation](https://arxiv.org/abs/2104.14040)<br>:house:[project](https://prior.allenai.org/projects/interactive-visual-navigation):tv:[video](https://www.youtube.com/watch?v=GvTI5XCMvPw)
    * [Differentiable SLAM-net: Learning Particle SLAM for Visual Navigation](https://arxiv.org/abs/2105.07593)<br>:house:[project](https://sites.google.com/view/slamnet):tv:[video](https://youtu.be/dk1fdtf3fNI)
* AR
  * [Stay Positive: Non-Negative Image Synthesis for Augmented Reality](https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Stay_Positive_Non-Negative_Image_Synthesis_for_Augmented_Reality_CVPR_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/katieluo88/StayPositive)
  * [HDR Environment Map Estimation for Real-Time Augmented Reality](https://arxiv.org/abs/2011.10687):tv:[video](https://docs-assets.developer.apple.com/ml-research/papers/hdr-environment-map.mp4)
  * [NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering Using RGB Cameras](https://arxiv.org/abs/2103.07700)
  * è™šæ‹Ÿè¯•ç©¿
    * [VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization](https://arxiv.org/abs/2103.16874)
    * [Self-Supervised Collision Handling via Generative 3D Garment Models for Virtual Try-On](https://arxiv.org/abs/2105.06462)<br>:house:[project](http://mslab.es/projects/SelfSupervisedGarmentCollisions/):tv:[video](https://youtu.be/9AnBNco6i2U)
    * [Toward Accurate and Realistic Outfits Visualization with Attention to Details](https://arxiv.org/abs/2106.06593)
    * [ANR: Articulated Neural Rendering for Virtual Avatars](https://arxiv.org/abs/2012.12890)<br>:house:[project](https://anr-avatars.github.io/)
    * [Parser-Free Virtual Try-On via Distilling Appearance Flows](https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_Parser-Free_Virtual_Try-On_via_Distilling_Appearance_Flows_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/geyuying/PF-AFN)

<a name="59"/>

## 59.Capsule Network(èƒ¶å›Šç½‘ç»œ)(æ·±åº¦å­¦ä¹ æ¨¡å‹)
* [Dynamic Slimmable Network](https://arxiv.org/abs/2103.13258)<br>:open_mouth:oral:star:[code](https://github.com/changlin31/DS-Net)
* [Towards Evaluating and Training Verifiably Robust Neural Networks](https://arxiv.org/abs/2104.00447)<br>:open_mouth:oral:star:[code](https://github.com/ZhaoyangLyu/VerifiablyRobustNN) 
* [Activate or Not: Learning Customized Activation](https://arxiv.org/abs/2009.04759)<br>:star:[code](https://github.com/nmaac/acon)<br>ç²—è§£ï¼š[4](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)<br>è§£è¯»ï¼š[CVPR 2021 | è‡ªé€‚åº”æ¿€æ´»å‡½æ•°ACON: ç»Ÿä¸€ReLUå’ŒSwishçš„æ–°èŒƒå¼](https://mp.weixin.qq.com/s/pbeA2w54MZ_-wXsmGoo3hg)
* [DISCO: Dynamic and Invariant Sensitive Channel Obfuscation for Deep Neural Networks](https://arxiv.org/abs/2012.11025)<br>:star:[code](https://github.com/splitlearning/InferenceBenchmark)
* Capsule Network(èƒ¶å›Šç½‘ç»œ)
  * [Capsule Network Is Not More Robust Than Convolutional Network](https://openaccess.thecvf.com/content/CVPR2021/papers/Gu_Capsule_Network_Is_Not_More_Robust_Than_Convolutional_Network_CVPR_2021_paper.pdf)

<a name="58"/>

## 58.Metric Learning(åº¦é‡å­¦ä¹ /ç›¸ä¼¼åº¦å­¦ä¹ )
* [Dynamic Metric Learning: Towards a Scalable Metric Space to Accommodate Multiple Semantic Scales](https://arxiv.org/abs/2103.11781)<br>:star:[code](https://github.com/SupetZYK/DynamicMetricLearning)
* [Embedding Transfer with Label Relaxation for Improved Metric Learning](https://arxiv.org/abs/2103.14908)
* [Noise-resistant Deep Metric Learning with Ranking-based Instance Selection](https://arxiv.org/abs/2103.16047)<br>:star:[code](https://github.com/alibaba-edu/Ranking-based-Instance-Selection)
* [Unsupervised Hyperbolic Metric Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_Unsupervised_Hyperbolic_Metric_Learning_CVPR_2021_paper.pdf)
* [Deep Compositional Metric Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Deep_Compositional_Metric_Learning_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/wzzheng/DCML)
* [SLADE: A Self-Training Framework for Distance Metric Learning](https://arxiv.org/abs/2011.10269)
* [Asymmetric Metric Learning for Knowledge Transfer](https://arxiv.org/abs/2006.16331)<br>:star:[code](https://github.com/budnikm/aml)
* [Relative Order Analysis and Optimization for Unsupervised Deep Metric Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Kan_Relative_Order_Analysis_and_Optimization_for_Unsupervised_Deep_Metric_Learning_CVPR_2021_paper.pdf)


<a name="57"/>

## 57.Sign Language Recognition(æ‰‹è¯­è¯†åˆ«)
  * [Skeleton Based Sign Language Recognition Using Whole-body Keypoints](https://arxiv.org/abs/2103.08833)<br>:star:[code](https://github.com/jackyjsy/CVPR21Chal-SLR)
  * [Read and Attend: Temporal Localisation in Sign Language Videos](https://arxiv.org/abs/2103.16481)<br>:house:[project](https://www.robots.ox.ac.uk/~vgg/research/bslattend/)
  * [Fingerspelling Detection in American Sign Language](https://arxiv.org/abs/2104.01291)
* æ‰‹è¯­ç¿»è¯‘
  * [Improving Sign Language Translation with Monolingual Data by Sign Back-Translation](https://arxiv.org/abs/2105.12397)<br>:sunflower:[dataset](http://home.ustc.edu.cn/~zhouh156/dataset/csl-daily/)



<a name="56"/>

## 56.Computational Photography(å…‰å­¦ã€å‡ ä½•ã€å…‰åœºæˆåƒã€è®¡ç®—æ‘„å½±)
  * [Deep Gaussian Scale Mixture Prior for Spectral Compressive Imaging](https://arxiv.org/abs/2103.07152)<br>:star:[code](https://github.com/TaoHuang95/DGSMP):house:[project](https://see.xidian.edu.cn/faculty/wsdong/Projects/DGSM-SCI.htm)
  * [Mask-ToF: Learning Microlens Masks for Flying Pixel Correction in Time-of-Flight Imaging](https://arxiv.org/abs/2103.16693)<br>:house:[project](https://light.princeton.edu/publication/mask-tof/)
  * [Passive Inter-Photon Imaging](https://arxiv.org/abs/2104.00059)<br>:open_mouth:oral
  * [Shape and Material Capture at Home](https://arxiv.org/abs/2104.06397)<br>:star:[code](https://github.com/dlichy/ShapeAndMaterial):house:[project](https://dlichy.github.io/ShapeAndMaterialAtHome/)
  * [Event-based Synthetic Aperture Imaging with a Hybrid Network](https://arxiv.org/abs/2103.02376)<br>åˆ†äº«ä¼š
  * [High-Speed Image Reconstruction Through Short-Term Plasticity for Spiking Cameras](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_High-Speed_Image_Reconstruction_Through_Short-Term_Plasticity_for_Spiking_Cameras_CVPR_2021_paper.pdf)
  * [Leveraging the Availability of Two Cameras for Illuminant Estimation](https://openaccess.thecvf.com/content/CVPR2021/papers/Abdelhamed_Leveraging_the_Availability_of_Two_Cameras_for_Illuminant_Estimation_CVPR_2021_paper.pdf)
* ç›¸æœºå§¿åŠ¿
  * [Fusing the Old with the New: Learning Relative Camera Pose with Geometry-Guided Uncertainty](https://arxiv.org/abs/2104.08278)<br>:open_mouth:oral
  * [Learning Neural Representation of Camera Pose with Matrix Representation of Pose Shift via View Synthesis](https://arxiv.org/abs/2104.01508)<br>:star:[code](https://github.com/AlvinZhuyx/camera_pose_representation)
  * [Uncertainty-Aware Camera Pose Estimation From Points and Lines](https://openaccess.thecvf.com/content/CVPR2021/papers/Vakhitov_Uncertainty-Aware_Camera_Pose_Estimation_From_Points_and_Lines_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/alexandervakhitov/uncertain-pnp):house:[project](https://alexandervakhitov.github.io/uncertain-pnp/) 
  * [Neural Reprojection Error: Merging Feature Learning and Camera Pose Estimation](https://arxiv.org/abs/2103.07153)<br>:star:[code](https://github.com/germain-hug/NRE):house:[project](https://www.hugogermain.com/nre)
  * [Wide-Baseline Relative Camera Pose Estimation with Directional Learning](https://arxiv.org/abs/2106.03336)
  * [Camera Pose Matters: Improving Depth Prediction by Mitigating Pose Distribution Bias](https://arxiv.org/abs/2007.03887)<br>:open_mouth:oral
* å®¤å†…ç…§æ˜ä¼°è®¡
  * [Indoor Lighting Estimation Using an Event Camera](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Indoor_Lighting_Estimation_Using_an_Event_Camera_CVPR_2021_paper.pdf)
* Phase Retrievalç›¸ä½æ¢å¤ç®—æ³•
  * [Physics-Based Iterative Projection Complex Neural Network for Phase Retrieval in Lensless Microscopy Imaging](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Physics-Based_Iterative_Projection_Complex_Neural_Network_for_Phase_Retrieval_in_CVPR_2021_paper.pdf)

  
<a name="55"/>

## 55.Graph Matching(å›¾åŒ¹é…)
  * [Deep Graph Matching under Quadratic Constraint](https://arxiv.org/abs/2103.06643)<br>:star:[code](https://github.com/Zerg-Overmind/QC-DGM)

<a name="54"/>

## 54.Emotion Perception(æƒ…ç»ªæ„ŸçŸ¥/æƒ…æ„Ÿé¢„æµ‹)
* [Affect2MM: Affective Analysis of Multimedia Content Using Emotion Causality](https://arxiv.org/abs/2103.06541)<br>:house:[project](https://gamma.umd.edu/researchdirections/affectivecomputing/affect2mm/)
* Human Multimodal Emotion Recognition(äººç±»å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«)
  * [Progressive Modality Reinforcement for Human Multimodal Emotion Recognition From Unaligned Multimodal Sequences](https://openaccess.thecvf.com/content/CVPR2021/papers/Lv_Progressive_Modality_Reinforcement_for_Human_Multimodal_Emotion_Recognition_From_Unaligned_CVPR_2021_paper.pdf)

<a name="53"/>

## 53.Dataset(æ•°æ®é›†)
  * [Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](https://arxiv.org/abs/2102.08981)<br>:sunflower:[dataset](https://github.com/google-research-datasets/conceptual-12m)
  * [Sewer-ML: A Multi-Label Sewer Defect Classification Dataset and Benchmark](https://arxiv.org/abs/2103.10895)<br>:house:[project](https://vap.aau.dk/sewer-ml/)
  * [Benchmarking Representation Learning for Natural World Image Collections](https://arxiv.org/abs/2103.16483)<br>:sunflower:[dataset](https://github.com/visipedia/newt)
  * [SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction from Video Data](https://arxiv.org/abs/2105.08612)<br>:open_mouth:oral:sunflower:[dataset](http://sailvos.web.illinois.edu/_site/index.html)
 * [Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback](https://arxiv.org/abs/1905.12794)<br>:sunflower:[dataset](https://github.com/XiaoxiaoGuo/fashion-iq)
 * äººè„¸å›¾åƒä¿®é¥°æ•°æ®é›†
  * [PPR10K: A Large-Scale Portrait Photo Retouching Dataset with Human-Region Mask and Group-Level Consistency](https://arxiv.org/abs/2105.09180)<br>:star:[code](https://github.com/csjliang/PPR10K)
* å®¤å¤–åœºæ™¯
  * [OpenRooms: An Open Framework for Photorealistic Indoor Scene Datasets](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_OpenRooms_An_Open_Framework_for_Photorealistic_Indoor_Scene_Datasets_CVPR_2021_paper.pdf)<br>:open_mouth:oral:sunflower:[dataset](https://vilab-ucsd.github.io/ucsd-openrooms/dataset/):house:[project](https://vilab-ucsd.github.io/ucsd-openrooms/)
* è§†è§‰è‰ºæœ¯
  * [ArtEmis: Affective Language for Visual Art](https://arxiv.org/abs/2101.07396)<br>:house:[project](https://www.artemisdataset.org/)ä¸»é¡µä¸­åŒ…å«å…¨éƒ¨ï¼šæ•°æ®é›†ã€ä»£ç ã€è§†é¢‘ç­‰
* UGC è§†é¢‘è´¨é‡è¯„ä¼°
  * [Rich Features for Perceptual Quality Assessment of UGC Videos](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Rich_Features_for_Perceptual_Quality_Assessment_of_UGC_Videos_CVPR_2021_paper.pdf)<br>:sunflower:[dataset](https://media.withyoutube.com/ugc-dataset)
* å®¤å†…å®šä½æ•°æ®é›†
  * [Large-Scale Localization Datasets in Crowded Indoor Spaces](https://arxiv.org/abs/2105.08941)<br>:sunflower:[dataset](https://naverlabs.com/datasets)
  * [Zillow Indoor Dataset: Annotated Floor Plans With 360deg Panoramas and 3D Room Layouts](https://openaccess.thecvf.com/content/CVPR2021/papers/Cruz_Zillow_Indoor_Dataset_Annotated_Floor_Plans_With_360deg_Panoramas_and_CVPR_2021_paper.pdf)
* æ•°æ®é›†(äººç±»æ„å›¾ç ”ç©¶)
  * [Intentonomy: A Dataset and Study Towards Human Intent Understanding](https://arxiv.org/abs/2011.05558)<br>:open_mouth:oral:star:[code](https://github.com/kmnp/intentonomy)
* äººè„¸è¯†åˆ«æ•°æ®é›†
  * [Virtual Fully-Connected Layer: Training a Large-Scale Face Recognition Dataset With Limited Computational Resources](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Virtual_Fully-Connected_Layer_Training_a_Large-Scale_Face_Recognition_Dataset_With_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/pengyuLPY/Virtual-Fully-Connected-Layer)
* è§†è§‰å±æ€§é¢„æµ‹æ•°æ®é›†
  * [Learning To Predict Visual Attributes in the Wild](https://openaccess.thecvf.com/content/CVPR2021/papers/Pham_Learning_To_Predict_Visual_Attributes_in_the_Wild_CVPR_2021_paper.pdf)<br>:sunflower:[dataset](https://vawdataset.com/)
* æ•°æ®é›†(Object-Centric Videos)
  * [Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild With Pose Annotations](https://arxiv.org/abs/2012.09988)<br>:sunflower:[dataset](https://github.com/google-research-datasets/Objectron)
* è§†é¢‘åœºæ™¯è§£æ
  * [VSPW: A Large-scale Dataset for Video Scene Parsing in the Wild](https://openaccess.thecvf.com/content/CVPR2021/papers/Miao_VSPW_A_Large-scale_Dataset_for_Video_Scene_Parsing_in_the_CVPR_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/VSPW-dataset/VSPW_code):house:[project](https://www.vspwdataset.com/)
* æ•°æ®é›†ï¼ˆæ‰‹è¯­ï¼‰
  * [How2Sign: A Large-Scale Multimodal Dataset for Continuous American Sign Language](https://arxiv.org/abs/2008.08143)<br>:sunflower:[dataset](https://how2sign.github.io/)

<a name="52"/>

## 52. Image Generation/Synthesis(å›¾åƒç”Ÿæˆ)

- [Spatially-Adaptive Pixelwise Networks for Fast Image Translation](https://arxiv.org/abs/2012.02992)<br>:house:[project](https://tamarott.github.io/ASAPNet_web/)<br>é‡‡ç”¨è¶…ç½‘ç»œå’Œéšå¼å‡½æ•°ï¼Œæå¿«çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘é€Ÿåº¦ï¼ˆæ¯”åŸºçº¿å¿«18å€ï¼‰
- [Image Generators with Conditionally-Independent Pixel Synthesis](https://arxiv.org/abs/2011.13775)<br>:open_mouth:oral:star:[code](https://github.com/saic-mdal/CIPS)
* [Im2Vec: Synthesizing Vector Graphics without Vector Supervision](https://arxiv.org/abs/2102.02798)<br>:open_mouth:oral:star:[code](https://github.com/preddy5/Im2Vec):house:[project](http://geometry.cs.ucl.ac.uk/projects/2021/im2vec/)
* [Context-Aware Layout to Image Generation with Enhanced Object Appearance](https://arxiv.org/abs/2103.11897)<br>:star:[code](https://github.com/wtliao/layout2img) 
* [Adversarial Generation of Continuous Images](https://arxiv.org/pdf/2011.12026.pdf)<br>:star:[code](https://github.com/universome/inr-gan)
* [StEP: Style-based Encoder Pre-training for Multi-modal Image Synthesis](https://arxiv.org/abs/2104.07098)
* [IMAGINE: Image Synthesis by Image-Guided Model Inversion](https://arxiv.org/abs/2104.05895)
* [SSN: Soft Shadow Network for Image Compositing](https://arxiv.org/abs/2007.08211)
* [Mask-Embedded Discriminator With Region-Based Semantic Regularization for Semi-Supervised Class-Conditional Image Synthesis](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Mask-Embedded_Discriminator_With_Region-Based_Semantic_Regularization_for_Semi-Supervised_Class-Conditional_Image_CVPR_2021_paper.pdf)
* [Learning Semantic Person Image Generation by Region-Adaptive Normalization](https://arxiv.org/abs/2104.06650)<br>:star:[code](https://github.com/cszy98/SPGNet)
* [MUST-GAN: Multi-Level Statistics Transfer for Self-Driven Person Image Generation](https://openaccess.thecvf.com/content/CVPR2021/papers/Ma_MUST-GAN_Multi-Level_Statistics_Transfer_for_Self-Driven_Person_Image_Generation_CVPR_2021_paper.pdf)
* [Combining Semantic Guidance and Deep Reinforcement Learning for Generating Human Level Paintings](https://arxiv.org/abs/2011.12589)<br>:star:[code](https://github.com/1jsingh/semantic-guidance)
* [Diverse Semantic Image Synthesis via Probability Distribution Modeling](https://arxiv.org/abs/2103.06878)<br>:star:[code](https://github.com/tzt101/INADE)
* [Mol2Image: Improved Conditional Flow Models for Molecule to Image Synthesis](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Mol2Image_Improved_Conditional_Flow_Models_for_Molecule_to_Image_Synthesis_CVPR_2021_paper.pdf) 

 
<a name="51"/>

## 51.Contrastive Learning(å¯¹æ¯”å­¦ä¹ )
* [AdCo: Adversarial Contrast for Efficient Learning of Unsupervised Representations from Self-Trained Negative Adversaries](https://arxiv.org/abs/2011.08435)<br>:star:[code](https://arxiv.org/abs/2011.08435)<br>è§£è¯»:[CVPR 2021æ¥æ”¶è®ºæ–‡ï¼šAdCoåŸºäºå¯¹æŠ—çš„å¯¹æ¯”å­¦ä¹ ](https://mp.weixin.qq.com/s/u7Lhzh8uYEEHfWiM32-4yQ)
* [LAFEAT: Piercing Through Adversarial Defenses with Latent Features](https://arxiv.org/abs/2104.09284)<br>:open_mouth:oral:star:[code](https://github.com/lafeat/lafeat)
* [Distilling Audio-Visual Knowledge by Compositional Contrastive Learning](https://arxiv.org/abs/2104.10955)<br>:star:[code](https://github.com/yanbeic/CCL)
* [Mining Better Samples for Contrastive Learning of Temporal Correspondence](https://openaccess.thecvf.com/content/CVPR2021/papers/Jeon_Mining_Better_Samples_for_Contrastive_Learning_of_Temporal_Correspondence_CVPR_2021_paper.pdf)
* [Jo-SRC: A Contrastive Approach for Combating Noisy Labels](https://openaccess.thecvf.com/content/CVPR2021/papers/Yao_Jo-SRC_A_Contrastive_Approach_for_Combating_Noisy_Labels_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/NUST-Machine-Intelligence-Laboratory/Jo-SRC)
* [Neighborhood Contrastive Learning for Novel Class Discovery](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhong_Neighborhood_Contrastive_Learning_for_Novel_Class_Discovery_CVPR_2021_paper.pdf) 

  
  
<a name="50"/>

## 50.OCR

* [Fourier Contour Embedding for Arbitrary-Shaped Text Detection](https://arxiv.org/abs/2104.10442)
* [Implicit Feature Alignment: Learn to Convert Text Recognizer to Text Spotter](https://arxiv.org/abs/2106.05920)
* [Sequence-to-Sequence Contrastive Learning for Text Recognition](http://arxiv.org/abs/2012.10873)
* [A Multiplexed Network for End-to-End, Multilingual OCR](https://arxiv.org/abs/2103.15992)
* [TAP: Text-Aware Pre-Training for Text-VQA and Text-Caption](https://arxiv.org/abs/2012.04638)
* åœºæ™¯æ–‡æœ¬æ£€æµ‹
  * [What If We Only Use Real Datasets for Scene Text Recognition? Toward Scene Text Recognition With Fewer Labels](https://arxiv.org/abs/2103.04400)<br>:star:[code](https://github.com/ku21fan/STR-Fewer-Labels)
  * [Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition](https://arxiv.org/abs/2103.06495)<br>:open_mouth:oral:star:[code](https://github.com/FangShancheng/ABINet)
  * [MOST: A Multi-Oriented Scene Text Detector with Localization Refinement](https://arxiv.org/abs/2104.01070)
  * [Scene Text Retrieval via Joint Text Detection and Similarity Learning](https://arxiv.org/abs/2104.01552)<br>:star:[code](https://github.com/lanfeng4659/STR-TDSL)
  * [TextOCR: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text](https://arxiv.org/abs/2105.05486)<br>:house:[project](https://textvqa.org/textocr)
  * [Progressive Contour Regression for Arbitrary-Shape Scene Text Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Dai_Progressive_Contour_Regression_for_Arbitrary-Shape_Scene_Text_Detection_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/dpengwen/PCR)
  * [Dictionary-guided Scene Text Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Nguyen_Dictionary-Guided_Scene_Text_Recognition_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/VinAIResearch/dict-guided)
  * [Primitive Representation Learning for Scene Text Recognition](https://arxiv.org/abs/2105.04286)
* æ‰‹å†™æ–‡æœ¬è¯†åˆ«
  * [MetaHTR: Towards Writer-Adaptive Handwritten Text Recognition](https://arxiv.org/abs/2104.01876)
* æ–‡æœ¬åˆ†å‰²
  * [Rethinking Text Segmentation: A Novel Dataset and a Text-Specific Refinement Approach](https://arxiv.org/abs/2011.14021)<br>:star:[code](https://github.com/SHI-Labs/Rethinking-Text-Segmentation)
* è§†é¢‘æ–‡æœ¬æ£€æµ‹
  * [Semantic-Aware Video Text Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Feng_Semantic-Aware_Video_Text_Detection_CVPR_2021_paper.pdf)
* æ–‡æœ¬æ£€æµ‹
  * [Self-Attention Based Text Knowledge Mining for Text Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Wan_Self-Attention_Based_Text_Knowledge_Mining_for_Text_Detection_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/CVI-SZU/STKM)



<a name="49"/>

## 49.Adversarial Learning(å¯¹æŠ—å­¦ä¹ )

- [Simulating Unknown Target Models for Query-Efficient Black-box Attacks](https://arxiv.org/abs/2009.00960)<br>:star:[code](https://github.com/machanic/MetaSimulator)<br>é»‘ç›’å¯¹æŠ—æ”»å‡»
- [Delving into Data: Effectively Substitute Training for Black-box Attack](https://arxiv.org/abs/2104.12378)<br>åŸºäºé«˜æ•ˆè®­ç»ƒæ›¿ä»£æ¨¡å‹çš„é»‘ç›’æ”»å‡»æ–¹æ³•<br>è§£è¯»ï¼š[8](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
- [LiBRe: A Practical Bayesian Approach to Adversarial Detection](https://arxiv.org/abs/2103.14835)<br>:star:[code](https://github.com/thudzj/ScalableBDL)
* [Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect](https://arxiv.org/abs/2011.13375)
* [Enhancing the Transferability of Adversarial Attacks Through Variance Tuning](https://arxiv.org/abs/2103.15571)<br>:star:[code](https://github.com/JHL-HUST/VT)
* [Natural Adversarial Examples](https://arxiv.org/abs/1907.07174)<br>:star:[code](https://github.com/hendrycks/natural-adv-examples)
* [SurFree: A Fast Surrogate-Free Black-Box Attack](https://arxiv.org/abs/2011.12807)<br>:star:[code](https://github.com/t-maho/SurFree)
* [Regularizing Neural Networks via Adversarial Model Perturbation](https://arxiv.org/abs/2010.04925)<br>:star:[code](https://github.com/hiyouga/AMP-Regularizer)
* [Adversarial Imaging Pipelines](https://arxiv.org/abs/2102.03728)
* [MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation](https://arxiv.org/abs/2005.03161)
* [Universal Spectral Adversarial Attacks for Deformable Shapes](https://arxiv.org/abs/2104.03356)
* [Adversarial Robustness Across Representation Spaces](https://arxiv.org/abs/2012.00802)<br>:star:[code](https://github.com/tensorflow/neural-structured-learning/tree/master/research/multi_representation_adversary)
* [Protecting Intellectual Property of Generative Adversarial Networks From Ambiguity Attacks](https://openaccess.thecvf.com/content/CVPR2021/papers/Ong_Protecting_Intellectual_Property_of_Generative_Adversarial_Networks_From_Ambiguity_Attacks_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/dingsheng-ong/ipr-gan)
* [Dual Attention Suppression Attack: Generate Adversarial Camouflage in Physical World](https://arxiv.org/abs/2103.01050)<br>:open_mouth:oral:star:[code](https://github.com/nlsde-safety-team/DualAttentionAttack)
* [Learning Compositional Representation for 4D Captures with Neural ODE](https://arxiv.org/abs/2103.08271)
* å¯¹æŠ—æ”»å‡»
  * [Adversarial Laser Beam: Effective Physical-World Attack to DNNs in a Blink](https://arxiv.org/abs/2103.06504)



<a name="48"/>

## 48.Image Representation(å›¾åƒè¡¨ç¤º)

- [Learning Continuous Image Representation with Local Implicit Image Function](https://arxiv.org/abs/2012.09161)<br>:open_mouth:oral:star:[code](https://github.com/yinboc/liif):house:[project](https://yinboc.github.io/liif/):tv:[video](https://youtu.be/6f2roieSY_8)

<a name="47"/>

## 47.Vision-Language(è§†è§‰è¯­è¨€)

- [Structured Scene Memory for Vision-Language Navigation](https://arxiv.org/abs/2103.03454)<br>:star:[code](https://github.com/HanqingWangAI/SSM-VLN)
* [Kaleido-BERT: Vision-Language Pre-training on Fashion Domain](https://arxiv.org/abs/2103.16110)<br>
* [Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning](https://arxiv.org/abs/2104.03135)<br>:star:[code](https://github.com/researchmm/soho)
* [UC2: Universal Cross-Lingual Cross-Modal Vision-and-Language Pre-Training](https://arxiv.org/abs/2104.00332)
* [VinVL: Revisiting Visual Representations in Vision-Language Models](https://arxiv.org/abs/2101.00529)<br>:star:[code](https://github.com/pzzhang/VinVL)
* [Connecting What To Say With Where To Look by Modeling Human Attention Traces](https://arxiv.org/abs/2105.05964)<br>:star:[code](https://github.com/facebookresearch/connect-caption-and-trace):house:[project](http://pages.cs.wisc.edu/~zihangm/connect_caption_trace)
* [Adaptive Cross-Modal Prototypes for Cross-Domain Visual-Language Retrieval](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Adaptive_Cross-Modal_Prototypes_for_Cross-Domain_Visual-Language_Retrieval_CVPR_2021_paper.pdf) 
* [VLN BERT: A Recurrent Vision-and-Language BERT for Navigation](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_VLN_BERT_A_Recurrent_Vision-and-Language_BERT_for_Navigation_CVPR_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/YicongHong/Recurrent-VLN-BERT)
* [Transitional Adaptation of Pretrained Models for Visual Storytelling](https://openaccess.thecvf.com/content/CVPR2021/papers/Yu_Transitional_Adaptation_of_Pretrained_Models_for_Visual_Storytelling_CVPR_2021_paper.pdf) 
* [Learning Better Visual Dialog Agents With Pretrained Visual-Linguistic Representation](https://arxiv.org/abs/2105.11541)<br>:star:[code](https://github.com/amazon-research/read-up)
* [Causal Attention for Vision-Language Tasks](https://arxiv.org/abs/2103.03493)<br>:star:[code](https://github.com/yangxuntu/catt) 
 
<a name="46"/>

## 46.Human-Object Interaction(äººç‰©äº¤äº’)

- [Learning Asynchronous and Sparse Human-Object Interaction in Videos](https://arxiv.org/abs/2103.02758)
- [QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information](https://arxiv.org/abs/2103.05399)<br>:star:[code](https://github.com/hitachi-rd-cv/qpic)
- [Reformulating HOI Detection as Adaptive Set Prediction](https://arxiv.org/abs/2103.05983)<br>:star:[code](https://github.com/yoyomimi/AS-Net)
* [Detecting Human-Object Interaction via Fabricated Compositional Learning](https://arxiv.org/abs/2103.08214)<br>:star:[code](https://github.com/zhihou7/FCL)
* [Affordance Transfer Learning for Human-Object Interaction Detection](https://arxiv.org/abs/2104.02867)<br>:star:[code](https://github.com/zhihou7/HOI-CL)
* [Glance and Gaze: Inferring Action-aware Points for One-Stage Human-Object Interaction Detection](https://arxiv.org/abs/2104.05269)<br>:star:[code](https://github.com/SherlockHolmes221/GGNet)
* [Hierarchical Video Prediction Using Relational Layouts for Human-Object Interactions](https://openaccess.thecvf.com/content/CVPR2021/papers/Bodla_Hierarchical_Video_Prediction_Using_Relational_Layouts_for_Human-Object_Interactions_CVPR_2021_paper.pdf)

<a name="45"/>

## 45.Camera Localization(ç›¸æœºå®šä½)

- [Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments](https://arxiv.org/abs/2012.04746)<br>:open_mouth:oral
- [Back to the Feature: Learning Robust Camera Localization from Pixels to Pose](https://arxiv.org/abs/2103.09213)<br>:star:[code](https://github.com/cvg/pixloc)
- [Learning Camera Localization via Dense Scene Matching](https://arxiv.org/abs/2103.16792)<br>:star:[code](https://github.com/Tangshitao/Dense-Scene-Matching)
- [Privacy Preserving Localization and Mapping From Uncalibrated Cameras](https://openaccess.thecvf.com/content/CVPR2021/papers/Geppert_Privacy_Preserving_Localization_and_Mapping_From_Uncalibrated_Cameras_CVPR_2021_paper.pdf)
* è§†è§‰å®šä½
  * [VS-Net: Voting with Segmentation for Visual Localization](https://arxiv.org/abs/2105.10886)<br>:star:[code](https://github.com/zju3dv/VS-Net):house:[project](https://drinkingcoder.github.io/publication/vs-net/):tv:[video](https://youtu.be/5WLEyyLdxAs)

<a name="44"/>

## 44. Image/video Captioning(å›¾åƒ/è§†é¢‘å­—å¹•)

- [Scan2Cap: Context-aware Dense Captioning in RGB-D Scans](https://arxiv.org/abs/2012.02206)<br>:star:[code](https://github.com/daveredrum/Scan2Cap):house:[project](https://daveredrum.github.io/Scan2Cap/):tv:[video](https://youtu.be/AgmIpDbwTCY)
- [VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs](https://arxiv.org/abs/2101.12059)<br>è§†é¢‘å­—å¹•ã€è§†é¢‘é—®ç­”å’Œè§†é¢‘å¯¹è¯ä»»åŠ¡çš„å¤šæ¨¡å¼æ¡†æ¶
- [Open-book Video Captioning with Retrieve-Copy-Generate Network](https://arxiv.org/abs/2103.05284)
* å›¾åƒå­—å¹•
  * [Human-like Controllable Image Captioning with Verb-specific Semantic Roles](https://arxiv.org/abs/2103.12204)<br>:star:[code](https://github.com/mad-red/VSR-guided-CIC)
  * [Towards Accurate Text-based Image Captioning with Content Diversity Exploration](https://arxiv.org/abs/2105.03236)<br>:star:[code](https://github.com/guanghuixu/AnchorCaptioner)
  * [Image Change Captioning by Learning From an Auxiliary Task](https://openaccess.thecvf.com/content/CVPR2021/papers/Hosseinzadeh_Image_Change_Captioning_by_Learning_From_an_Auxiliary_Task_CVPR_2021_paper.pdf)
  * [FAIEr: Fidelity and Adequacy Ensured Image Caption Evaluation](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_FAIEr_Fidelity_and_Adequacy_Ensured_Image_Caption_Evaluation_CVPR_2021_paper.pdf)<br>:star:[code](https://vipl.ict.ac.cn/view_database.php?id=6)
  * [Towards Bridging Event Captioner and Sentence Localizer for Weakly Supervised Dense Event Captioning](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Towards_Bridging_Event_Captioner_and_Sentence_Localizer_for_Weakly_Supervised_CVPR_2021_paper.pdf)
  * [Improving OCR-Based Image Captioning by Incorporating Geometrical Relationship](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Improving_OCR-Based_Image_Captioning_by_Incorporating_Geometrical_Relationship_CVPR_2021_paper.pdf)
  
<a name="43"/>

## 43.Active Learning(ä¸»åŠ¨å­¦ä¹ )

- [Vab-AL: Incorporating Class Imbalance and Difficulty with Variational Bayes for Active Learning](https://arxiv.org/abs/2003.11249)
* [Task-Aware Variational Adversarial Active Learning](https://arxiv.org/abs/2002.04709)<br>:star:[code](https://github.com/cubeyoung/TA-VAAL)

<a name="42"/>

## 42.Scene Flow Estimation(åœºæ™¯æµä¼°è®¡)
* åœºæ™¯æµä¼°è®¡
  * [Self-Supervised Multi-Frame Monocular Scene Flow](https://arxiv.org/abs/2105.02216)<br>:star:[code](https://github.com/visinf/multi-mono-sf)
  * [HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs and Position-aware Flow Embedding](https://arxiv.org/abs/2105.07751)
  * [Self-Point-Flow: Self-Supervised Scene Flow Estimation from Point Clouds with Optimal Transport and Random Walk](https://arxiv.org/abs/2105.08248)<br>:open_mouth:oral
  * [FlowStep3D: Model Unrolling for Self-Supervised Scene Flow Estimation](https://arxiv.org/abs/2011.10147)<br>:star:[code](https://github.com/yairkit/flowstep3d)
  * [RAFT-3D: Scene Flow Using Rigid-Motion Embeddings](https://openaccess.thecvf.com/content/CVPR2021/papers/Teed_RAFT-3D_Scene_Flow_Using_Rigid-Motion_Embeddings_CVPR_2021_paper.pdf)

<a name="41"/>

## 41. Representation Learning(è¡¨ç¤ºå­¦ä¹ ï¼ˆå›¾åƒ+å­—å¹•ï¼‰)

- [VirTex: Learning Visual Representations from Textual Annotations](https://arxiv.org/abs/2006.06666)<br>:star:[code](https://github.com/kdexd/virtex)
- [Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566)<br>:open_mouth:oral:star:[code](https://github.com/facebookresearch/simsiam)
- [Representation Learning via Global Temporal Alignment and Cycle-Consistency](https://arxiv.org/abs/2105.05217)<br>:star:[code](https://github.com/hadjisma/VideoAlignment)
* [SelfDoc: Self-Supervised Document Representation Learning](https://arxiv.org/abs/2106.03331)
* [CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models](https://arxiv.org/abs/2004.08697)
* [Unsupervised Hyperbolic Representation Learning via Message Passing Auto-Encoders](https://arxiv.org/abs/2103.16046)<br>:star:[code](https://github.com/junhocho/HGCAE)
* [Boosting Video Representation Learning With Multi-Faceted Integration](https://openaccess.thecvf.com/content/CVPR2021/papers/Qiu_Boosting_Video_Representation_Learning_With_Multi-Faceted_Integration_CVPR_2021_paper.pdf)

<a name="40"/>

## 40.Superpixel (è¶…åƒç´ )

- [Learning the Superpixel in a Non-iterative and Lifelong Manner](https://arxiv.org/abs/2103.10681)<br>:star:[code](https://github.com/zh460045050/LNSNet)

<a name="39"/>

## 39.Debiasing(å»åè§)

- [Fair Attribute Classification through Latent Space De-biasing](https://arxiv.org/abs/2012.01469)<br>:star:[code](https://github.com/princetonvisualai/gan-debiasing):house:[project](https://princetonvisualai.github.io/gan-debiasing/)<br>
- [Reducing Domain Gap by Reducing Style Bias](https://arxiv.org/abs/1910.11645)<br>:star:[code](https://github.com/hyeonseobnam/sagnet)
* åå·®çŸ«æ­£
  * [EnD: Entangling and Disentangling deep representations for bias correction](https://arxiv.org/abs/2103.02023)<br>:star:[code](https://github.com/EIDOSlab/entangling-disentangling-bias)

<a name="38"/>

## 38.Class-Incremental learning(ç±»å¢é‡å­¦ä¹ )

- [IIRC: Incremental Implicitly-Refined Classification](https://arxiv.org/abs/2012.12477)<br>:house:[project](https://chandar-lab.github.io/IIRC/)<br>
- [Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2103.04059)<br>:star:[code](https://github.com/ali-chr/Semantic-aware-Knowledge-Distillation-for-Few-ShotClass-Incremental-Learning)
- [DER: Dynamically Expandable Representation for Class Incremental Learning](https://arxiv.org/abs/2103.16788)<br>:star:[code](https://github.com/Rhyssiyan/DER-ClassIL.pytorch)
- [Distilling Causal Effect of Data in Class-Incremental Learning](https://arxiv.org/abs/2103.01737)<br>:star:[code](https://github.com/JoyHuYY1412/DDE_CIL)
- [Self-Promoted Prototype Refinement for Few-Shot Class-Incremental Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Self-Promoted_Prototype_Refinement_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.pdf)
* [Adaptive Aggregation Networks for Class-Incremental Learning](https://arxiv.org/abs/2010.05063)<br>:star:[code](https://github.com/yaoyao-liu/class-incremental-learning)
* å¢é‡å­¦ä¹ 
  * [Few-Shot Incremental Learning with Continually Evolved Classifiers](https://arxiv.org/abs/2104.03047)
  * [On Learning the Geodesic Path for Incremental Learning](https://arxiv.org/abs/2104.08572)<br>:star:[code](https://github.com/chrysts/geodesic_continual_learning)
  * [Prototype Augmentation and Self-Supervision for Incremental Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Prototype_Augmentation_and_Self-Supervision_for_Incremental_Learning_CVPR_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/Impression2805/CVPR21_PASS)
  * [Incremental Learning via Rate Reduction](https://arxiv.org/abs/2011.14593)

<a name="37"/>

## 37. Continual Learning(æŒç»­å­¦ä¹ )

- [Training Networks in Null Space for Continual Learning]()<br>:open_mouth:oral:star:[code](https://github.com/ShipengWang/Adam-NSCL)
* [Efficient Feature Transformations for Discriminative and Generative Continual Learning](https://arxiv.org/abs/2103.13558)
* [Rainbow Memory: Continual Learning with a Memory of Diverse Samples](https://arxiv.org/abs/2103.17230) 
* [Rectification-based Knowledge Retention for Continual Learning](https://arxiv.org/abs/2103.16597) 
* [Layerwise Optimization by Gradient Decomposition for Continual Learning](https://arxiv.org/abs/2105.07561)
* [Continual Learning via Bit-Level Information Preserving](https://arxiv.org/abs/2105.04444)<br>:star:[code](https://github.com/Yujun-Shi/BLIP)
* [Training Networks in Null Space of Feature Covariance for Continual Learning](https://arxiv.org/abs/2103.07113)<br>:open_mouth:oral
* [ORDisCo: Effective and Efficient Usage of Incremental Unlabeled Data for Semi-Supervised Continual Learning](https://arxiv.org/abs/2101.00407)

<a name="36"/>

## 36.Action Detection and Recognition(åŠ¨ä½œæ£€æµ‹ä¸è¯†åˆ«)

- [Coarse-Fine Networks for Temporal Activity Detection in Videos](https://arxiv.org/abs/2103.01302)<br>:star:[code](https://github.com/kkahatapitiya/Coarse-Fine-Networks)
- [3D CNNs with Adaptive Temporal Feature Resolutions](https://arxiv.org/abs/2011.08652)<br>:star:[code](https://github.com/SimilarityGuidedSampling/Similarity-Guided-Sampling):house:[project](https://similarityguidedsampling.github.io/)
- [Understanding the Robustness of Skeleton-based Action Recognition under Adversarial Attack](https://arxiv.org/abs/2103.05347)<br>:tv:[video](https://www.youtube.com/watch?v=DeMkN3efp9s)
- [BASAR:Black-box Attack on Skeletal Action Recognition](https://arxiv.org/abs/2103.05266)<br>:house:[project](http://drhewang.com/pages/AAHAR.html):tv:[video](https://www.youtube.com/watch?v=PjWgwnAkV8g)<br>è§£è¯»ï¼š[å¯¹æŠ—æ”»é˜²æ–°æ–¹å‘ï¼šåŠ¨ä½œè¯†åˆ«ç®—æ³•å®¹æ˜“è¢«æ”»å‡»ï¼](https://mp.weixin.qq.com/s/AKxGfguKZK5QAT_k0CdbtQ)
- [TDN: Temporal Difference Networks for Efficient Action Recognition]( https://arxiv.org/abs/2012.10071)<br>:star:[code](https://github.com/MCG-NJU/TDN)
- [ACTION-Net: Multipath Excitation for Action Recognition](https://arxiv.org/abs/2103.07372)<br>:star:[code](https://github.com/V-Sense/ACTION-Net)<br>è§£è¯»ï¼š[CVPR 2021 | ç”¨äºåŠ¨ä½œè¯†åˆ«ï¼Œå³æ’å³ç”¨ã€æ··åˆæ³¨æ„åŠ›æœºåˆ¶çš„ ACTION æ¨¡å—](https://mp.weixin.qq.com/s/L2_lkhKbVhW8fjAaDdsyWQ)<br>è§£è¯»ï¼š[CVPR 2021 ï½œé’ˆå¯¹å¼ºæ—¶åºä¾èµ–ï¼Œå³æ’å³ç”¨ã€æ··åˆæ³¨æ„åŠ›æœºåˆ¶çš„ ACTION æ¨¡å—](https://mp.weixin.qq.com/s/tonyk649KzU1Y_c6p8isuQ)
- [No frame left behind: Full Video Action Recognition](https://arxiv.org/abs/2103.15395)
* [Recognizing Actions in Videos from Unseen Viewpoints](https://arxiv.org/abs/2103.16516)
* [Beyond Short Clips: End-to-End Video-Level Learning with Collaborative Memories](https://arxiv.org/abs/2104.01198)
* [Motion Representations for Articulated Animation](https://arxiv.org/abs/2104.11280)<br>:star:[code](https://github.com/snap-research/articulated-animation):house:[project](https://snap-research.github.io/articulated-animation/):tv:[video](https://www.youtube.com/watch?v=gpBYN8t8_yY)
* [Home Action Genome: Cooperative Compositional Action Understanding](https://arxiv.org/abs/2105.05226)
* [Anticipating human actions by correlating past with the future with Jaccard similarity measures](https://arxiv.org/abs/2105.12414)
* [Graph-Based High-Order Relation Modeling for Long-Term Action Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Graph-Based_High-Order_Relation_Modeling_for_Long-Term_Action_Recognition_CVPR_2021_paper.pdf)
* [Representing Videos As Discriminative Sub-Graphs for Action Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Representing_Videos_As_Discriminative_Sub-Graphs_for_Action_Recognition_CVPR_2021_paper.pdf)
* [Three Birds with One Stone: Multi-Task Temporal Action Detection via Recycling Temporal Annotations](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Three_Birds_with_One_Stone_Multi-Task_Temporal_Action_Detection_via_CVPR_2021_paper.pdf)
* [Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization](https://arxiv.org/abs/2012.01405)<br>:star:[code](https://github.com/google-research/google-research/tree/master/poem) 
* [Spatio-temporal Contrastive Domain Adaptation for Action Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Spatio-temporal_Contrastive_Domain_Adaptation_for_Action_Recognition_CVPR_2021_paper.pdf) 
* [Deep Analysis of CNN-Based Spatio-Temporal Representations for Action Recognition](https://arxiv.org/abs/2010.11757)<br>:star:[code](https://github.com/IBM/action-recognition-pytorch)
* [Semi-Supervised Action Recognition With Temporal Contrastive Learning](https://arxiv.org/abs/2102.02751)<br>:star:[code](https://github.com/CVIR/TCL):house:[project](https://cvir.github.io/TCL/):tv:[video](https://www.youtube.com/watch?v=_qIYu3EU2kY)
* [WOAD: Weakly Supervised Online Action Detection in Untrimmed Videos](https://arxiv.org/abs/2006.03732)
* [BABEL: Bodies, Action and Behavior With English Labels](https://openaccess.thecvf.com/content/CVPR2021/papers/Punnakkal_BABEL_Bodies_Action_and_Behavior_With_English_Labels_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/abhinanda-punnakkal/BABEL):house:[project](https://babel.is.tue.mpg.de/):tv:[video](https://www.youtube.com/watch?v=BYWxvjKpCqA)
* åŠ¨ä½œå®šä½
  * [Few-Shot Transformation of Common Actions into Time and Space](https://arxiv.org/abs/2104.02439)<br>:star:[code](https://github.com/PengWan-Yang) 
* æ—¶åºåŠ¨ä½œå®šä½
  * [Modeling Multi-Label Action Dependencies for Temporal Action Localization](https://arxiv.org/abs/2103.03027)<br>:open_mouth:oral:star:[code](https://github.com/ptirupat/MLAD)<br>æå‡ºåŸºäºæ³¨æ„åŠ›çš„ç½‘ç»œæ¶æ„æ¥å­¦ä¹ è§†é¢‘ä¸­çš„åŠ¨ä½œä¾èµ–æ€§ï¼Œç”¨äºè§£å†³å¤šæ ‡ç­¾æ—¶é—´åŠ¨ä½œå®šä½ä»»åŠ¡ã€‚
  * [The Blessings of Unlabeled Background in Untrimmed Videos](https://arxiv.org/abs/2103.13183)<br>:star:[code](https://github.com/liuyuancv/WTAL_blessing)
  * [Temporal Context Aggregation Network for Temporal Action Proposal Refinement](https://arxiv.org/abs/2103.13141)
  * [Learning Salient Boundary Feature for Anchor-free Temporal Action Localization](https://arxiv.org/abs/2103.13137)<br>åŸºäºæ˜¾è‘—è¾¹ç•Œç‰¹å¾å­¦ä¹ çš„æ— é”šæ¡†æ—¶åºåŠ¨ä½œå®šä½<br>è§£è¯»ï¼š[10](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning](https://arxiv.org/abs/2103.16392)
  * [Action Unit Memory Network for Weakly Supervised Temporal Action Localization](https://arxiv.org/abs/2104.14135)
  * [Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization](https://arxiv.org/abs/2006.07976)<br>:star:[code](https://github.com/Siyu-C/ACAR-Net) 
  * [Uncertainty Guided Collaborative Training for Weakly Supervised Temporal Action Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Uncertainty_Guided_Collaborative_Training_for_Weakly_Supervised_Temporal_Action_Detection_CVPR_2021_paper.pdf)
* Video Actor Segmentation
  * [Collaborative Spatial-Temporal Modeling for Language-Queried Video Actor Segmentation](https://arxiv.org/abs/2105.06818)
* åŠ¨ä½œåˆ†å‰²
  * [Learning To Segment Actions From Visual and Language Instructions via Differentiable Weak Sequence Alignment](https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_Learning_To_Segment_Actions_From_Visual_and_Language_Instructions_via_CVPR_2021_paper.pdf) 
  * æ—¶åºåŠ¨ä½œåˆ†å‰²
    * [Temporal Action Segmentation from Timestamp Supervision](https://arxiv.org/abs/2103.06669)<br>:star:[code](https://github.com/ZheLi2020/TimestampActionSeg)
    * [Temporally-Weighted Hierarchical Clustering for Unsupervised Action Segmentation](https://arxiv.org/abs/2103.11264)<br>:star:[code](https://github.com/ssarfraz/FINCH-Clustering/tree/master/TW-FINCH)
  * æ— ç›‘ç£åŠ¨ä½œåˆ†å‰²
    * [Action Shuffle Alternating Learning for Unsupervised Action Segmentation](https://arxiv.org/abs/2104.02116)
  * ç›‘ç£åŠ¨ä½œåˆ†å‰²
   * [Anchor-Constrained Viterbi for Set-Supervised Action Segmentation](https://arxiv.org/abs/2104.02113)
  * è§†é¢‘åŠ¨ä½œåˆ†å‰²
    * [Global2Local: Efficient Structure Search for Video Action Segmentation](https://arxiv.org/abs/2101.00910)<br>:star:[code](https://github.com/ShangHua-Gao/G2L-search)<br>ä»å…¨å±€åˆ°å±€éƒ¨ï¼šé¢å‘è§†é¢‘åŠ¨ä½œåˆ†å‰²çš„é«˜æ•ˆç½‘ç»œç»“æ„æœç´¢<br>è§£è¯»ï¼š[19](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* Video Moment Localization(è§†é¢‘æ—¶åˆ»å®šä½)
  * [Structured Multi-Level Interaction Network for Video Moment Localization via Language Query](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Structured_Multi-Level_Interaction_Network_for_Video_Moment_Localization_via_Language_CVPR_2021_paper.pdf)   
* æ—¶ç©ºäº‹ä»¶å®šä½
  * [Multi-Shot Temporal Event Localization: A Benchmark](https://arxiv.org/abs/2012.09434)<br>:star:[code](https://github.com/xlliu7/MUSES):house:[project](https://songbai.site/muses/) 

<a name="35"/>

## 35.Image Clustering(å›¾åƒèšç±») 

- [Improving Unsupervised Image Clustering With Robust Learning](https://arxiv.org/abs/2012.11150)<br>:star:[code](https://github.com/deu30303/RUC)<br>åˆ©ç”¨é²æ£’å­¦ä¹ æ”¹è¿›æ— ç›‘ç£å›¾åƒèšç±»æŠ€æœ¯<br>
- [Jigsaw Clustering for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2104.00323)<br>:open_mouth:oral:star:[code](https://github.com/Jia-Research-Lab/JigsawClustering)
- [COMPLETER: Incomplete Multi-view Clustering via Contrastive Prediction](https://openaccess.thecvf.com/content/CVPR2021/papers/Lin_COMPLETER_Incomplete_Multi-View_Clustering_via_Contrastive_Prediction_CVPR_2021_paper.pdf)<br>:star:[code](https://pengxi.me/)

<a name="34"/>

## 34.Image Classification(å›¾åƒåˆ†ç±»)

- [Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels](https://arxiv.org/abs/2101.05022)<br>:star:[code](https://github.com/naver-ai/relabel_imagenet)<br>
- [Differentiable Patch Selection for Image Recognition](https://arxiv.org/abs/2104.03059)<br>:star:[code](https://github.com/google-research/google-research/tree/master/ptopk_patch_selection/)
- [Achieving Robustness in Classification Using Optimal Transport With Hinge Regularization](https://openaccess.thecvf.com/content/CVPR2021/papers/Serrurier_Achieving_Robustness_in_Classification_Using_Optimal_Transport_With_Hinge_Regularization_CVPR_2021_paper.pdf)
- [Are Labels Always Necessary for Classifier Accuracy Evaluation?](https://arxiv.org/abs/2007.02915)
* ç»†ç²’åº¦åˆ†ç±»
  * [Fine-grained Angular Contrastive Learning with Coarse Labels](https://arxiv.org/abs/2012.03515)<br>:open_mouth:oral<br>:star:[code](https://github.com/guybuk/ANCOR)<br>ä½¿ç”¨è‡ªç›‘ç£è¿›è¡Œ Coarse Labelsï¼ˆç²—æ ‡ç­¾ï¼‰çš„ç»†ç²’åº¦åˆ†ç±»æ–¹é¢çš„å·¥ä½œã€‚ç²—æ ‡ç­¾ä¸ç»†ç²’åº¦æ ‡ç­¾ç›¸æ¯”ï¼Œæ›´å®¹æ˜“å’Œæ›´ä¾¿å®œï¼Œå› ä¸ºç»†ç²’åº¦æ ‡ç­¾é€šå¸¸éœ€è¦åŸŸä¸“å®¶ã€‚
  * [Graph-based High-Order Relation Discovery for Fine-grained Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_Graph-Based_High-Order_Relation_Discovery_for_Fine-Grained_Recognition_CVPR_2021_paper.pdf)<br>åŸºäºç‰¹å¾é—´é«˜é˜¶å…³ç³»æŒ–æ˜çš„ç»†ç²’åº¦è¯†åˆ«æ–¹æ³•<br>è§£è¯»ï¼š[20](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [Few-Shot Classification with Feature Map Reconstruction Networks](https://arxiv.org/abs/2012.01506)<br>:star:[code](https://github.com/Tsingularity/FRN):tv:[video](https://www.youtube.com/watch?v=kbsRsbQKTRc)
  * [A Realistic Evaluation of Semi-Supervised Learning for Fine-Grained Classification](https://arxiv.org/abs/2104.00679)<br>:open_mouth:oral
  * [GLAVNet: Global-Local Audio-Visual Cues for Fine-Grained Material Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_GLAVNet_Global-Local_Audio-Visual_Cues_for_Fine-Grained_Material_Recognition_CVPR_2021_paper.pdf)
  * [Learning Deep Classifiers Consistent With Fine-Grained Novelty Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Cheng_Learning_Deep_Classifiers_Consistent_With_Fine-Grained_Novelty_Detection_CVPR_2021_paper.pdf)
  * [Your "Flamingo" is My "Bird": Fine-Grained, or Not](https://arxiv.org/abs/2011.09040)<br>:open_mouth:oral:star:[code](https://github.com/PRIS-CV/Fine-Grained-or-Not)
  * [Discrimination-Aware Mechanism for Fine-Grained Representation Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Discrimination-Aware_Mechanism_for_Fine-Grained_Representation_Learning_CVPR_2021_paper.pdf)
  * [Neural Prototype Trees for Interpretable Fine-grained Image Recognition](https://arxiv.org/abs/2012.02046)<br>:star:[code](https://github.com/M-Nauta/ProtoTree)
* å›¾åƒåˆ†ç±»
  * [MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition](https://arxiv.org/abs/2103.12579)<br>:star:[code](https://github.com/BIT-DA/MetaSAug)
  * [PML: Progressive Margin Loss for Long-tailed Age Classification](https://arxiv.org/abs/2103.02140)<br>
  * [Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification](https://arxiv.org/abs/2103.14267)<br>:house:[project](https://www.kaihan.org/HybridLT/)
  * [Capsule Network is Not More Robust than Convolutional Network](https://arxiv.org/abs/2103.15459)
  * [Model-Contrastive Federated Learning](https://arxiv.org/abs/2103.16257)<br>:star:[code](https://github.com/QinbinLi/MOON)
  * [Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets](https://arxiv.org/abs/2104.12690)<br>:open_mouth:oral:star:[code](https://github.com/fidler-lab/efficient-annotation-cookbook):house:[project](https://fidler-lab.github.io/efficient-annotation-cookbook/)
  * [Correlated Input-Dependent Label Noise in Large-Scale Image Classification](https://arxiv.org/abs/2105.10305)<br>:open_mouth:oral:star:[code](https://github.com/google/uncertainty-baselines/tree/master/baselines/imagenet)
  * [Towards Robust Classification Model by Counterfactual and Invariant Data Generation](https://arxiv.org/abs/2106.01127)<br>:star:[code](https://github.com/zzzace2000/robust_cls_model)
  *  [Dual-Stream Multiple Instance Learning Network for Whole Slide Image Classification With Self-Supervised Contrastive Learning](https://arxiv.org/abs/2011.08939)<br>:star:[code](https://github.com/binli123/dsmil-wsi)
  * [Generative Classifiers as a Basis for Trustworthy Image Classification](https://arxiv.org/abs/2007.15036)<br>:star:[code](https://github.com/VLL-HD/trustworthy_GCs)
  * [Synthesize-It-Classifier: Learning a Generative Classifier Through Recurrent Self-Analysis](https://openaccess.thecvf.com/content/CVPR2021/papers/Pal_Synthesize-It-Classifier_Learning_a_Generative_Classifier_Through_Recurrent_Self-Analysis_CVPR_2021_paper.pdf)
  * [Background Splitting: Finding Rare Classes in a Sea of Background](https://arxiv.org/abs/2008.12873)
  * [Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image Classification](https://arxiv.org/abs/2010.05785)
  * [Self-Supervised Wasserstein Pseudo-Labeling for Semi-Supervised Image Classification](https://openaccess.thecvf.com/content/CVPR2021/papers/Taherkhani_Self-Supervised_Wasserstein_Pseudo-Labeling_for_Semi-Supervised_Image_Classification_CVPR_2021_paper.pdf)
  * [DAP: Detection-Aware Pre-training with Weak Supervision](https://arxiv.org/abs/2103.16651)
* åŠç›‘ç£å›¾åƒåˆ†ç±»
  * [SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification](https://arxiv.org/abs/2103.16725)<br>:star:[code](https://github.com/zijian-hu/SimPLE)
* è§†è§‰è¯†åˆ« 
  * [Fair Feature Distillation for Visual Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Jung_Fair_Feature_Distillation_for_Visual_Recognition_CVPR_2021_paper.pdf)
  * é•¿å°¾è§†è§‰è¯†åˆ«
    * [Distribution Alignment: A Unified Framework for Long-tail Visual Recognition](https://arxiv.org/abs/2103.16370)<br>:star:[code](https://github.com/Megvii-BaseDetection/DisAlign)
    * [Improving Calibration for Long-Tailed Recognition](https://arxiv.org/abs/2104.00466)<br>:star:[code](https://github.com/Jia-Research-Lab/MiSLAS)
    * [Adversarial Robustness under Long-Tailed Distribution](https://arxiv.org/abs/2104.02703)<br>:open_mouth:oral:star:[code](https://github.com/wutong16/Adversarial_Long-Tail)
    * [Disentangling Label Distribution for Long-Tailed Visual Recognition](https://arxiv.org/abs/2012.00321)<br>:star:[code](https://github.com/hyperconnect/LADE)
    * [Long-Tailed Multi-Label Visual Recognition by Collaborative Training on Uniform and Re-Balanced Samplings](https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_Long-Tailed_Multi-Label_Visual_Recognition_by_Collaborative_Training_on_Uniform_and_CVPR_2021_paper.pdf) 
* ç‰©ä½“åˆ†ç±»
  * [Object Classification From Randomized EEG Trials](https://arxiv.org/abs/2004.06046)
* Nearest Neighbor Matching(æœ€è¿‘é‚»åŒ¹é…)
  * [Nearest Neighbor Matching for Deep Clustering](https://openaccess.thecvf.com/content/CVPR2021/papers/Dang_Nearest_Neighbor_Matching_for_Deep_Clustering_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/ZhiyuanDang/NNM) 
* OODæ£€æµ‹
  * [MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space](https://arxiv.org/abs/2105.01879)<br>:open_mouth:oral:star:[code](https://github.com/deeplearning-wisc/large_scale_ood)
  * [MOOD: Multi-level Out-of-distribution Detection](https://arxiv.org/abs/2104.14726)<br>:star:[code](https://github.com/deeplearning-wisc/MOOD)
  * [Out-of-Distribution Detection Using Union of 1-Dimensional Subspaces](https://openaccess.thecvf.com/content/CVPR2021/papers/Zaeemzadeh_Out-of-Distribution_Detection_Using_Union_of_1-Dimensional_Subspaces_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/zaeemzadeh/OOD)
 
<a name="33"/>

## 33.6D Pose Estimation(6Dä½å§¿ä¼°è®¡)

- [FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation](https://arxiv.org/abs/2103.02242)<br>:open_mouth:oral:star:[code](https://github.com/ethnhe/FFB6D)<br>
- [GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation](http://arxiv.org/abs/2102.12145)<br>:star:[code](https://github.com/THU-DA-6D-Pose-Group/GDR-Net)
- [FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism](https://arxiv.org/abs/2103.07054)<br>:open_mouth:oral:star:[code](https://github.com/DC1991/FS-Net)
- [Wide-Depth-Range 6D Object Pose Estimation in Space](https://arxiv.org/abs/2104.00337)<br>:star:[code](https://github.com/cvlab-epfl/wide-depth-range-pose)
- [DSC-PoseNet: Learning 6DoF Object Pose Estimation via Dual-scale Consistency](https://arxiv.org/abs/2104.03658)
* [Single-view robot pose and joint angle estimation via render & compare](https://arxiv.org/abs/2104.09359)<br>:open_mouth:oral:star:[code](https://github.com/ylabbe/robopose):house:[project](https://www.di.ens.fr/willow/research/robopose/):tv:[video](https://www.youtube.com/watch?v=3yzwS99sgLI)
* [Keypoint-Graph-Driven Learning Framework for Object Pose Estimation](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Keypoint-Graph-Driven_Learning_Framework_for_Object_Pose_Estimation_CVPR_2021_paper.pdf)  
* [StablePose: Learning 6D Object Poses From Geometrically Stable Patches](https://arxiv.org/abs/2102.09334)

<a name="32"/>

## 32.View Synthesis(è§†å›¾åˆæˆ)

- [ID-Unet: Iterative Soft and Hard Deformation for View Synthesis](https://arxiv.org/abs/2103.02264)<br>:open_mouth:oral:star:[code](https://github.com/MingyuY/Iterative-view-synthesis)
- [NeX: Real-time View Synthesis with Neural Basis Expansion](https://arxiv.org/abs/2103.05606)<br>:open_mouth:oral:house:[project](https://nex-mpi.github.io/):tv:[video](https://www.youtube.com/watch?v=HyfkF7Z-ddA)<br>åˆ©ç”¨ç¥ç»åŸºç¡€æ‰©å±•çš„å®æ—¶è§†å›¾åˆæˆæŠ€æœ¯
- [Layout-Guided Novel View Synthesis from a Single Indoor Panorama](https://arxiv.org/abs/2103.17022)<br>:star:[code](https://github.com/bluestyle97/PNVS)
- [Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes](https://arxiv.org/abs/2104.06935)<br>:house:[project](https://virtualhumans.mpi-inf.mpg.de/)
- [Stable View Synthesis](https://arxiv.org/abs/2011.07233)<br>:star:[code](https://github.com/intel-isl/StableViewSynthesis)
* [Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes](https://arxiv.org/abs/2011.13084)<br>:house:[project](http://www.cs.cornell.edu/~zl548/NSFF/):tv:[video](https://youtu.be/qsMIH7gYRCc)

<a name="31"/>

## 31.Open-Set Recognition(å¼€æ”¾é›†è¯†åˆ«)

- [Counterfactual Zero-Shot and Open-Set Visual Recognition](https://arxiv.org/abs/2103.00887)<br>:star:[code](https://github.com/yue-zhongqi/gcm-cf)<br>
- [Few-shot Open-set Recognition by Transformation Consistency](https://arxiv.org/abs/2103.01537)<br>
- [Learning Placeholders for Open-Set Recognition](https://arxiv.org/abs/2103.15086)<br>:open_mouth:oral

<a name="30"/>

## 30.Neural rendering(ç¥ç»æ¸²æŸ“)

- [DeRF: Decomposed Radiance Fields](https://arxiv.org/abs/2011.12490)<br>:house:[project](https://ubc-vision.github.io/derf/)<br>
- [D-NeRF: Neural Radiance Fields for Dynamic Scenes](https://arxiv.org/abs/2011.13961)<br>:house:[project](https://www.albertpumarola.com/research/D-NeRF/index.html)<br>
* [Neural Lumigraph Rendering](https://arxiv.org/abs/2103.11571)<br>:sunflower:[dataset](https://drive.google.com/file/d/1BBpIfrqwZNYmG1TiFljlCnwsmL2OUxNT/view):house:[project](http://www.computationalimaging.org/publications/nlr/):tv:[video](https://www.youtube.com/watch?v=maVF-7x9644)<br>æ–¯å¦ç¦å¤§å­¦
* [AutoInt: Automatic Integration for Fast Neural Volume Rendering](https://arxiv.org/abs/2012.01714)<br>:open_mouth:oral:house:[project](http://www.computationalimaging.org/publications/automatic-integration/):tv:[video](https://youtu.be/GYxFYbih0PU)<br>æ–¯å¦ç¦å¤§å­¦
* [pixelNeRF: Neural Radiance Fields from One or Few Images](https://arxiv.org/abs/2012.02190)<br>:star:[code](https://github.com/sxyu/pixel-nerf):house:[project](https://alexyu.net/pixelnerf/):tv:[video](https://youtu.be/voebZx7f32g)
* [IBRNet: Learning Multi-View Image-Based Rendering](https://arxiv.org/abs/2102.13090)<br>:house:[project](https://ibrnet.github.io/)<br>å¤‡æ³¨ï¼šæœ‰å­¦è€…è¯„è®ºpixelNeRFå’ŒIBRNetçš„å·¥ä½œæ€æƒ³ç›¸è¿‘ï¼Œä½†IBRNetä¼¼ä¹æ›´åŠ æˆç†Ÿã€‚
* [Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans](https://arxiv.org/abs/2012.15838)<br>:star:[code](https://github.com/zju3dv/neuralbody):house:[project](https://zju3dv.github.io/neuralbody/):tv:[video](https://youtu.be/BPCAMeBCE-8)<br>æµ™å¤§ç­‰å­¦è€…å‘æ˜çš„Neural Bodyç®—æ³•ï¼Œè¾“å…¥å¤šè§’åº¦è§†é¢‘å¯è¾“å‡º3Däººä½“å’Œæ–°è§’åº¦è§†å›¾ã€‚
* [NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis](https://arxiv.org/abs/2012.03927)<br>:house:[project](https://pratulsrinivasan.github.io/nerv/):tv:[video](https://youtu.be/4XyDdvhhjVo)<br>åœ¨ä»»æ„ç…§æ˜æ¡ä»¶ä¸‹ï¼Œæ ¹æ®ä¸€ç»„è¾“å…¥å›¾åƒç”Ÿæˆå®Œæ•´çš„3Dåœºæ™¯
* [Self-Supervised Visibility Learning for Novel View Synthesis](https://arxiv.org/abs/2103.15407)<br>:star:[code](https://github.com/shiyujiao/SVNVS)
* [STaR: Self-Supervised Tracking and Reconstruction of Rigid Objects in Motion With Neural Rendering](https://arxiv.org/abs/2101.01602)<br>:star:[code](https://github.com/wentaoyuan):house:[project](https://wentaoyuan.github.io/star/):tv:[video](https://wentaoyuan.github.io/star/videos/overview.mp4)
* [Pulsar: Efficient Sphere-Based Neural Rendering](https://arxiv.org/abs/2004.07484)
* [Learning Compositional Radiance Fields of Dynamic Human Heads](https://arxiv.org/abs/2012.09955)<br>:open_mouth:oral:house:[project](https://ziyanw1.github.io/hybrid_nerf/)
* [NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections](https://openaccess.thecvf.com/content/CVPR2021/papers/Martin-Brualla_NeRF_in_the_Wild_Neural_Radiance_Fields_for_Unconstrained_Photo_CVPR_2021_paper.pdf)  
* [Neural Geometric Level of Detail: Real-Time Rendering With Implicit 3D Shapes](https://arxiv.org/abs/2101.10994)<br>:star:[code](https://github.com/nv-tlabs/nglod):house:[project](https://nv-tlabs.github.io/nglod/)
* [Space-Time Neural Irradiance Fields for Free-Viewpoint Video](https://arxiv.org/abs/2011.12950)<br>:house:[project](https://video-nerf.github.io/):tv:[video](https://youtu.be/2tN8ghNu2sI)   
* [Neural Scene Graphs for Dynamic Scenes](https://arxiv.org/abs/2011.10379)<br>:open_mouth:oral:house:[project](https://light.princeton.edu/publication/neural-scene-graphs/):tv:[video](https://youtu.be/ea4Y6P0Hk3o)
* [NeuTex: Neural Texture Mapping for Volumetric Neural Rendering](https://arxiv.org/abs/2103.00762) 

<a name="29"/>

## 29.Human Pose Estimation(äººä½“å§¿æ€ä¼°è®¡)

- [Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration](https://arxiv.org/abs/2103.02845)<br>:star:[code](https://github.com/SeanChenxy/HandMesh)<br>
- [Monocular Real-time Full Body Capture with Inter-part Correlations](https://arxiv.org/abs/2012.06087)<br>:tv:[video](https://www.youtube.com/watch?v=pAcywTUTv-E)<br>åœ¨ç”µå½±åŠ¨ä½œç‰¹æ•ˆä¸­ï¼Œäººä½“è¿åŠ¨æ•æ‰æ˜¯å…³é”®æŠ€æœ¯ï¼Œé«˜è´¨é‡çš„æ•æ‰å¾€å¾€éœ€è¦ç‰¹æ®Šè®¾å¤‡ï¼Œè€Œå¦‚æœèƒ½ä½¿ç”¨æ™®é€šRGBç›¸æœºè¿›è¡Œè¿åŠ¨æ•æ‰ï¼Œå°†ä¼šä½¿äººäººéƒ½æ˜¯ç‰¹æ•ˆå¸ˆã€‚è¯¥è§†é¢‘æ¥è‡ªæ¸…åã€é©¬æ™®æ‰€ç­‰å•ä½çš„å­¦è€…å‘è¡¨äºCVPR2021çš„è®ºæ–‡ç»“æœï¼Œä½¿ç”¨å•ç›®RGBç›¸æœºçš„åŠ¨ä½œæ•æ‰ã€‚
- [Behavior-Driven Synthesis of Human Dynamics](https://arxiv.org/abs/2103.04677)<br>:star:[code](https://github.com/CompVis/behavior-driven-video-synthesis):house:[project](https://compvis.github.io/behavior-driven-video-synthesis/)
- [Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation](https://arxiv.org/abs/2012.15175)<br>:star:[code](https://github.com/greatlog/SWAHR-HumanPose)<br>ç²—è§£ï¼š[2](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
- [Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression](https://arxiv.org/abs/2104.02300)<br>:star:[code](https://github.com/HRNet/DEKR)
- [SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks](https://arxiv.org/abs/2104.03313)<br>:open_mouth:oral:house:[project](https://scanimate.is.tue.mpg.de/)
- [On Self-Contact and Human Pose](https://arxiv.org/abs/2104.03176)<br>:house:[project](https://tuch.is.tue.mpg.de/)
- [Lite-HRNet: A Lightweight High-Resolution Network](https://arxiv.org/abs/2104.06403)<br>:star:[code](https://github.com/HRNet/)<br>è§£è¯»ï¼š[Lite-HRNetï¼šè½»é‡çº§HRNetï¼ŒFLOPså¤§å¹…ä¸‹é™](https://mp.weixin.qq.com/s/4V6EOYVSybMR9oxpcsWv9w)
- [Deep Dual Consecutive Network for Human Pose Estimation](https://arxiv.org/abs/2103.07254)<br>:star:[code](https://github.com/Pose-Group/DCPose)
- [3D Human Action Representation Learning via Cross-View Consistency Pursuit](https://arxiv.org/abs/2104.14466)<br>:star:[code](https://github.com/LinguoLi/CrosSCLR)
- [Body Meshes as Points](https://arxiv.org/abs/2105.02467)<br>:star:[code](https://github.com/jfzhang95/BMP)
- [Unsupervised Human Pose Estimation through Transforming Shape Templates](https://arxiv.org/abs/2105.04154)<br>:house:[project](https://infantmotion.github.io/)
- [When Human Pose Estimation Meets Robustness: Adversarial Algorithms and Benchmarks](https://arxiv.org/abs/2105.06152)
- [Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking](https://arxiv.org/abs/2106.03772) 
* 3Dæ‰‹éƒ¨é‡å»º
  * [Model-based 3D Hand Reconstruction via Self-Supervised Learning](https://arxiv.org/abs/2103.11703)<br>:star:[code](https://github.com/TerenceCYJ/S2HAND):tv:[video](https://www.youtube.com/watch?v=tuQzu-UfSe8&feature=youtu.be)
* äººä½“è¿åŠ¨è¿ç§»
  * [Few-Shot Human Motion Transfer by Personalized Geometry and Texture Modeling](https://arxiv.org/abs/2103.14338)<br>:star:[code](https://github.com/HuangZhiChao95/FewShotMotionTransfer):tv:[video](https://www.youtube.com/watch?v=ZJ15X-sdKSU)
* Human Volumetric Capture
  * [POSEFusion: Pose-guided Selective Fusion for Single-view Human Volumetric Capture](https://arxiv.org/abs/2103.15331)<br>:open_mouth:oral:house:[project](http://www.liuyebin.com/posefusion/posefusion.html)
  * [High-Fidelity Neural Human Motion Transfer from Monocular Video](https://arxiv.org/abs/2012.10974)
* 3Däººä½“å§¿æ€ä¼°è®¡
  * [CanonPose: Self-supervised Monocular 3D Human Pose Estimation in the Wild](https://arxiv.org/abs/2011.14679)<br>:star:[code](https://github.com/bastianwandt/CanonPose)
  * [Context Modeling in 3D Human Pose Estimation: A Unified Perspective](https://arxiv.org/abs/2103.15507)
  * [PCLs: Geometry-aware Neural Reconstruction of 3D Pose with Perspective Crop Layers](https://arxiv.org/abs/2011.13607)<br>:star:[code](https://github.com/yu-frank/PerspectiveCropLayers):tv:[video](https://twitter.com/i/status/1334395954644930560)<br>é€šè¿‡æ¶ˆé™¤ location-dependent é€è§†æ•ˆæœæ¥æ”¹è¿›3Däººä½“å§¿åŠ¿ä¼°è®¡æŠ€æœ¯å·¥ä½œã€‚<br>
  * [Graph Stacked Hourglass Networks for 3D Human Pose Estimation](https://arxiv.org/abs/2103.16385)
  * [Human POSEitioning System (HPS): 3D Human Pose Estimation and Self-localization in Large Scenes from Body-Mounted Sensors](https://arxiv.org/abs/2103.17265)<br>:open_mouth:oral:house:[project](http://virtualhumans.mpi-inf.mpg.de/hps/)
  * [SimPoE: Simulated Character Control for 3D Human Pose Estimation](https://arxiv.org/abs/2104.00683)<br>:open_mouth:oral:house:[project](https://www.ye-yuan.com/simpoe/)
  * [Reconstructing 3D Human Pose by Watching Humans in the Mirror](https://arxiv.org/abs/2104.00340)<br>:open_mouth:oral:star:[code](https://github.com/zju3dv/Mirrored-Human):house:[project](https://zju3dv.github.io/Mirrored-Human/)
  * [Multi-View Multi-Person 3D Pose Estimation with Plane Sweep Stereo](https://arxiv.org/abs/2104.02273)<br>:star:[code](https://github.com/jiahaoLjh/PlaneSweepPose)
  * [PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation](https://arxiv.org/abs/2105.02465)<br>:open_mouth:oral:star:[code](https://github.com/jfzhang95/PoseAug)
  * [AGORA: Avatars in Geography Optimized for Regression Analysis](https://arxiv.org/abs/2104.14643)<br>:house:[project](https://agora.is.tue.mpg.de/)
  * [Intelligent Carpet: Inferring 3D Human Pose From Tactile Signals](https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Intelligent_Carpet_Inferring_3D_Human_Pose_From_Tactile_Signals_CVPR_2021_paper.pdf)
  * [HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_HybrIK_A_Hybrid_Analytical-Neural_Inverse_Kinematics_Solution_for_3D_Human_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/Jeff-sjtu/HybrIK)
  * [Neural Descent for Visual 3D Human Pose and Shape](https://arxiv.org/abs/2008.06910)
  * [Probabilistic 3D Human Shape and Pose Estimation from Multiple Unconstrained Images in the Wild](https://arxiv.org/abs/2103.10978) 
* åŠ¨ç‰©å§¿æ€ä¼°è®¡
  * [From Synthetic to Real: Unsupervised Domain Adaptation for Animal Pose Estimation](https://arxiv.org/abs/2103.14843)<br>:open_mouth:oral:star:[code](https://github.com/chaneyddtt/UDA-Animal-Pose):tv:[video](https://www.youtube.com/watch?v=uF8BE9J7wNw)
* 3Däººä½“ç½‘æ ¼é…å‡†
  * [Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration](https://arxiv.org/abs/2104.08160)<br>:star:[code](https://github.com/taconite/PTF):house:[project](https://taconite.github.io/PTF/website/PTF.html):tv:[video](https://youtu.be/XNk4o2Z0S2c)
* å¤šäººäººä½“é‡å»º
  * [Multi-person Implicit Reconstruction from a Single Image](https://arxiv.org/abs/2104.09283)
* 3Däººä½“è¿åŠ¨
  * [We are More than Our Joints: Predicting how 3D Bodies Move](https://arxiv.org/pdf/2012.00619.pdf)<br>:house:[project](https://yz-cnsdqz.github.io/MOJO/MOJO.html):tv:[video](https://youtu.be/5DqLWAb37X0)<br>åˆ†äº«ä¼š
* äººä½“è¿åŠ¨æ•æ‰
  * [Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer RGBD Sensors](http://www.liuyebin.com/Function4D/assets/Function4D.pdf)<br>:open_mouth:oral:house:[project](http://www.liuyebin.com/Function4D/Function4D.html):tv:[video](https://www.youtube.com/watch?v=-rWUn4fEQNU)
  * [ChallenCap: Monocular 3D Capture of Challenging Human Performances Using Multi-Modal References](https://arxiv.org/abs/2103.06747)
* å¤šäººå§¿æ€ä¼°è®¡
  * [FCPose: Fully Convolutional Multi-Person Pose Estimation with Dynamic Instance-Aware Convolutions](https://arxiv.org/abs/2105.14185)<br>:star:[code](https://git.io/AdelaiDet)<br>FCPoseï¼Œæ—  ROI å’Œæ— åˆ†ç»„çš„ç«¯åˆ°ç«¯å¯è®­ç»ƒäººä½“å§¿åŠ¿ä¼°è®¡å™¨å¯ä»¥è¾¾åˆ°æ›´å¥½çš„å‡†ç¡®æ€§å’Œé€Ÿåº¦ï¼Œåœ¨ COCO æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ DLA-34 ä¸»å¹²çš„ FCPose å®æ—¶ç‰ˆæœ¬æ¯” Mask R-CNNï¼ˆResNet-101ï¼‰å¿« 4.5 å€ï¼ˆ41.67FPS vs. 9.26FPSï¼‰ï¼ŒåŒæ—¶å®ç°äº†æ€§èƒ½çš„æé«˜ã€‚ä¸æœ€è¿‘çš„è‡ªä¸Šè€Œä¸‹å’Œè‡ªä¸‹è€Œä¸Šçš„æ–¹æ³•ç›¸æ¯”ï¼ŒFCPose è¿˜å®ç°äº†æ›´å¥½çš„é€Ÿåº¦/å‡†ç¡®åº¦æƒè¡¡ã€‚
  * [Monocular 3D Multi-Person Pose Estimation by Integrating Top-Down and Bottom-Up Networks](https://arxiv.org/abs/2104.01797)<br>:star:[code](https://github.com/3dpose/3D-Multi-Person-Pose)
* æ‰‹-ç‰©äº¤äº’å§¿æ€ä¼°è®¡
  * [Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in Time](https://arxiv.org/abs/2106.05266)<br>:star:[code](https://github.com/stevenlsw/Semi-Hand-Object):house:[project](https://stevenlsw.github.io/Semi-Hand-Object/):tv:[video](https://youtu.be/7bnl2olUt-0)
* äººä½“å…³é”®ç‚¹æ£€æµ‹
  * [Regressive Domain Adaptation for Unsupervised Keypoint Detection](https://arxiv.org/abs/2103.06175)<br>:star:[code](https://github.com/thuml/Transfer-Learning-Library)
* 3Däººä½“å½¢çŠ¶
  * [LEAP: Learning Articulated Occupancy of People](https://arxiv.org/abs/2104.06849)<br>:star:[code](https://github.com/neuralbodies/leap):house:[project](https://neuralbodies.github.io/LEAP/):tv:[video](https://youtu.be/UVB8A_T5e3c)
  * [Beyond Static Features for Temporally Consistent 3D Human Pose and Shape From a Video](https://arxiv.org/abs/2011.08627)<br>:star:[code](https://github.com/hongsukchoi/TCMR_RELEASE):tv:[video](https://www.youtube.com/watch?v=WB3nTnSQDII)
* äººä½“åŠ¨ç”»ï¼ˆå§¿åŠ¿è¿ç§»ï¼‰
  * [Pose-Guided Human Animation From a Single Image in the Wild](https://arxiv.org/abs/2012.03796)
* åŸºäºäººä½“æ„Ÿåº”çš„3Då¥èº«è®­ç»ƒè‡ªåŠ¨ç³»ç»Ÿ
  * [AIFit: Automatic 3D Human-Interpretable Feedback Models for Fitness Training](https://openaccess.thecvf.com/content/CVPR2021/papers/Fieraru_AIFit_Automatic_3D_Human-Interpretable_Feedback_Models_for_Fitness_Training_CVPR_2021_paper.pdf)<br>:house:[project](http://vision.imar.ro/fit3d/)
* ä¸‰ç»´äººä½“è¿åŠ¨
  * [Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes](https://arxiv.org/abs/2012.05522)<br>:star:[code](https://github.com/jiashunwang/Long-term-Motion-in-3D-Scenes):house:[project](https://jiashunwang.github.io/Long-term-Motion-in-3D-Scenes/):tv:[video](https://youtu.be/qQ0GmCP1Ksw)
* ä¸‰ç»´äººä½“é‡å»º
  * [StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision](https://arxiv.org/abs/2104.05289)<br>:star:[code](https://github.com/CrisHY1995/StereoPIFu_Code):house:[project](https://hy1995.top/StereoPIFuProject/)
* æ‰‹åŠ¿åˆ°æ‰‹åŠ¿ç¿»è¯‘
  * [Model-Aware Gesture-to-Gesture Translation](https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Model-Aware_Gesture-to-Gesture_Translation_CVPR_2021_paper.pdf)
* 3Däººä½“è¿åŠ¨é¢„æµ‹
  * [Towards Accurate 3D Human Motion Prediction From Incomplete Observations](https://openaccess.thecvf.com/content/CVPR2021/papers/Cui_Towards_Accurate_3D_Human_Motion_Prediction_From_Incomplete_Observations_CVPR_2021_paper.pdf)
* æ‰‹åŠ¿è¯†åˆ«
  * [Body2Hands: Learning To Infer 3D Hands From Conversational Gesture Body Dynamics](https://arxiv.org/abs/2007.12287)<br>:star:[code](https://github.com/facebookresearch/body2hands):house:[project](http://people.eecs.berkeley.edu/~evonne_ng/projects/body2hands/):tv:[video](http://people.eecs.berkeley.edu/~evonne_ng/projects/body2hands/supp_pres_vCVPR_blur.mp4)
* ä¸‰ç»´äººä½“ç½‘æ ¼é‡å»º
  * [Holistic 3D Human and Scene Mesh Estimation From Single View Images](https://arxiv.org/abs/2012.01591)
* å¾®è§‚æ‰‹åŠ¿æƒ…æ„Ÿåˆ†æ
  * [iMiGUE: An Identity-Free Video Dataset for Micro-Gesture Understanding and Emotion Analysis](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_iMiGUE_An_Identity-Free_Video_Dataset_for_Micro-Gesture_Understanding_and_Emotion_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/linuxsino/iMiGUE)
* Dense Human Correspondences
  * [HumanGPS: Geodesic PreServing Feature for Dense Human Correspondences](https://arxiv.org/abs/2103.15573)<br>:star:[code](https://github.com/googleinterns/humangps):house:[project](https://feitongt.github.io/HumanGPS/):tv:[video](https://youtu.be/Ji34XtrJQ5o)

<a name="28"/>

## 28.Dense prediction(å¯†é›†é¢„æµ‹)

- [Densely connected multidilated convolutional networks for dense prediction tasks](https://arxiv.org/abs/2011.11844)<br>æå‡ºçš„D3Netåœ¨è¯­ä¹‰åˆ†å‰²&éŸ³ä¹æºåˆ†ç¦»ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºSOTAç½‘ç»œ<br>
- [Dense Contrastive Learning for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2011.09157)<br>:open_mouth:oral:star:[code](https://github.com/WXinlong/DenseCL)
* [Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2011.10043)<br>:star:[code](https://github.com/zdaxie/PixPro)
* [Densely Connected Multi-Dilated Convolutional Networks for Dense Prediction Tasks](https://openaccess.thecvf.com/content/CVPR2021/papers/Takahashi_Densely_Connected_Multi-Dilated_Convolutional_Networks_for_Dense_Prediction_Tasks_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/sony/ai-research-code/tree/master/d3net)

<a name="27"/>

## 27.Semantic Line Detection(è¯­ä¹‰çº¿æ£€æµ‹)
* [Harmonious Semantic Line Detection via Maximal Weight Clique Selection](https://arxiv.org/abs/2104.06903)<br>:star:[code](https://github.com/dongkwonjin)
 

<a name="26"/>

## 26.Video Processing(è§†é¢‘ç›¸å…³æŠ€æœ¯)
* [Skip-Convolutions for Efficient Video Processing](https://arxiv.org/abs/2104.11487)
* [VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples](https://arxiv.org/abs/2103.05905)<br>:star:[code](https://github.com/tinapan-pt/VideoMoCo)
* [Learning by Aligning Videos in Time](https://arxiv.org/abs/2103.17260)
* [Hierarchical Motion Understanding via Motion Programs](https://arxiv.org/abs/2104.11216)<br>:house:[project](https://sumith1896.github.io/motion2prog/):tv:[video](https://youtu.be/OpyY-s0LKAs)
* [Stochastic Image-to-Video Synthesis using cINNs](https://arxiv.org/abs/2105.04551)<br>:star:[code](https://github.com/CompVis/image2video-synthesis-using-cINNs):house:[project](https://compvis.github.io/image2video-synthesis-using-cINNs/)
* [Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions](https://arxiv.org/abs/2105.04489)<br>:house:[project](http://moments.csail.mit.edu/spoken.html)
* [Gradient Forward-Propagation for Large-Scale Temporal Video Modelling](https://arxiv.org/abs/2106.08318)
* [Learning To Reconstruct High Speed and High Dynamic Range Videos From Events](https://openaccess.thecvf.com/content/CVPR2021/papers/Zou_Learning_To_Reconstruct_High_Speed_and_High_Dynamic_Range_Videos_CVPR_2021_paper.pdf) 
* è§†é¢‘æ‘˜è¦
  * [Learning Discriminative Prototypes with Dynamic Time Warping](https://arxiv.org/abs/2103.09458)<br>:star:[code](https://github.com/BorealisAI/TSC-Disc-Proto)
  * [Learning Triadic Belief Dynamics in Nonverbal Communication from Videos](https://arxiv.org/abs/2104.02841)<br>:open_mouth:oral:star:[code](https://github.com/LifengFan/Triadic-Belief-Dynamics)
* è§†é¢‘ç¼–è§£ç 
  * [MetaSCI: Scalable and Adaptive Reconstruction for Video Compressive Sensing](https://arxiv.org/abs/2103.01786)<br>:star:[code](https://github.com/xyvirtualgroup/MetaSCI-CVPR2021)
  * [FVC: A New Framework towards Deep Video Compression in Feature Space](https://arxiv.org/abs/2105.09600)<br>:open_mouth:oral
  * [Memory-Efficient Network for Large-Scale Video Compressive Sensing](https://arxiv.org/abs/2103.03089)<br>:star:[code](https://github.com/BoChenGroup/RevSCI-net)
  * [Deep Learning in Latent Space for Video Prediction and Compression](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Deep_Learning_in_Latent_Space_for_Video_Prediction_and_Compression_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/BowenL0218/Video_Compression)
* è§†é¢‘æ’å¸§
  * [FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation](https://arxiv.org/pdf/2012.08512.pdf)<br>:star:[code](https://tarun005.github.io/FLAVR/Code):house:[project](https://tarun005.github.io/FLAVR/)<br>
  * [Deep Animation Video Interpolation in the Wild](https://arxiv.org/abs/2104.02495)<br>:star:[code](https://github.com/lisiyao21/AnimeInterp/)
  * [TimeLens: Event-based Video Frame Interpolation](https://arxiv.org/abs/2106.07286)<br>:star:[code](https://github.com/uzh-rpg/rpg_timelens):sunflower:[dataset](http://rpg.ifi.uzh.ch/TimeLens.html):tv:[video](https://youtu.be/dVLyia-ezvo)
  * [Time Lens: Event-based Video Frame Interpolation](https://openaccess.thecvf.com/content/CVPR2021/papers/Tulyakov_Time_Lens_Event-Based_Video_Frame_Interpolation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/uzh-rpg/rpg_timelens):house:[project](http://rpg.ifi.uzh.ch/TimeLens.html):tv:[video](https://www.youtube.com/watch?v=dVLyia-ezvo)
* è§†é¢‘è¯­è¨€å­¦ä¹ ï¼ˆvideo-and-language learningï¼‰
  * [Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling](https://arxiv.org/pdf/2102.06183.pdf)<br>:open_mouth:oral:star:[code](https://github.com/jayleicn/ClipBERT)<br>
* è§†é¢‘é¢„æµ‹
  * [Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction](https://arxiv.org/abs/2103.04174)<br>:house:[project](https://sites.google.com/view/ghvae):tv:[video](https://youtu.be/C8_-z8SEGOU)
  * [Learning Semantic-Aware Dynamics for Video Prediction](https://arxiv.org/abs/2104.09762)
  * [Video Prediction Recalling Long-term Motion Context via Memory Alignment Learning ](https://arxiv.org/abs/2104.00924)<br>:star:[code](https://github.com/sangmin-git/LMC-Memory)<br>è§£è¯»ï¼š[å¼•å…¥è®°å¿†æ¨¡å—ï¼Œçªç ´é•¿è·ç¦»ä¾èµ–è§†é¢‘é¢„æµ‹çš„æ€§èƒ½ç“¶é¢ˆ](https://mp.weixin.qq.com/s/GXcoHk9ks_ekVv-o14fVGg)
  * [Learning Goals from Failure](https://arxiv.org/abs/2006.15657)<br>:star:[code](https://github.com/cvlab-columbia/aha):house:[project](https://aha.cs.columbia.edu/)
  * [MotionRNN: A Flexible Model for Video Prediction With Spacetime-Varying Motions](https://arxiv.org/abs/2103.02243)
* è§†é¢‘ç†è§£
  * [Context-aware Biaffine Localizing Network for Temporal Sentence Grounding](https://arxiv.org/abs/2103.11555)<br>:star:[code](https://github.com/liudaizong/CBLN)
  * [Co-Grounding Networks with Semantic Attention for Referring Expression Comprehension in Videos](https://arxiv.org/abs/2103.12346)<br>:house:[project](https://sijiesong.github.io/co-grounding/)
  * [Visual Semantic Role Labeling for Video Understanding](https://arxiv.org/abs/2104.00990)<br>:house:[project](https://vidsitu.org/)
  * [Temporal Query Networks for Fine-grained Video Understanding](https://arxiv.org/abs/2104.09496)<br>:open_mouth:oral:house:[project](https://www.robots.ox.ac.uk/~vgg/research/tqn/) 
  * [Shot Contrastive Self-Supervised Learning for Scene Boundary Detection](https://arxiv.org/abs/2104.13537)
  * [FrameExit: Conditional Early Exiting for Efficient Video Recognition](https://arxiv.org/abs/2104.13400)<br>:open_mouth:oral
  * [Towards Long-Form Video Understanding](https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Towards_Long-Form_Video_Understanding_CVPR_2021_paper.pdf)
* è§†é¢‘ç¼©æ”¾
  * [Video Rescaling Networks with Joint Optimization Strategies for Downscaling and Upscaling](https://arxiv.org/abs/2103.14858)<br>:star:[code](https://github.com/ding3820/MIMO-VRN):house:[project](https://ding3820.github.io/MIMO-VRN/)
* è§†é¢‘å¼‚å¸¸æ£€æµ‹
  * [MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection](https://arxiv.org/abs/2104.01633)
  * [Learning Normal Dynamics in Videos With Meta Prototype Network](https://arxiv.org/abs/2104.06689)<br>:star:[code](https://github.com/ktr-hubrt/MPN/)<br>[åˆå¥½åˆå¿«çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼Œå¼•å…¥å…ƒå­¦ä¹ çš„åŠ¨æ€åŸå‹å­¦ä¹ ç»„ä»¶](https://mp.weixin.qq.com/s/osEi-MtD6ViYT9_mzWDS-Q)
  * [Anomaly Detection in Video via Self-Supervised and Multi-Task Learning](https://arxiv.org/abs/2011.07491)
* è§†é¢‘å£°æºå®šä½
  * [Localizing Visual Sounds the Hard Way](https://arxiv.org/abs/2104.02691)<br>:star:[code](https://github.com/hche11/Localizing-Visual-Sounds-the-Hard-Way):house:[project](https://www.robots.ox.ac.uk/~vgg/research/lvs/)
* è§†é¢‘åˆ†æ
  * [Self-Supervised Learning for Semi-Supervised Temporal Action Proposal](https://arxiv.org/abs/2104.03214)<br>:star:[code](https://github.com/wangxiang1230/SSTAP)
* è§†é¢‘ç”Ÿæˆ
  * [Playable Video Generation](https://arxiv.org/abs/2101.12195)<br>:open_mouth:oral:star:[code](https://github.com/willi-menapace/PlayableVideoGeneration):house:[project](https://willi-menapace.github.io/playable-video-generation-website/):tv:[video](https://www.youtube.com/watch?v=QtDjSyZERpg)
  * [One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing](https://arxiv.org/abs/2011.15126)<br>:open_mouth:oral:star:[code](https://github.com/NVlabs/imaginaire):house:[project](https://nvlabs.github.io/face-vid2vid/):tv:[video](https://youtu.be/nLYg9Waw72U)<br>è§£è¯»ï¼š[é¢ è¦†è§†é¢‘å‹ç¼©çš„ä¸ä¸€å®šæ˜¯æ–°å‹ç¼©ç®—æ³•ï¼Œè€Œå¯èƒ½æ˜¯GANï¼è‹±ä¼Ÿè¾¾æ–°ç®—æ³•æœ€é«˜å‹ç¼©90%æµé‡](https://mp.weixin.qq.com/s/UpfgxiIaSU4iIjbrkS--zA)<br>Nvidiaçš„æ–°ç ”ç©¶ï¼Œä½¿ç”¨äººè„¸å…³é”®ç‚¹+GANé‡å»ºè§†é¢‘é€šè¯ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„H.264èŠ‚çœ90%æµé‡ã€‚ä»£ç æœªå¼€æºï¼Œä½†è‹±ä¼Ÿè¾¾çš„GANæ¡†æ¶å¼€æºäº†ã€‚
* è§†é¢‘è§†è§’åˆ‡æ¢
  * [Ego-Exo: Transferring Visual Representations from Third-person to First-person Videos](https://arxiv.org/abs/2104.07905)
* Action Selection Learning
  * [Weakly Supervised Action Selection Learning in Video](https://arxiv.org/abs/2105.02439)<br>:star:[code](https://github.com/layer6ai-labs/ASL)
* è§†é¢‘æè¿°
  * [Towards Diverse Paragraph Captioning for Untrimmed Videos](https://arxiv.org/abs/2105.14477)<br>:star:[code](https://github.com/syuqings/video-paragraph)
* è§†é¢‘åˆ†ç±»
  * [Over-the-Air Adversarial Flickering Attacks Against Video Recognition Networks](https://arxiv.org/abs/2002.05123)<br>:star:[code](https://github.com/roiponytch/Flickering_Adversarial_Video)
* è§†é¢‘å­—å¹•
  * [Sketch, Ground, and Refine: Top-Down Dense Video Captioning](https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_Sketch_Ground_and_Refine_Top-Down_Dense_Video_Captioning_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/bearcatt/SGR)
* Video Grounding
  * [Cascaded Prediction Network via Segment Tree for Temporal Video Grounding](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_Cascaded_Prediction_Network_via_Segment_Tree_for_Temporal_Video_Grounding_CVPR_2021_paper.pdf)
  * [Interventional Video Grounding With Dual Contrastive Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Nan_Interventional_Video_Grounding_With_Dual_Contrastive_Learning_CVPR_2021_paper.pdf)
* è§†é¢‘ä¿®å¤
  * [Progressive Temporal Feature Alignment Network for Video Inpainting](https://arxiv.org/abs/2104.03507)<br>:star:[code](https://github.com/MaureenZOU/TSAM)<br>ä½œè€…æå‡º Progressive Temporal Feature Alignment Networkï¼Œåˆ©ç”¨å…‰æµä»ç›¸é‚»å¸§ä¸­æå–çš„ç‰¹å¾é€æ­¥ä¸°å¯Œå½“å‰å¸§çš„ç‰¹å¾ã€‚çº æ­£äº†æ—¶ç©ºç‰¹å¾ä¼ æ’­é˜¶æ®µçš„ spatial misalignmentï¼Œæå¤§åœ°æé«˜äº† inpainted videos çš„è§†è§‰è´¨é‡å’Œæ—¶ç©ºä¸€è‡´æ€§ã€‚åœ¨ DAVIS å’Œ FVI æ•°æ®é›†ä¸Šå®ç°äº†ä¸ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•ç›¸æ¯”çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚
  * [Restore From Restored: Video Restoration With Pseudo Clean Video](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Restore_From_Restored_Video_Restoration_With_Pseudo_Clean_Video_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/shlee0/RFR-video-denoising)
* è§†é¢‘å»æ¨¡ç³ŠåŒ–
  * [Gated Spatio-Temporal Attention-Guided Video Deblurring](https://openaccess.thecvf.com/content/CVPR2021/papers/Suin_Gated_Spatio-Temporal_Attention-Guided_Video_Deblurring_CVPR_2021_paper.pdf)
* è§†é¢‘å»å™ª
  * [Efficient Multi-Stage Video Denoising With Recurrent Spatio-Temporal Fusion](https://arxiv.org/abs/2103.05407)
* è§†é¢‘è´¨é‡è¯„ä¼°
  * [Patch-VQ: 'Patching Up' the Video Quality Problem](https://openaccess.thecvf.com/content/CVPR2021/papers/Ying_Patch-VQ_Patching_Up_the_Video_Quality_Problem_CVPR_2021_paper.pdf)<br>:house:[project](https://live.ece.utexas.edu/research.php)
* è§†é¢‘åŠ¨ä½œè®¡æ•°
  * [Repetitive Activity Counting by Sight and Sound](https://arxiv.org/abs/2103.13096)<br>:star:[code](https://github.com/xiaobai1217/RepetitionCounting):tv:[video](https://user-images.githubusercontent.com/22721775/112766700-2c7b7980-9013-11eb-8667-95ce6ec31067.mp4)
 * è§†é¢‘ç¨³å®š
  * [3D Video Stabilization With Depth Estimation by CNN-Based Optimization](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_3D_Video_Stabilization_With_Depth_Estimation_by_CNN-Based_Optimization_CVPR_2021_paper.pdf)<br>:tv:[video](https://www.youtube.com/watch?v=pMluFVA7NDQ)
  * [Real-Time Selfie Video Stabilization](https://arxiv.org/abs/2009.02007)<br>:star:[code](https://github.com/jiy173/selfievideostabilization)
* è§†é¢‘å»é›¨
  * [Self-Aligned Video Deraining With Transmission-Depth Consistency](https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_Self-Aligned_Video_Deraining_With_Transmission-Depth_Consistency_CVPR_2021_paper.pdf)
  * [Semi-Supervised Video Deraining With Dynamical Rain Generator](https://arxiv.org/abs/2103.07939)<br>:star:[code](https://github.com/zsyOAOA/S2VD)
* video looping technique
  * [Animating Pictures with Eulerian Motion Fields](https://arxiv.org/abs/2011.15128)<br>:house:[project](https://eulerian.cs.washington.edu/):tv:[video](https://www.youtube.com/watch?v=4zKliOMilGY)
* è§†é¢‘è¯†åˆ«
  * [2D or not 2D? Adaptive 3D Convolution Selection for Efficient Video Recognition](https://arxiv.org/abs/2012.14950)
  * [MoViNets: Mobile Video Networks for Efficient Video Recognition](https://arxiv.org/abs/2103.11511)<br>:star:[code](https://github.com/google-research)
* è¡Œä¸ºè¯†åˆ«
  * [Multi-Label Activity Recognition using Activity-specific Features and Activity Correlations](https://arxiv.org/abs/2009.07420)
* è§†é¢‘è¡¨å¾å­¦ä¹ 
  * [Spatiotemporal Contrastive Video Representation Learning](https://arxiv.org/abs/2008.03800)<br>:star:[code](https://github.com/tensorflow/models/tree/master/official/) 
  * [Removing the Background by Adding the Background: Towards Background Robust Self-Supervised Video Representation Learning](https://arxiv.org/abs/2009.05769) 
* è§†é¢‘ç¼–ç 
  * [Deep Perceptual Preprocessing for Video Coding](https://openaccess.thecvf.com/content/CVPR2021/papers/Chadha_Deep_Perceptual_Preprocessing_for_Video_Coding_CVPR_2021_paper.pdf)


<a name="25"/>

## 25.3D(ä¸‰ç»´è§†è§‰)

- [A Deep Emulator for Secondary Motion of 3D Characters](https://arxiv.org/abs/2103.01261)<br>:open_mouth:oral:house:[project](https://zhengmianlun.github.io/publications/deepEmulator.html)
- [Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction](https://arxiv.org/abs/2012.01451)<br>:open_mouth:oral:house:[project](https://aljazbozic.github.io/neural_deformation_graphs/):tv:[video](https://www.youtube.com/watch?v=vyq36eFkdWo)<br>
- [Deep Implicit Templates for 3D Shape Representation](https://arxiv.org/abs/2011.14565)<br>:open_mouth:oral:star:[code](https://github.com/ZhengZerong/DeepImplicitTemplates):house:[project](http://www.liuyebin.com/dit/dit.html):tv:[video](http://www.liuyebin.com/dit/assets/supp_vid.mp4)<br>[CVPR 2021 Oralï¼Œæ¸…åå­¦è€…æå‡ºDeep Implicit Templatesï¼Œæå¤§æ‰©å±•DIFèƒ½åŠ›](https://zhuanlan.zhihu.com/p/354737798)<br>
- [SMPLicit: Topology-aware Generative Model for Clothed People](https://arxiv.org/abs/2103.06871)<br>:house:[project](http://www.iri.upc.edu/people/ecorona/smplicit/)
- [Picasso: A CUDA-based Library for Deep Learning over 3D Meshes](https://arxiv.org/abs/2103.15076)<br>:star:[code](https://github.com/hlei-ziyan/Picasso)
- [Semi-supervised Synthesis of High-Resolution Editable Textures for 3D Humans](https://arxiv.org/abs/2103.17266)
* [RGB-D Local Implicit Function for Depth Completion of Transparent Objects](https://arxiv.org/abs/2104.00622)<br>:house:[project](https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit)
* [Deep Two-View Structure-from-Motion Revisited](https://arxiv.org/abs/2104.00556)
* [Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence](https://arxiv.org/abs/2011.13650)
* [S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling](https://arxiv.org/abs/2101.06571)
* [Deep Polarization Imaging for 3D Shape and SVBRDF Acquisition](https://arxiv.org/abs/2105.02875)<br>:open_mouth:oral:house:[project](https://wp.doc.ic.ac.uk/rgi/project/deep-polarization-3d-imaging/):tv:[video](https://youtu.be/QUTDlgF5ih0)
* [Learning Feature Aggregation for Deep 3D Morphable Models](https://arxiv.org/abs/2105.02173)<br>:star:[code](https://github.com/zxchen110/Deep3DMM/)
* [Plan2Scene: Converting Floorplans to 3D Scenes](https://arxiv.org/abs/2106.05375)<br>:star:[code](https://github.com/3dlg-hcvc/plan2scene):house:[project](https://3dlg-hcvc.github.io/plan2scene/):tv:[video](https://youtu.be/rTgnW7AobLs)
* [View Generalization for Single Image Textured 3D Models](https://arxiv.org/abs/2106.06533)<br>:house:[project](https://nv-adlr.github.io/view-generalization):tv:[video](https://youtu.be/4mU6Mb8hWpg)
* [Mirror3D: Depth Refinement for Mirror Surfaces](https://arxiv.org/abs/2106.06629)<br>:star:[code](https://github.com/3dlg-hcvc/mirror3d):house:[project](https://3dlg-hcvc.github.io/mirror3d/#/)
* [Learning To Recover 3D Scene Shape From a Single Image](https://arxiv.org/abs/2012.09365)<br>:star:[code](https://github.com/aim-uofa/AdelaiDepth)
* [Normal Integration via Inverse Plane Fitting With Minimum Point-to-Plane Distance](https://openaccess.thecvf.com/content/CVPR2021/papers/Cao_Normal_Integration_via_Inverse_Plane_Fitting_With_Minimum_Point-to-Plane_Distance_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/hoshino042/NormalIntegration)
* [Shelf-Supervised Mesh Prediction in the Wild](https://arxiv.org/abs/2102.06195)<br>:house:[project](https://judyye.github.io/ShSMesh/)
* [Unsupervised Learning of 3D Object Categories From Videos in the Wild](https://arxiv.org/abs/2103.16552)
* [DeepVideoMVS: Multi-View Stereo on Video With Recurrent Spatio-Temporal Fusion](https://arxiv.org/abs/2012.02177)<br>:star:[code](https://github.com/ardaduz/deep-video-mvs):tv:[video](https://www.youtube.com/watch?v=ikpotjxwcp4)
* [NeuroMorph: Unsupervised Shape Interpolation and Correspondence in One Go](https://openaccess.thecvf.com/content/CVPR2021/papers/Eisenberger_NeuroMorph_Unsupervised_Shape_Interpolation_and_Correspondence_in_One_Go_CVPR_2021_paper.pdf)
* [Learning Monocular 3D Reconstruction of Articulated Categories From Motion](https://arxiv.org/abs/2103.16352)<br>:star:[code](https://github.com/fkokkinos/acfm_video_3d_reconstruction):house:[project](https://fkokkinos.github.io/video_3d_reconstruction/)
* [Deep Active Surface Models](https://arxiv.org/abs/2011.08826)
* [Neural Splines: Fitting 3D Surfaces With Infinitely-Wide Neural Networks](https://arxiv.org/abs/2006.13782)<br>:open_mouth:oral:star:[code](https://github.com/fwilliams/neural-splines)
* [Learning View Selection for 3D Scenes](https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_Learning_View_Selection_for_3D_Scenes_CVPR_2021_paper.pdf)
* [StruMonoNet: Structure-Aware Monocular 3D Prediction](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_StruMonoNet_Structure-Aware_Monocular_3D_Prediction_CVPR_2021_paper.pdf)
* [Physically-Aware Generative Network for 3D Shape Modeling](https://openaccess.thecvf.com/content/CVPR2021/papers/Mezghanni_Physically-Aware_Generative_Network_for_3D_Shape_Modeling_CVPR_2021_paper.pdf)
* [Hybrid Rotation Averaging: A Fast and Robust Rotation Averaging Approach](https://arxiv.org/abs/2101.09116)
* [DeepSurfels: Learning Online Appearance Fusion](https://arxiv.org/abs/2012.14240)<br>:star:[code](https://github.com/onlinereconstruction/deep_surfels):house:[project](http://onlinereconstruction.github.io/DeepSurfels):tv:[video](https://youtu.be/ilq_5vy4q90)
* æ·±åº¦ä¼°è®¡
  * [PLADE-Net: Towards Pixel-Level Accuracy for Self-Supervised Single-View Depth Estimation with Neural Positional Encoding and Distilled Matting Loss](https://arxiv.org/abs/2103.07362)
  * [Beyond Image to Depth: Improving Depth Prediction using Echoes](https://arxiv.org/abs/2103.08468)<br>:star:[code](https://github.com/krantiparida/beyond-image-to-depth):house:[project](https://krantiparida.github.io/projects/bimgdepth.html)
  * [Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos](https://arxiv.org/abs/2103.03319)<br>:open_mouth:oral:star:[code](https://github.com/yasaminjafarian/HDNet_TikTok):house:[project](https://www.yasamin.page/hdnet_tiktok):tv:[video](https://youtu.be/EFJ8WXdKghs)
  * [LED2-Net: Monocular 360 Layout Estimation via Differentiable Depth Rendering](https://arxiv.org/abs/2104.00568)<br>:open_mouth:oral:star:[code](https://github.com/fuenwang/LED2-Net):house:[project](https://fuenwang.ml/project/led2net/)
  * [S2R-DepthNet: Learning a Generalizable Depth-specific Structural Representation](https://arxiv.org/abs/2104.00877)<br>:open_mouth:oral
  * [Depth Completion with Twin Surface Extrapolation at Occlusion Boundaries](https://arxiv.org/abs/2104.02253)<br>:star:[code](https://github.com/imransai/TWISE)
  * [Self-supervised Learning of Depth Inference for Multi-view Stereo](https://arxiv.org/abs/2104.02972)<br>:star:[code](https://github.com/JiayuYANG/Self-supervised-CVP-MVSNet)
  * [SMD-Nets: Stereo Mixture Density Networks](https://arxiv.org/abs/2104.03866)<br>:star:[code](https://github.com/fabiotosi92/SMD-Nets)
  * [The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth](https://arxiv.org/abs/2104.14540)<br>:star:[code](https://github.com/nianticlabs/manydepth)
  * [Single Image Depth Estimation using Wavelet Decomposition](https://arxiv.org/abs/2106.02022)<br>:star:[code](https://github.com/nianticlabs/wavelet-monodepth)
  * [Differentiable Diffusion for Dense Depth Estimation from Multi-view Images](https://arxiv.org/abs/2106.08917)<br>:star:[code](https://github.com/brownvc/diffdiffdepth):house:[project](http://visual.cs.brown.edu/projects/diffdiffdepth-webpage/):tv:[video](http://visual.cs.brown.edu/projects/diffdiffdepth-webpage/video/diffdiffdepth_cvpr2021.mp4)
  * [SliceNet: Deep Dense Depth Estimation From a Single Indoor Panorama Using a Slice-Based Representation](https://openaccess.thecvf.com/content/CVPR2021/papers/Pintore_SliceNet_Deep_Dense_Depth_Estimation_From_a_Single_Indoor_Panorama_CVPR_2021_paper.pdf)
  * [AdaBins: Depth Estimation Using Adaptive Bins](https://arxiv.org/abs/2011.14141)
  * [Sparse Auxiliary Networks for Unified Monocular Depth Prediction and Completion](https://arxiv.org/abs/2103.16690)<br>:star:[code](https://github.com/TRI-ML/packnet-sfm)
  * [S3: Learnable Sparse Signal Superdensity for Guided Depth Estimation](https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_S3_Learnable_Sparse_Signal_Superdensity_for_Guided_Depth_Estimation_CVPR_2021_paper.pdf)
  * [LED2-Net: Monocular 360deg Layout Estimation via Differentiable Depth Rendering](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_LED2-Net_Monocular_360deg_Layout_Estimation_via_Differentiable_Depth_Rendering_CVPR_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/fuenwang/LED2-Net):house:[project](https://fuenwang.ml/project/led2net/)
  * [Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks](https://arxiv.org/abs/2011.13118)
  * [Robust Consistent Video Depth Estimation](https://arxiv.org/abs/2012.05901)<br>:house:[project](https://robust-cvd.github.io/):tv:[video](https://youtu.be/x-wHrYHJSm8)
  * å•ç›®æ·±åº¦ä¼°è®¡
    * [Monocular Depth Estimation via Listwise Ranking Using the Plackett-Luce Model](https://openaccess.thecvf.com/content/CVPR2021/papers/Lienen_Monocular_Depth_Estimation_via_Listwise_Ranking_Using_the_Plackett-Luce_Model_CVPR_2021_paper.pdf)
    * [Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging](https://arxiv.org/abs/2105.14021)<br>:star:[code](https://github.com/compphoto/BoostingMonocularDepth):house:[project](http://yaksoy.github.io/highresdepth/):tv:[video](https://youtu.be/lDeI17pHlqo)
    * [3D Packing for Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/1905.02693)<br>:open_mouth:oral:star:[code](https://github.com/TRI-ML/packnet-sfm)
  * æ·±åº¦é¢„æµ‹
    * [Single Image Depth Prediction With Wavelet Decomposition](https://openaccess.thecvf.com/content/CVPR2021/papers/Ramamonjisoa_Single_Image_Depth_Prediction_With_Wavelet_Decomposition_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/nianticlabs/wavelet-monodepth)
* ä¸‰ç»´é‡å»º
  * [Deep Implicit Moving Least-Squares Functions for 3D Reconstruction](https://arxiv.org/abs/2103.12266)<br>:star:[code](https://github.com/Andy97/DeepMLS)
  * [Bilevel Online Adaptation for Out-of-Domain Human Mesh Reconstruction](https://arxiv.org/abs/2103.16449)<br>:house:[project](https://sites.google.com/view/humanmeshboa)
  * [Learning Parallel Dense Correspondence from Spatio-Temporal Descriptors for Efficient and Robust 4D Reconstruction](https://arxiv.org/abs/2103.16341)<br>:star:[code](https://github.com/tangjiapeng/LPDC-Net)
  * [Fostering Generalization in Single-view 3D Reconstruction by Learning a Hierarchy of Local and Global Shape Priors](https://arxiv.org/abs/2104.00476)
  * [NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video](https://arxiv.org/abs/2104.00681)<br>:open_mouth:oral:star:[code](https://github.com/zju3dv/NeuralRecon):house:[project](https://zju3dv.github.io/neuralrecon/)
  * [Fully Understanding Generic Objects: Modeling, Segmentation, and Reconstruction](https://arxiv.org/abs/2104.00858)<br>:star:[code](https://github.com/liuf1990/Fully_3D_Object):house:[project](http://cvlab.cse.msu.edu/project-fully3dobject.html):tv:[video](https://youtu.be/2Km23OZaDGA)
  * [CodedStereo: Learned Phase Masks for Large Depth-of-field Stereo](https://arxiv.org/abs/2104.04641)<br>:open_mouth:oral
  * [SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements](https://arxiv.org/abs/2104.07660)<br>:house:[project](https://qianlim.github.io/SCALE):tv:[video](https://www.youtube.com/watch?v=-EvWqFCUb7U)
  * [LASR: Learning Articulated Shape Reconstruction from a Monocular Video](https://arxiv.org/abs/2105.02976)<br>:house:[project](https://lasr-google.github.io/)
  * [Sketch2Model: View-Aware 3D Modeling from Single Free-Hand Sketches](https://arxiv.org/abs/2105.06663)
  * [Birds of a Feather: Capturing Avian Shape Models from Images](https://arxiv.org/abs/2105.09396)<br>:house:[project](https://yufu-wang.github.io/aves/):tv:[video](https://youtu.be/TDR2LC7mFpw)
  * [Multi-view 3D Reconstruction of a Texture-less Smooth Surface of Unknown Generic Reflectance](https://arxiv.org/abs/2105.11599)<br>:star:[code](https://github.com/za-cheng/PM-PMVS/)
  * [Generative PointNet: Deep Energy-Based Learning on Unordered Point Sets for 3D Generation, Reconstruction and Classification](https://arxiv.org/abs/2004.01301)<br>:star:[code](https://github.com/fei960922/GPointNet):house:[project](http://www.stat.ucla.edu/~jxie/GPointNet/)
  * [From Points to Multi-Object 3D Reconstruction](https://arxiv.org/abs/2012.11575)
  * [DI-Fusion: Online Implicit 3D Reconstruction With Deep Priors](https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_DI-Fusion_Online_Implicit_3D_Reconstruction_With_Deep_Priors_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/huangjh-pub/di-fusion)
  * [D2IM-Net: Learning Detail Disentangled Implicit Fields From Single Images](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_D2IM-Net_Learning_Detail_Disentangled_Implicit_Fields_From_Single_Images_CVPR_2021_paper.pdf)
  * [Residential Floor Plan Recognition and Reconstruction](https://openaccess.thecvf.com/content/CVPR2021/papers/Lv_Residential_Floor_Plan_Recognition_and_Reconstruction_CVPR_2021_paper.pdf)
  * [Indoor Panorama Planar 3D Reconstruction via Divide and Conquer](https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_Indoor_Panorama_Planar_3D_Reconstruction_via_Divide_and_Conquer_CVPR_2021_paper.pdf)
  * [Single-View 3D Object Reconstruction from Shape Priors in Memory](https://arxiv.org/abs/2003.03711)
  * [Deep Optimized Priors for 3D Shape Modeling and Reconstruction](https://arxiv.org/abs/2012.07241)
  * [MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera](https://arxiv.org/abs/2011.11814)<br>:star:[code](https://github.com/Brummi/MonoRec):house:[project](https://vision.in.tum.de/research/monorec):tv:[video](https://youtu.be/-gDSBIm0vgk)
  * [PluckerNet: Learn to Register 3D Line Reconstructions ](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_PluckerNet_Learn_To_Register_3D_Line_Reconstructions_CVPR_2021_paper.pdf)
  * ä¸‰ç»´ç½‘æ ¼é‡å»º
    * [Self-Supervised 3D Mesh Reconstruction From Single Images](https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Self-Supervised_3D_Mesh_Reconstruction_From_Single_Images_CVPR_2021_paper.pdf)
* è¯­ä¹‰åœºæ™¯è¡¥å…¨
  * [Semantic Scene Completion via Integrating Instances and Scene in-the-Loop](https://arxiv.org/abs/2104.03640)<br>:star:[code](https://github.com/yjcaimeow/SISNet)
* ä¸‰ç»´å…³é”®ç‚¹
  * [KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control](https://arxiv.org/abs/2104.11224)<br>:open_mouth:oral:star:[code](https://github.com/tomasjakab/keypoint_deformer/):house:[project](https://tomasjakab.github.io/KeypointDeformer/):tv:[video](https://youtu.be/GdDX1ZFh1k0)
* ä¸‰ç»´å½¢çŠ¶è¡¥å…¨
  * [Unsupervised 3D Shape Completion through GAN Inversion](https://arxiv.org/abs/2104.13366)<br>:star:[code](https://github.com/junzhezhang/shape-inversion):house:[project](https://junzhezhang.github.io/projects/ShapeInversion/)
* ä¸‰ç»´å½¢çŠ¶é€‚é…
  * [Cuboids Revisited: Learning Robust 3D Shape Fitting to Single RGB Images](https://arxiv.org/abs/2105.02047)<br>:star:[code](https://github.com/fkluger/cuboids_revisited)
* ä¸‰ç»´å‹ç¼©
  * [Neural 3D Scene Compression via Model Compression](https://arxiv.org/abs/2105.03120)
* Stereo Matching-ç«‹ä½“åŒ¹é…
   * [A Decomposition Model for Stereo Matching](https://arxiv.org/abs/2104.07516)
* Depth Completion-æ·±åº¦è¡¥å…¨
   * [Depth Completion using Plane-Residual Representation](https://arxiv.org/abs/2104.07350)
   * [Radar-Camera Pixel Depth Association for Depth Completion](https://arxiv.org/abs/2106.02778)<br>:star:[code](https://github.com/longyunf/rc-pda)
* ä¸‰ç»´ç½‘æ ¼
  * [DeepMetaHandles: Learning Deformation Meta-Handles of 3D Meshes With Biharmonic Coordinates](https://arxiv.org/abs/2102.09105)<br>:open_mouth:oral:star:[code](https://github.com/Colin97/DeepMetaHandles)
* 3Då½¢çŠ¶
  * [DECOR-GAN: 3D Shape Detailization by Conditional Refinement](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_DECOR-GAN_3D_Shape_Detailization_by_Conditional_Refinement_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/czq142857/DECOR-GAN):tv:[oral video](https://www.youtube.com/watch?v=5Fg3RF45mGg):tv:[demo](https://www.youtube.com/watch?v=xIQ0aslpn8g)
* depth map fusion
  * [NeuralFusion: Online Depth Fusion in Latent Space](https://arxiv.org/abs/2011.14791)
* ç½‘æ ¼é‡å»º
  * [Learning Delaunay Surface Elements for Mesh Reconstruction](https://arxiv.org/abs/2012.01203)<br>:star:[code](https://github.com/mrakotosaon/dse-meshing)
* 3D morphable model(ä¸‰ç»´å½¢å˜æ¨¡å‹)
  * [i3DMM: Deep Implicit 3D Morphable Model of Human Heads](https://arxiv.org/abs/2011.14143)<br>:house:[project](http://gvv.mpi-inf.mpg.de/projects/i3DMM/):tv:[video](https://youtu.be/4pYzV3ButPY)

<a name="24"/> 

## 24.Reinforcement Learning(å¼ºåŒ–å­¦ä¹ )
- [Hierarchical and Partially Observable Goal-driven Policy Learning with Goals Relational Graph](https://arxiv.org/abs/2103.01350)<br>:star:[code](https://github.com/Xin-Ye-1/HRL-GRG):house:[project](https://xin-ye-1.github.io/HRL-GRG/)
- [Unsupervised Learning for Robust Fitting:A Reinforcement Learning Approach](https://arxiv.org/abs/2103.03501)
- [Unsupervised Visual Attention and Invariance for Reinforcement Learning](https://arxiv.org/abs/2104.02921)
* [Unsupervised Learning for Robust Fitting: A Reinforcement Learning Approach](https://arxiv.org/abs/2103.03501)<br>:star:[code](https://github.com/hagianga21/MaxCon_RL)
* [Iterative Shrinking for Referring Expression Grounding Using Deep Reinforcement Learning](https://arxiv.org/abs/2103.05187)<br>:star:[code](https://github.com/insomnia94/ISREG)

<a name="23"/> 

## 23.Autonomous Driving(è‡ªåŠ¨é©¾é©¶)

- [Patch-NetVLAD: Multi-Scale Fusion of Locally-Global Descriptors for Place Recognition](https://arxiv.org/abs/2103.01486)<br>:star:[code](https://github.com/QVPR/Patch-NetVLAD)<br>ECCV 2020 Facebook Mapillary Visual Place Recognition Challenge å† å†›æ–¹æ¡ˆ
- [AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles](https://arxiv.org/abs/2101.06549)
- [Self-Supervised Pillar Motion Learning for Autonomous Driving](https://arxiv.org/abs/2104.08683)<br>:star:[code](https://github.com/qcraftai/pillar-motion)
- [Learning by Watching](https://arxiv.org/abs/2106.05966)
- [Binary TTC: A Temporal Geofence for Autonomous Navigation](https://arxiv.org/abs/2101.04777)<br>:star:[code](https://github.com/NVlabs/BiTTC):tv:[video](https://www.youtube.com/watch?v=uUQJcjyerM4)
* [GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving](https://arxiv.org/abs/2101.06543)<br>:open_mouth:oral:house:[project](https://tmux.top/publication/geosim/):tv:[video](https://www.youtube.com/watch?v=_VLXc_VN0fE)
* è½¦é“çº¿é¢„æµ‹
  * [LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of Dynamic Agents](https://arxiv.org/abs/2104.00249)
  * [Divide-and-Conquer for Lane-Aware Diverse Trajectory Prediction](https://arxiv.org/abs/2104.08277)<br>:open_mouth:oral
  * [Focus on Local: Detecting Lane Marker from Bottom Up via Key Point](https://arxiv.org/abs/2105.13680)
  * [Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection](https://arxiv.org/abs/2010.12035)<br>:star:[code](https://github.com/lucastabelini/LaneATT)
* è½¨è¿¹é¢„æµ‹
  * [SGCN:Sparse Graph Convolution Network for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2104.01528)<br>:star:[code](https://github.com/shuaishiliu/SGCN)
  * [Pedestrian and Ego-Vehicle Trajectory Prediction From Monocular Camera](https://openaccess.thecvf.com/content/CVPR2021/papers/Neumann_Pedestrian_and_Ego-Vehicle_Trajectory_Prediction_From_Monocular_Camera_CVPR_2021_paper.pdf)<br>:star:[code](https://gitlab.com/lukeN86/pedFutureTracking)
  * [Trajectory Prediction With Latent Belief Energy-Based Model](https://arxiv.org/abs/2104.03086)<br>:star:[code](https://github.com/bpucla/lbebm)
  * [Shared Cross-Modal Trajectory Prediction for Autonomous Driving](https://arxiv.org/abs/2011.08436)<br>:open_mouth:oral
* äººä½“è½¨è¿¹é¢„æµ‹
  * [Interpretable Social Anchors for Human Trajectory Forecasting in Crowds](https://arxiv.org/abs/2105.03136)
  * [Introvert: Human Trajectory Prediction via Conditional 3D Attention](https://openaccess.thecvf.com/content/CVPR2021/papers/Shafiee_Introvert_Human_Trajectory_Prediction_via_Conditional_3D_Attention_CVPR_2021_paper.pdf)
  * [SGCN: Sparse Graph Convolution Network for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2104.01528)<br>:star:[code](https://github.com/shuaishiliu/SGCN)
* äº¤é€šåœºæ™¯
  * [SceneGen: Learning To Generate Realistic Traffic Scenes](https://arxiv.org/abs/2101.06541)
* è½¦è¾†é‡è¯†åˆ«
  * [PhD Learning: Learning With Pompeiu-Hausdorff Distances for Video-Based Vehicle Re-Identification](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_PhD_Learning_Learning_With_Pompeiu-Hausdorff_Distances_for_Video-Based_Vehicle_Re-Identification_CVPR_2021_paper.pdf)
* HD map reconstruction
  * [Projecting Your View Attentively: Monocular Road Scene Layout Estimation via Cross-View Transformation](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Projecting_Your_View_Attentively_Monocular_Road_Scene_Layout_Estimation_via_CVPR_2021_paper.pdf)
* HD å›¾ç”Ÿæˆ
    * [HDMapGen: A Hierarchical Graph Generative Model of High Definition Maps](https://openaccess.thecvf.com/content/CVPR2021/papers/Mi_HDMapGen_A_Hierarchical_Graph_Generative_Model_of_High_Definition_Maps_CVPR_2021_paper.pdf)
* è½¦è¾†æ£€æµ‹
  * [Robust Multimodal Vehicle Detection in Foggy Weather Using Complementary Lidar and Radar Signals](https://openaccess.thecvf.com/content/CVPR2021/papers/Qian_Robust_Multimodal_Vehicle_Detection_in_Foggy_Weather_Using_Complementary_Lidar_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/qiank10/MVDNet)
* è½¦è¾†å§¿æ€ä¼°è®¡
  * [Exploring intermediate representation for monocular vehicle pose estimation](https://arxiv.org/abs/2011.08464)<br>:star:[code](https://github.com/Nicholasli1995/EgoNet)

<a name="22"/> 

## 22.Medical Imaging(åŒ»å­¦å½±åƒ)

- [3D Graph Anatomy Geometry-Integrated Network for Pancreatic Mass Segmentation, Diagnosis, and Quantitative Patient Management](https://arxiv.org/abs/2012.04701)<br>ç”¨çº¯å¤šæ¨¡æ€ CT å½±åƒå¯æ›¿ä»£ç›®å‰ JHMI çš„éœ€è¦åšè‚¿ç˜¤åŒ–å­¦æ£€æµ‹å’Œ DNA æµ‹åº+åŒ»å­¦å½±åƒçš„ç»¼åˆå¤šæ¨¡æ€è¯Šæ–­æµç¨‹ï¼Œä»è¯Šæ–­å‡†ç¡®åº¦ä¸Šæœ‰å¯æ¯”è¾ƒæ€§ï¼Œå®šé‡è¯Šæ–­ç²¾åº¦æ›´ä¼˜<br>
- [Deep Lesion Tracker: Monitoring Lesions in 4D Longitudinal Imaging Studies](https://arxiv.org/abs/2012.04872)<br>:star:[code](https://github.com/JimmyCai91/DLT)<br>è‚¿ç˜¤å½±åƒé‡Œé¢æ™ºèƒ½ PACS è¾…åŠ©åŒ»ç”Ÿè¯»ç‰‡çš„é‡è¦åŠŸèƒ½<br>
- [Automatic Vertebra Localization and Identification in CT by Spine Rectification and Anatomically-constrained Optimization](https://arxiv.org/abs/2012.07947)<br>åŸºäºCT å½±åƒçš„éª¨æŠ˜/éª¨è´¨ç–æ¾ç³»ç»Ÿ<br>
- [Multi-institutional Collaborations for Improving Deep Learning-based Magnetic Resonance Image Reconstruction Using Federated Learning](https://arxiv.org/abs/2103.02148)<br>:star:[code](https://github.com/guopengf/FL-MRCM)<br>å¤šæœºæ„åˆä½œï¼Œåˆ©ç”¨è”åˆå­¦ä¹ æ”¹è¿›åŸºäºæ·±åº¦å­¦ä¹ çš„ç£å…±æŒ¯å›¾åƒé‡å»ºæŠ€æœ¯<br>
- [DeepTag: An Unsupervised Deep Learning Method for Motion Tracking on Cardiac Tagging Magnetic Resonance Images](https://arxiv.org/abs/2103.02772)<br>:open_mouth:oral:star:[code](https://github.com/DeepTag/cardiac_tagging_motion_estimation)<br>DeepTag: ä¸€ç§æ— ç›‘ç£çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¿ƒè„æ ‡è®°ç£å…±æŒ¯å›¾åƒçš„è¿åŠ¨è·Ÿè¸ª<br>
- [Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles](https://arxiv.org/abs/2103.05121)
* [XProtoNet: Diagnosis in Chest Radiography with Global and Local Explanations](https://arxiv.org/abs/2103.10663)
* [Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation](https://arxiv.org/abs/2106.06963)
* åŒ»å­¦å›¾åƒåˆ†å‰²
  * [FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space](https://arxiv.org/abs/2103.06030)<br>:star:[code](https://github.com/liuquande/FedDG-ELCFS)
  * [DoDNet: Learning to segment multi-organ and tumors from multiple partially labeled datasets](https://arxiv.org/abs/2011.10217)<br>:star:[code](https://github.com/jianpengz/DoDNet):sunflower:[dataset](https://github.com/aim-uofa/partially-labelled)
  * [DiNTS: Differentiable Neural Network Topology Search for 3D Medical Image Segmentation](https://arxiv.org/abs/2103.15954)<br>:open_mouth:oral
  * [DARCNN: Domain Adaptive Region-based Convolutional Neural Network for Unsupervised Instance Segmentation in Biomedical Images](https://arxiv.org/abs/2104.01325)<br>
  * [Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation](https://arxiv.org/abs/2104.13243)
  * [Learning Calibrated Medical Image Segmentation via Multi-Rater Agreement Modeling](https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Learning_Calibrated_Medical_Image_Segmentation_via_Multi-Rater_Agreement_Modeling_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/jiwei0921/MRNet/)
  * [clDice - A Novel Topology-Preserving Loss Function for Tubular Structure Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Shit_clDice_-_A_Novel_Topology-Preserving_Loss_Function_for_Tubular_Structure_CVPR_2021_paper.pdf)
* åŒ»å­¦å›¾åƒåˆæˆ
  * [Brain Image Synthesis with Unsupervised Multivariate Canonical CSCâ„“4Net](https://arxiv.org/abs/2103.11587)<br>:open_mouth:oral
  * [Brain Image Synthesis with Unsupervised Multivariate Canonical CSCâ„“4Net
](https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Brain_Image_Synthesis_With_Unsupervised_Multivariate_Canonical_CSCl4Net_CVPR_2021_paper.pdf)
* æ‰‹æœ¯æŠ€èƒ½è¯„ä¼°
  * [Towards Unified Surgical Skill Assessment](https://arxiv.org/abs/2106.01035)<br>:star:[code](https://github.com/Finspire13/Towards-Unified-Surgical-Skill-Assessment)
* å¾®åˆ›æ‰‹æœ¯
  * [Minimally Invasive Surgery for Sparse Neural Networks in Contrastive Manner](https://openaccess.thecvf.com/content/CVPR2021/papers/Yu_Minimally_Invasive_Surgery_for_Sparse_Neural_Networks_in_Contrastive_Manner_CVPR_2021_paper.pdf)
* æ”¾å°„çº¿æŠ¥å‘Šç”Ÿæˆ
  * [A Self-Boosting Framework for Automated Radiographic Report Generation](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_A_Self-Boosting_Framework_for_Automated_Radiographic_Report_Generation_CVPR_2021_paper.pdf)
  * [Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Exploring_and_Distilling_Posterior_and_Prior_Knowledge_for_Radiology_Report_CVPR_2021_paper.pdf)
* MRå›¾åƒé‡å»º
  * [MR Image Super-Resolution With Squeeze and Excitation Reasoning Attention Network](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_MR_Image_Super-Resolution_With_Squeeze_and_Excitation_Reasoning_Attention_Network_CVPR_2021_paper.pdf)
  * [Joint Deep Model-Based MR Image and Coil Sensitivity Reconstruction Network (Joint-ICNet) for Fast MRI](https://openaccess.thecvf.com/content/CVPR2021/papers/Jun_Joint_Deep_Model-Based_MR_Image_and_Coil_Sensitivity_Reconstruction_Network_CVPR_2021_paper.pdf)
* å…³é”®ç‚¹æ£€æµ‹ä¸è·Ÿè¸ª
  * [Reciprocal Landmark Detection and Tracking With Extremely Few Annotations](https://arxiv.org/abs/2101.11224)
* Xå…‰æ£€æµ‹
  * [Leveraging Large-Scale Weakly Labeled Data for Semi-Supervised Mass Detection in Mammograms](https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_Leveraging_Large-Scale_Weakly_Labeled_Data_for_Semi-Supervised_Mass_Detection_in_CVPR_2021_paper.pdf)

<a name="21"/> 

## 21.Transformer 

- [Transformer Interpretability Beyond Attention Visualization](https://arxiv.org/pdf/2012.09838.pdf)<br>:star:[code](https://github.com/hila-chefer/Transformer-Explainability)<br> 
- [MIST: Multiple Instance Spatial Transformer Network](https://arxiv.org/abs/1811.10725)<br>:star:[code](https://github.com/ubc-vision/mist)<br>è¯•å›¾ä»çƒ­å›¾ä¸­è¿›è¡Œå¯å¾®çš„top-Ké€‰æ‹©(MIST)ï¼ˆç›®å‰åœ¨è‡ªç„¶å›¾åƒä¸Šä¹Ÿæœ‰äº†ä¸€äº›ç»“æœï¼›) ç”¨å®ƒå¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•å®šä½ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œæ£€æµ‹å’Œåˆ†ç±»ï¼ˆå¹¶ä¸æ˜¯å®ƒå”¯ä¸€èƒ½åšçš„äº‹æƒ…!ï¼‰
- [Variational Transformer Networks for Layout Generation](https://arxiv.org/abs/2104.02416)
- [Lesion-Aware Transformers for Diabetic Retinopathy Grading](https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_Lesion-Aware_Transformers_for_Diabetic_Retinopathy_Grading_CVPR_2021_paper.pdf)
- [Gaussian Context Transformer](https://openaccess.thecvf.com/content/CVPR2021/papers/Ruan_Gaussian_Context_Transformer_CVPR_2021_paper.pdf)
* å°æ ·æœ¬åŠ¨ä½œè¯†åˆ«
  * [Temporal-Relational CrossTransformers for Few-Shot Action Recognition](https://arxiv.org/abs/2101.06184)<br>:star:[code](https://github.com/tobyperrett/TRX)
* ç›®æ ‡æ£€æµ‹
  * [UP-DETR: Unsupervised Pre-training for Object Detection with Transformers](https://arxiv.org/pdf/2011.09094.pdf)<br>:open_mouth:oral:star:[code](https://github.com/dddzg/up-detr)
  * å•æ ·æœ¬ç›®æ ‡æ£€æµ‹
    * [Adaptive Image Transformer for One-Shot Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Adaptive_Image_Transformer_for_One-Shot_Object_Detection_CVPR_2021_paper.pdf)
* å›¾åƒå¤„ç†
  * [Pre-Trained Image Processing Transformer](https://arxiv.org/pdf/2012.00364.pdf)<br>:star:[code](https://github.com/huawei-noah/Pretrained-IPT):star:[gitee](https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT)
* äººæœºäº¤äº’
  * [End-to-End Human Object Interaction Detection with HOI Transformer](https://arxiv.org/abs/2103.04503)<br>:star:[code](https://github.com/bbepoch/HoiTransformer)
  * [HOTR: End-to-End Human-Object Interaction Detection with Transformers](https://arxiv.org/abs/2104.13682)<br>:open_mouth:oral
* å›¾åƒåˆ†å‰²
  * è¯­ä¹‰åˆ†å‰²
    * [Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers](https://arxiv.org/abs/2012.15840)<br>:star:[code](https://github.com/fudan-zvg/SETR):house:[project](https://fudan-zvg.github.io/SETR/)<br>åŸºäºTransformersä»åºåˆ—åˆ°åºåˆ—çš„è§’åº¦é‡æ–°æ€è€ƒè¯­ä¹‰åˆ†å‰²<br>è§£è¯»ï¼š[16](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)<br>è§£è¯»ï¼š[Transformer åœ¨è¯­ä¹‰åˆ†å‰²ä¸­çš„åº”ç”¨ï¼Œæ›¾ä½ADE20K æ¦œé¦–ï¼ˆ44.42% mIoUï¼‰](https://zhuanlan.zhihu.com/p/341768446)
    * [Embedded Discriminative Attention Mechanism for Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Embedded_Discriminative_Attention_Mechanism_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2021_paper.pdf)
  * è§†é¢‘å®ä¾‹åˆ†å‰²
    * [VisTR: End-to-End Video Instance Segmentation with Transformers](https://arxiv.org/abs/2011.14503)<br>:open_mouth:oral:star:[code](https://github.com/Epiphqny/VisTR)
  * å…¨æ™¯åˆ†å‰²
    * [MaX-DeepLab: End-to-End Panoptic Segmentation With Mask Transformers](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_MaX-DeepLab_End-to-End_Panoptic_Segmentation_With_Mask_Transformers_CVPR_2021_paper.pdf)
* è·Ÿè¸ª
  * [Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking](https://arxiv.org/abs/2103.11681)<br>:open_mouth:oral:star:[code](https://github.com/594422814/TransformerTrack)<br>more:[Transformerå†è“„åŠ›ï¼Œè·Ÿè¸ªä»»åŠ¡ä¸­åˆ›æ–°é«˜ï¼Œæ¡¥æ¥ç‹¬ç«‹å¸§ï¼Œè·¨å¸§ä¼ é€’æ—¶åŸŸä¿¡æ¯ï¼ŒCVPR 2021 Oral](https://zhuanlan.zhihu.com/p/359237554)
  * [Transformer Tracking](https://arxiv.org/abs/2103.15436)<br>:star:[code](https://github.com/chenxin-dlut/TransT)
* åŠ¨ä½œé¢„æµ‹
  * [Multimodal Motion Prediction with Stacked Transformers](https://arxiv.org/abs/2103.11624)<br>:star:[code](https://github.com/Mrmoore98/mmTransformer-Multimodal-Motion-Prediction-with-Stacked-Transformers):house:[project](https://decisionforce.github.io/mmTransformer/):tv:[video](https://youtu.be/ytqS8dgVcx0)  
* Self-attentionè‡ªæ³¨æ„åŠ›æœºåˆ¶
  * [Scaling Local Self-Attention For Parameter Efficient Visual Backbones](https://arxiv.org/abs/2103.12731)<br>:open_mouth:oral<br>è§£è¯»ï¼š[è¶…è¶Šå·ç§¯çš„è‡ªæ³¨æ„åŠ›æ¨¡å‹ï¼Œè°·æ­Œã€UCä¼¯å…‹åˆ©æå‡ºHaloNet](https://mp.weixin.qq.com/s/EBg7_MM_c-Pi0G1fc8xaag)
* æ£€ç´¢
  * [Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning](https://arxiv.org/abs/2103.13061)<br>:star:[code](https://github.com/amzn/image-to-recipe-transformers)
* ç‰¹å¾åŒ¹é…
  * [LoFTR: Detector-Free Local Feature Matching with Transformers](https://arxiv.org/abs/2104.00680)<br>:star:[code](https://github.com/zju3dv/LoFTR):house:[project](https://zju3dv.github.io/loftr/)
* å§¿åŠ¿è¯†åˆ«
  * [Pose Recognition with Cascade Transformers](https://arxiv.org/abs/2104.06976)<br>:star:[code](https://github.com/mlpc-ucsd/PRTR)
* è‡ªåŠ¨é©¾é©¶
  * [Multi-Modal Fusion Transformer for End-to-End Autonomous Driving](https://arxiv.org/abs/2104.09224)<br>:star:[code](https://github.com/autonomousvision/transfuser)
* è§†è§‰è¯†åˆ«
  * [Bottleneck Transformers for Visual Recognition](https://arxiv.org/abs/2101.11605)
* Video Hashing
  * [Self-Supervised Video Hashing via Bidirectional Transformers](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Self-Supervised_Video_Hashing_via_Bidirectional_Transformers_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/Lily1994/BTH)
* è§†è§‰å’Œè¯­è¨€å¯¼èˆª
  * [Topological Planning With Transformers for Vision-and-Language Navigation](https://arxiv.org/abs/2012.05292)
* äººä½“å§¿æ€ä¸ç½‘æ ¼é‡å»º
  * [End-to-End Human Pose and Mesh Reconstruction with Transformers](https://arxiv.org/abs/2012.09760)<br>:star:[code](https://github.com/microsoft/MeshTransformer)
* ç›´çº¿æ®µæ£€æµ‹  
  * [Line Segment Detection Using Transformers Without Edges](https://arxiv.org/abs/2101.01909)<br>:open_mouth:oral:star:[code](https://github.com/mlpc-ucsd/LETR)
* å›¾åƒåˆ†ç±»
  * [General Multi-Label Image Classification With Transformers](https://arxiv.org/abs/2011.14027)
* æ—¶åºè¯­è¨€å®šä½
  * [Multi-Stage Aggregated Transformer Network for Temporal Language Localization in Videos](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Multi-Stage_Aggregated_Transformer_Network_for_Temporal_Language_Localization_in_Videos_CVPR_2021_paper.pdf)
* åœºæ™¯å¸ƒå±€
  * [LayoutTransformer: Scene Layout Generation With Conceptual and Spatial Diversity](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_LayoutTransformer_Scene_Layout_Generation_With_Conceptual_and_Spatial_Diversity_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/davidhalladay/LayoutTransformer)
* é¢éƒ¨åŠ¨ä½œå•å…ƒæ£€æµ‹
  * [Facial Action Unit Detection With Transformers](https://openaccess.thecvf.com/content/CVPR2021/papers/Jacob_Facial_Action_Unit_Detection_With_Transformers_CVPR_2021_paper.pdf)
* é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆ
  * [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841)<br>:open_mouth:oral:star:[code](https://github.com/CompVis/taming-transformers) 


<a name="20"/> 

## 20.Person Re-Identification(äººå‘˜é‡è¯†åˆ«)

- [Meta Batch-Instance Normalization for Generalizable Person Re-Identification](https://arxiv.org/abs/2011.14670)<br>:star:[code](https://github.com/bismex/MetaBIN)
- [Watching You: Global-guided Reciprocal Learning for Video-based Person Re-identification](https://arxiv.org/abs/2103.04337)
- [Joint Noise-Tolerant Learning and Meta Camera Shift Adaptation for Unsupervised Person Re-Identification](https://arxiv.org/abs/2103.04618)<br>:star:[code](https://github.com/FlyingRoastDuck/MetaCam_DSCE)
- [Intra-Inter Camera Similarity for Unsupervised Person Re-Identification](https://arxiv.org/abs/2103.11658)<br>:star:[code](https://github.com/SY-Xuan/IICS)<br>è®ºæ–‡å…¬å¼€
- [Anchor-Free Person Search](https://arxiv.org/abs/2103.11617)<br>:star:[code](https://github.com/daodaofr/AlignPS)
* [Lifelong Person Re-Identification via Adaptive Knowledge Accumulation](https://arxiv.org/abs/2103.12462)<br>:star:[code](https://github.com/TPCD/LifelongReID)
* [Group-aware Label Transfer for Domain Adaptive Person Re-identification](https://arxiv.org/abs/2103.12366)<br>:star:[code](https://github.com/zkcys001/UDAStrongBaseline)|[code](https://github.com/JDAI-CV/fast-reid)
* [Neural Feature Search for RGB-Infrared Person Re-Identification](https://arxiv.org/abs/2104.02366)
* [Combined Depth Space based Architecture Search For Person Re-identification](https://arxiv.org/abs/2104.04163)
* [Unsupervised Multi-Source Domain Adaptation for Person Re-Identification](https://arxiv.org/abs/2104.12961)<br>:open_mouth:oral
* [Spatial-Temporal Correlation and Topology Learning for Person Re-Identification in Videos](https://arxiv.org/abs/2104.08241)<br>:open_mouth:oral
* [BiCnet-TKS: Learning Efficient Spatial-Temporal Representation for Video Person Re-Identification](https://arxiv.org/abs/2104.14783)<br>:star:[code](https://github.com/blue-blue272/BiCnet-TKS)
* [Generalizable Person Re-identification with Relevance-aware Mixture of Experts](https://arxiv.org/abs/2105.09156)
* [Person30K: A Dual-Meta Generalization Network for Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2021/papers/Bai_Person30K_A_Dual-Meta_Generalization_Network_for_Person_Re-Identification_CVPR_2021_paper.pdf)
* [Prototype-Guided Saliency Feature Learning for Person Search](https://openaccess.thecvf.com/content/CVPR2021/papers/Kim_Prototype-Guided_Saliency_Feature_Learning_for_Person_Search_CVPR_2021_paper.pdf)
* [UnrealPerson: An Adaptive Pipeline Towards Costless Person Re-Identification](http://arxiv.org/abs/2012.04268)<br>:star:[code](https://github.com/FlyHighest/UnrealPerson)
* [Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification](https://arxiv.org/abs/2012.00417)<br>:star:[code](https://github.com/HeliosZhao/M3L)
* [Farewell to Mutual Information: Variational Distillation for Cross-Modal Person Re-Identification](https://arxiv.org/abs/2104.02862)
* [Learning 3D Shape Feature for Texture-Insensitive Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Learning_3D_Shape_Feature_for_Texture-Insensitive_Person_Re-Identification_CVPR_2021_paper.pdf)
* [Partial Person Re-Identification With Part-Part Correspondence Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/He_Partial_Person_Re-Identification_With_Part-Part_Correspondence_Learning_CVPR_2021_paper.pdf)
* [Coarse-To-Fine Person Re-Identification With Auxiliary-Domain Classification and Second-Order Information Bottleneck](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Coarse-To-Fine_Person_Re-Identification_With_Auxiliary-Domain_Classification_and_Second-Order_Information_Bottleneck_CVPR_2021_paper.pdf)
* [Unsupervised Pre-Training for Person Re-Identification](http://arxiv.org/abs/2012.03753)
* [Joint Generative and Contrastive Learning for Unsupervised Person Re-Identification](http://arxiv.org/abs/2012.09071)<br>:star:[code](https://github.com/chenhao2345/GCL):tv:[video](https://drive.google.com/file/d/1VCL3loaR3H_d_oel-XCsuKt78pXy06hQ/view)
* [Wide-Baseline Multi-Camera Calibration Using Person Re-Identification](https://arxiv.org/abs/2104.08568)
* [Watching You: Global-Guided Reciprocal Learning for Video-Based Person Re-Identification](https://arxiv.org/abs/2103.04337)<br>:star:[code](https://github.com/flysnowtiger/GRL)
* [Discover Cross-Modality Nuances for Visible-Infrared Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Discover_Cross-Modality_Nuances_for_Visible-Infrared_Person_Re-Identification_CVPR_2021_paper.pdf)
* [Person Re-identification using Heterogeneous Local Graph Attention Networks](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Person_Re-Identification_Using_Heterogeneous_Local_Graph_Attention_Networks_CVPR_2021_paper.pdf) 
* [Fine-Grained Shape-Appearance Mutual Learning for Cloth-Changing Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Fine-Grained_Shape-Appearance_Mutual_Learning_for_Cloth-Changing_Person_Re-Identification_CVPR_2021_paper.pdf)
* æ‹¥æŒ¤äººç¾¤è®¡æ•°
  * [Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT Benchmark for Crowd Counting](https://arxiv.org/abs/2012.04529)<br>:star:[code](https://github.com/chen-judge/RGBTCrowdCounting):house:[project](http://lingboliu.com/RGBT_Crowd_Counting.html)
  * [Cross-View Cross-Scene Multi-View Crowd Counting](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Cross-View_Cross-Scene_Multi-View_Crowd_Counting_CVPR_2021_paper.pdf)
  * [A Generalized Loss Function for Crowd Counting and Localization](https://openaccess.thecvf.com/content/CVPR2021/papers/Wan_A_Generalized_Loss_Function_for_Crowd_Counting_and_Localization_CVPR_2021_paper.pdf)
* åŸºäº Transformer
  * [Diverse Part Discovery: Occluded Person Re-identification with Part-Aware Transformer](https://arxiv.org/abs/2106.04095)
* è¡Œäººæ£€æµ‹
  * [Variational Pedestrian Detection](https://arxiv.org/abs/2104.12389)
  * [Generalizable Pedestrian Detection: The Elephant in the Room](https://arxiv.org/abs/2003.08799)<br>:star:[code](https://github.com/hasanirtiza/Pedestron)
* è¡Œäººè·Ÿè¸ª
  * [Tracking Pedestrian Heads in Dense Crowd](https://arxiv.org/abs/2103.13516)<br>:star:[code](https://github.com/Sentient07/HeadHunter):house:[project](https://project.inria.fr/crowdscience/project/dense-crowd-head-tracking/)
* æ­¥æ€è¯†åˆ«
  * [Cross-View Gait Recognition With Deep Universal Linear Embeddings](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Cross-View_Gait_Recognition_With_Deep_Universal_Linear_Embeddings_CVPR_2021_paper.pdf)

<a name="19"/> 

## 19.Quantization/Pruning/Knowledge Distillation/Model Compression(é‡åŒ–ã€å‰ªæã€è’¸é¦ã€æ¨¡å‹å‹ç¼©/æ‰©å±•ä¸ä¼˜åŒ–)

- [Learning Student Networks in the Wild](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Learning_Student_Networks_in_the_Wild_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/huawei-noah/Data-Efficient-Model-Compression)
- [ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network](https://arxiv.org/abs/2007.00992)<br>:star:[code](https://github.com/clovaai/rexnet)<br>
- [RepVGG: Making VGG-style ConvNets Great Again](https://arxiv.org/abs/2101.03697)<br>:star:[code](https://github.com/megvii-model/RepVGG)<br>
- [Coordinate Attention for Efficient Mobile Network Design](https://arxiv.org/abs/2103.02907)<br>:star:[code](https://github.com/Andrew-Qibin/CoordAttention)
* å‰ªæ
  * [Manifold Regularized Dynamic Network Pruning](https://arxiv.org/abs/2103.05861)
  * [Neural Response Interpretation through the Lens of Critical Pathways](https://arxiv.org/abs/2103.16886)<br>:star:[code](https://github.com/CAMP-eXplain-AI/PathwayGrad)|[code](https://github.com/CAMP-eXplain-AI/RoarTorch)
  * [Riggable 3D Face Reconstruction via In-Network Optimization](https://zqbai-jeremy.github.io/files/INORig.pdf)<br>:star:[code](https://github.com/zqbai-jeremy/INORig)
  * [Towards Compact CNNs via Collaborative Compression](https://arxiv.org/abs/2105.11228)
  * [BCNet: Searching for Network Width with Bilaterally Coupled Network](https://arxiv.org/abs/2105.10533)
  * [The Lottery Ticket Hypothesis for Object Recognition](https://arxiv.org/abs/2012.04643)
  * [Network Pruning via Performance Maximization](https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_Network_Pruning_via_Performance_Maximization_CVPR_2021_paper.pdf)
  * [Convolutional Neural Network Pruning With Structural Redundancy Reduction](https://arxiv.org/abs/2104.03438)
* æ¨¡å‹æ‰©å±•
  * [Fast and Accurate Model Scaling](https://arxiv.org/abs/2103.06877)<br>:star:[code](https://github.com/facebookresearch/pycls)
* é‡åŒ–  
  * [Learnable Companding Quantization for Accurate Low-bit Neural Networks](https://arxiv.org/abs/2103.07156)
  * [Diversifying Sample Generation for Accurate Data-Free Quantization](https://arxiv.org/abs/2103.01049)
  * [Zero-shot Adversarial Quantization](https://arxiv.org/abs/2103.15263)<br>:open_mouth:oral:star:[code](https://github.com/FLHonker/ZAQ-code)
  * [Network Quantization with Element-wise Gradient Scaling](https://arxiv.org/abs/2104.00903)<br>:star:[code](https://github.com/cvlab-yonsei/EWGS):house:[project](https://cvlab.yonsei.ac.kr/projects/EWGS/) 
  * [Automated Log-Scale Quantization for Low-Cost Deep Neural Networks](https://openaccess.thecvf.com/content/CVPR2021/papers/Oh_Automated_Log-Scale_Quantization_for_Low-Cost_Deep_Neural_Networks_CVPR_2021_paper.pdf)
  * [Optimal Quantization Using Scaled Codebook](https://openaccess.thecvf.com/content/CVPR2021/papers/Idelbayev_Optimal_Quantization_Using_Scaled_Codebook_CVPR_2021_paper.pdf)
  * [QPP: Real-Time Quantization Parameter Prediction for Deep Neural Networks](https://openaccess.thecvf.com/content/CVPR2021/papers/Kryzhanovskiy_QPP_Real-Time_Quantization_Parameter_Prediction_for_Deep_Neural_Networks_CVPR_2021_paper.pdf)
  * [Distribution-Aware Adaptive Multi-Bit Quantization](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_Distribution-Aware_Adaptive_Multi-Bit_Quantization_CVPR_2021_paper.pdf)
  * [Scalability vs. Utility: Do We Have to Sacrifice One for the Other in Data Importance Quantification?](https://arxiv.org/abs/1911.07128)<br>:star:[code](https://github.com/AI-secure/Shapley-Study)
  * [Permute, Quantize, and Fine-Tune: Efficient Compression of Neural Networks](https://openaccess.thecvf.com/content/CVPR2021/papers/Martinez_Permute_Quantize_and_Fine-Tune_Efficient_Compression_of_Neural_Networks_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/uber-research/permute-quantize-finetune) 
* çŸ¥è¯†è’¸é¦
  * [Refine Myself by Teaching Myself: Feature Refinement via Self-Knowledge Distillation](https://arxiv.org/abs/2103.08273)<br>:star:[code](https://github.com/MingiJi/FRSKD)
  * [Complementary Relation Contrastive Distillation](https://arxiv.org/abs/2103.16367)
  * [Distilling Knowledge via Knowledge Review](https://arxiv.org/abs/2104.09044)<br>:star:[code](https://github.com/Jia-Research-Lab/ReviewKD)
  * [Learning From the Master: Distilling Cross-Modal Advanced Knowledge for Lip Reading](https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Learning_From_the_Master_Distilling_Cross-Modal_Advanced_Knowledge_for_Lip_CVPR_2021_paper.pdf)
  * [Multi-Scale Aligned Distillation for Low-Resolution Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Qi_Multi-Scale_Aligned_Distillation_for_Low-Resolution_Detection_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/dvlab-research/MSAD)
  * [Tree-Like Decision Distillation](https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Tree-Like_Decision_Distillation_CVPR_2021_paper.pdf)
  * [Revisiting Knowledge Distillation: An Inheritance and Exploration Framework](https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Revisiting_Knowledge_Distillation_An_Inheritance_and_Exploration_Framework_CVPR_2021_paper.pdf) 
  * [Wasserstein Contrastive Representation Distillation](https://arxiv.org/abs/2012.08674)
  * [Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation](https://arxiv.org/abs/2007.01951)
  * [EvDistill: Asynchronous Events To End-Task Learning via Bidirectional Reconstruction-Guided Cross-Modal Knowledge Distillation](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_EvDistill_Asynchronous_Events_To_End-Task_Learning_via_Bidirectional_Reconstruction-Guided_Cross-Modal_CVPR_2021_paper.pdf)
* å¯é€†ç¥ç»ç½‘ç»œ
  * [Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks](https://arxiv.org/abs/2103.10429)<br>:house:[project](https://paschalidoud.github.io/neural_parts)
* æ¨¡å‹å‹ç¼©
  * [CDFI: Compression-Driven Network Design for Frame Interpolation](https://arxiv.org/abs/2103.10559)<br>:star:[code](https://github.com/tding1/CDFI)
  * [Towards Efficient Tensor Decomposition-Based DNN Model Compression With Optimization Framework](https://openaccess.thecvf.com/content/CVPR2021/papers/Yin_Towards_Efficient_Tensor_Decomposition-Based_DNN_Model_Compression_With_Optimization_Framework_CVPR_2021_paper.pdf)
* æ¨¡å‹ä¼˜åŒ–
  * [Rethinking Channel Dimensions for Efficient Model Design](https://arxiv.org/abs/2007.00992)<br>:star:[code](https://github.com/clovaai/rexnet)

<a name="18"/> 

## 18.Aerial/Drones/Satellite/RS Image(èˆªç©ºå½±åƒ/æ— äººæœº)

- [UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles](https://arxiv.org/abs/2104.00946)
- [Detection, Tracking, and Counting Meets Drones in Crowds: A Benchmark](https://arxiv.org/abs/2105.02440)<br>:star:[code](https://github.com/VisDrone/DroneCrowd)
- [SIPSA-Net: Shift-Invariant Pan Sharpening with Moving Object Alignment for Satellite Imagery](https://arxiv.org/abs/2105.02400)<br>:star:[code](https://github.com/brachiohyup/SIPSA)
* èˆªç©ºå½±åƒåˆ†å‰²
  * [PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation](https://arxiv.org/abs/2103.06564)<br>:star:[code](https://github.com/lxtGH/PFSegNets)
* èˆªç©ºå½±åƒæ£€æµ‹
  * [ReDet: A Rotation-equivariant Detector for Aerial Object Detection](https://arxiv.org/abs/2103.07733)<br>:star:[code](https://github.com/csuhan/ReDet)
* æ— äººæœºæ£€æµ‹
  * [Dogfight: Detecting Drones from Drones Videos](https://arxiv.org/abs/2103.17242)
* å¤šè§†è§’å«æ˜Ÿæ‘„å½±æµ‹é‡
  * [Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry](https://arxiv.org/abs/2104.09877)

<a name="17"/> 

## 17.Super-Resolution(è¶…åˆ†è¾¨ç‡)

- [Data-Free Knowledge Distillation For Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Data-Free_Knowledge_Distillation_for_Image_Super-Resolution_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/huawei-noah/Data-Efficient-Model-Compression)
- [AdderSR: Towards Energy Efficient Image Super-Resolution](https://arxiv.org/pdf/2009.08891.pdf)<br>:star:[code](https://github.com/huawei-noah/AdderNet)<br>
- [Cross-MPI: Cross-scale Stereo for Image Super-Resolution using Multiplane Images](https://arxiv.org/abs/2011.14631)<br>:house:[project](http://www.liuyebin.com/crossMPI/crossMPI.html):tv:[video](http://www.liuyebin.com/crossMPI/assets/supp_vid.mp4)<br>[CVPR 2021ï¼ŒCross-MPIä»¥åº•å±‚åœºæ™¯ç»“æ„ä¸ºçº¿ç´¢çš„ç«¯åˆ°ç«¯ç½‘ç»œï¼Œåœ¨å¤§åˆ†è¾¨ç‡ï¼ˆx8ï¼‰å·®è·ä¸‹ä¹Ÿå¯å®Œæˆé«˜ä¿çœŸçš„è¶…åˆ†è¾¨ç‡](https://zhuanlan.zhihu.com/p/354752197)
- [ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic](https://arxiv.org/abs/2103.04039)<br>:star:[code](https://github.com/Xiangtaokong/ClassSR)
* [Robust Reference-based Super-Resolution via CÂ²-Matching](https://arxiv.org/abs/2106.01863)<br>:star:[code](https://github.com/yumingj/C2-Matching)
* [GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution](https://arxiv.org/abs/2012.00739)<br>:open_mouth:oral:house:[project](https://ckkelvinchan.github.io/projects/GLEAN/)<br>è§£è¯»ï¼š[CVPR 2021 Oral | GLEAN: åŸºäºéšå¼ç”Ÿæˆåº“çš„é«˜å€ç‡å›¾åƒè¶…åˆ†è¾¨ç‡](https://mp.weixin.qq.com/s/ZdfoS_VkFfQVsRhA4g6Yog)
* [BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond](https://arxiv.org/abs/2012.02181)<br>:star:[code](https://github.com/ckkelvinchan/BasicVSR-IconVSR):house:[project](https://ckkelvinchan.github.io/projects/BasicVSR/)
* [Temporal Modulation Network for Controllable Space-Time Video Super-Resolution](https://arxiv.org/abs/2104.10642)<br>:star:[code](https://github.com/CS-GangXu/TMNet)[ä½œè€…ä¸»é¡µ](https://csjunxu.github.io/)<br>åŸºäºæ—¶ç©ºç‰¹å¾å¯æ§æ’å€¼çš„è§†é¢‘è¶…åˆ†è¾¨ç‡ç½‘ç»œ<br>è§£è¯»ï¼š[18](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* [Unsupervised Degradation Representation Learning for Blind Super-Resolution](https://arxiv.org/abs/2104.00416)<br>:star:[code](https://github.com/LongguangWang/DASR)
* [SRWarp: Generalized Image Super-Resolution under Arbitrary Transformation](https://arxiv.org/abs/2104.10325)<br>:star:[code](https://github.com/sanghyun-son/srwarp)
* [MASA-SR: Matching Acceleration and Spatial Adaptation for Reference-Based Image Super-Resolution](https://arxiv.org/abs/2106.02299)<br>:star:[code](https://github.com/dvlab-research/MASA-SR)<br>ä½œè€…æå‡ºç”¨äº RefSR çš„æ–°æ–¹æ³•ï¼šMASA ç½‘ç»œï¼ŒåŒ…å«ä¸¤ä¸ªæ–°è®¾è®¡çš„æ¨¡å—ã€‚å…¶ä¸­ Match ï¼ˆåŒ¹é…ï¼‰å’Œ Extractionï¼ˆæå–ï¼‰æ¨¡å—å¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬ã€‚Spatial Adaptationï¼ˆç©ºé—´é€‚åº”ï¼‰æ¨¡å—ç”¨æ¥å­¦ä¹  LR å’Œ Ref å›¾åƒä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ï¼Œå¹¶ä»¥ç©ºé—´é€‚åº”çš„æ–¹å¼å°†å‚è€ƒç‰¹å¾çš„åˆ†å¸ƒ remapsï¼ˆé‡æ–°æ˜ å°„ï¼‰ä¸º LRç‰¹å¾çš„åˆ†å¸ƒã€‚ä»¥æ­¤æ›´åŠ é²æ£’åœ°å¤„ç†ä¸åŒçš„å‚è€ƒå›¾åƒã€‚
* [Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark Dataset and Baseline](https://openaccess.thecvf.com/content/CVPR2021/papers/He_Towards_Fast_and_Accurate_Real-World_Depth_Super-Resolution_Benchmark_Dataset_and_CVPR_2021_paper.pdf)
* [Exploring Sparsity in Image Super-Resolution for Efficient Inference](https://arxiv.org/abs/2006.09603)<br>:star:[code](https://github.com/LongguangWang/SMSR)
* [Neural Side-by-Side: Predicting Human Preferences for No-Reference Super-Resolution Evaluation](https://openaccess.thecvf.com/content/CVPR2021/papers/Khrulkov_Neural_Side-by-Side_Predicting_Human_Preferences_for_No-Reference_Super-Resolution_Evaluation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/KhrulkovV/NeuralSBS)
* [Tackling the Ill-Posedness of Super-Resolution Through Adaptive Target Generation](https://openaccess.thecvf.com/content/CVPR2021/papers/Jo_Tackling_the_Ill-Posedness_of_Super-Resolution_Through_Adaptive_Target_Generation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/yhjo09/AdaTarget)
* [LAU-Net: Latitude Adaptive Upscaling Network for Omnidirectional Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_LAU-Net_Latitude_Adaptive_Upscaling_Network_for_Omnidirectional_Image_Super-Resolution_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/wangh-allen/LAU-Net)
* [Image Super-Resolution With Non-Local Sparse Attention](https://openaccess.thecvf.com/content/CVPR2021/papers/Mei_Image_Super-Resolution_With_Non-Local_Sparse_Attention_CVPR_2021_paper.pdf)
* [Unsupervised Real-World Image Super Resolution via Domain-Distance Aware Training](https://arxiv.org/abs/2004.01178)<br>:star:[code](https://github.com/ShuhangGu/DASR)
* [Single Pair Cross-Modality Super Resolution](https://arxiv.org/abs/2004.09965)
* [End-to-End Learning for Joint Image Demosaicing, Denoising and Super-Resolution](https://openaccess.thecvf.com/content/CVPR2021/papers/Xing_End-to-End_Learning_for_Joint_Image_Demosaicing_Denoising_and_Super-Resolution_CVPR_2021_paper.pdf) 
* [Learning Scene Structure Guidance via Cross-Task Knowledge Transfer for Single Depth Super-Resolution](https://arxiv.org/abs/2103.12955)
* [Deep Burst Super-Resolution](https://openaccess.thecvf.com/content/CVPR2021/papers/Bhat_Deep_Burst_Super-Resolution_CVPR_2021_paper.pdf)
* [Light Field Super-Resolution With Zero-Shot Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Cheng_Light_Field_Super-Resolution_With_Zero-Shot_Learning_CVPR_2021_paper.pdf)
* [Fast Bayesian Uncertainty Estimation and Reduction of Batch Normalized Single Image Super-Resolution Network](https://arxiv.org/abs/1903.09410)<br>:star:[code](https://github.com/aupendu/sr-uncertainty):house:[project](https://aupendu.github.io/sr-uncertainty)
* [Practical Single-Image Super-Resolution Using Look-Up Table](https://openaccess.thecvf.com/content/CVPR2021/papers/Jo_Practical_Single-Image_Super-Resolution_Using_Look-Up_Table_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/yhjo09/SR-LUT)
* [Interpreting Super-Resolution Networks With Local Attribution Maps](https://arxiv.org/abs/2011.11036)
* [Scene Text Telescope: Text-Focused Scene Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Scene_Text_Telescope_Text-Focused_Scene_Image_Super-Resolution_CVPR_2021_paper.pdf) 
* ç›²è¶…åˆ†è¾¨
  * [Learning the Non-Differentiable Optimization for Blind Super-Resolution](https://openaccess.thecvf.com/content/CVPR2021/papers/Hui_Learning_the_Non-Differentiable_Optimization_for_Blind_Super-Resolution_CVPR_2021_paper.pdf)
  * [Flow-based Kernel Prior with Application to Blind Super-Resolution](https://arxiv.org/abs/2103.15977)<br>:star:[code](https://github.com/JingyunLiang/FKP)
  * [KOALAnet: Blind Super-Resolution Using Kernel-Oriented Adaptive Local Adjustment](https://openaccess.thecvf.com/content/CVPR2021/papers/Kim_KOALAnet_Blind_Super-Resolution_Using_Kernel-Oriented_Adaptive_Local_Adjustment_CVPR_2021_paper.pdf)
* è§†é¢‘è¶…åˆ†è¾¨ç‡
  * [Space-Time Distillation for Video Super-Resolution](https://openaccess.thecvf.com/content/CVPR2021/papers/Xiao_Space-Time_Distillation_for_Video_Super-Resolution_CVPR_2021_paper.pdf)
  * [Turning Frequency to Resolution: Video Super-Resolution via Event Cameras](https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Turning_Frequency_to_Resolution_Video_Super-Resolution_via_Event_Cameras_CVPR_2021_paper.pdf)

<a name="16"/> 

## 16.Visual Question Answering(è§†è§‰é—®ç­”)

* [Counterfactual VQA: A Cause-Effect Look at Language Bias](https://arxiv.org/abs/2006.04315)<br>:star:[code](https://github.com/yuleiniu/cfvqa)
* [AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning](https://arxiv.org/abs/2103.16002)<br>:house:[project](https://cs.stanford.edu/people/ranjaykrishna/agqa/):tv:[video](https://youtu.be/6Rw1QF9Hono)
* [Domain-robust VQA with diverse datasets and methods but no target labels](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Domain-Robust_VQA_With_Diverse_Datasets_and_Methods_but_No_Target_CVPR_2021_paper.pdf)<br>:house:[project](https://people.cs.pitt.edu/~mzhang/domain-robust-vqa/)
* [Found a Reason for me? Weakly-supervised Grounded Visual Question Answering using Capsules](https://arxiv.org/abs/2105.04836)<br>:star:[code](https://github.com/aurooj/WeakGroundedVQA_Capsules)
* [Perception Matters: Detecting Perception Failures of VQA Models Using Metamorphic Testing](https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Perception_Matters_Detecting_Perception_Failures_of_VQA_Models_Using_Metamorphic_CVPR_2021_paper.pdf)
* [Roses Are Red, Violets Are Blue... but Should VQA Expect Them To?](https://openaccess.thecvf.com/content/CVPR2021/papers/Kervadec_Roses_Are_Red_Violets_Are_Blue..._but_Should_VQA_Expect_CVPR_2021_paper.pdf)<br>:sunflower:[dataset](https://github.com/gqa-ood/GQA-OOD)
* [Predicting Human Scanpaths in Visual Question Answering](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Predicting_Human_Scanpaths_in_Visual_Question_Answering_CVPR_2021_paper.pdf)
* [Separating Skills and Concepts for Novel Visual Question Answering](https://openaccess.thecvf.com/content/CVPR2021/papers/Whitehead_Separating_Skills_and_Concepts_for_Novel_Visual_Question_Answering_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/SpencerWhitehead/novelvqa)
* [How Transferable Are Reasoning Patterns in VQA?](https://arxiv.org/abs/2104.03656)<br>:star:[code](https://github.com/reasoningpatterns/reasoningpatterns.github.io):house:[project](https://reasoningpatterns.github.io/):tv:[video](https://reasoningpatterns.github.io/demo.mp4)
* [Explicit Knowledge Incorporation for Visual Reasoning](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Explicit_Knowledge_Incorporation_for_Visual_Reasoning_CVPR_2021_paper.pdf)
* [KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA](https://arxiv.org/abs/2012.11014)
* Image-Text Matching
  * [Discrete-Continuous Action Space Policy Gradient-Based Attention for Image-Text Matching](https://arxiv.org/abs/2104.10406) 
* è§†é¢‘é—®ç­”
  * [TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events](https://arxiv.org/abs/2103.15538)<br>:star:[code](https://github.com/SUTDCV/SUTD-TrafficQA)
  * [Bridge to Answer: Structure-aware Graph Interaction Network for Video Question Answering](https://arxiv.org/abs/2104.14085)
  * [NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions](https://arxiv.org/abs/2105.08276)<br>:star:[code](https://github.com/doc-doc/NExT-QA)
  * [Look Before You Speak: Visually Contextualized Utterances](https://arxiv.org/abs/2012.05710)
* äº¤é€šç›¸å…³VQA
  * [SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning Over Traffic Events](https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_SUTD-TrafficQA_A_Question_Answering_Benchmark_and_an_Efficient_Network_for_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/SUTDCV/SUTD-TrafficQA)

  
<a name="15"/> 

## 15.GAN
- [Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing](https://arxiv.org/abs/2104.14754)<br>:star:[code](https://github.com/naver-ai/StyleMapGAN)
- [Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs](https://arxiv.org/pdf/2011.14107.pdf)<br>:star:[code](https://github.com/a514514772/hijackgan)
- [Efficient Conditional GAN Transfer with Knowledge Propagation across Classes](https://arxiv.org/abs/2102.06696)<br>:star:[code](https://github.com/mshahbazi72/cGANTransfer)
- [Anycost GANs for Interactive Image Synthesis and Editing](https://arxiv.org/abs/2103.03243)<br>:star:[code](https://github.com/mit-han-lab/anycost-gan):house:[project](https://hanlab.mit.edu/projects/anycost-gan/):tv:[video](https://www.youtube.com/watch?v=_yEziPl9AkM&t=90s)<br>Anycost GANï¼Œå¯é€‚åº”å¹¿æ³›çš„ç¡¬ä»¶å’Œå»¶è¿Ÿè¦æ±‚ï¼Œä»¥åŠå®ç°äº¤äº’å¼å›¾åƒç¼–è¾‘
- [TediGAN: Text-Guided Diverse Image Generation and Manipulation](https://arxiv.org/abs/2012.03308)<br>:star:[code](https://github.com/weihaox/TediGAN):house:[project](https://xiaweihao.com/projects/tedigan/):tv:[video](https://www.youtube.com/watch?v=L8Na2f5viAM)
- [Generative Hierarchical Features from Synthesizing Images](https://arxiv.org/abs/2007.10379)<br>:open_mouth:oral:star:[code](https://github.com/genforce/ghfeat):house:[project](https://genforce.github.io/ghfeat/)<br>ä½œè€…ç§°é¢„è®­ç»ƒ GAN ç”Ÿæˆå™¨å¯ä»¥å½“ä½œæ˜¯ä¸€ç§å­¦ä¹ çš„å¤šå°ºåº¦æŸå¤±ã€‚ç”¨å®ƒè¿›è¡Œè®­ç»ƒå¯ä»¥å¸¦æ¥é«˜åº¦ç«äº‰çš„å±‚æ¬¡åŒ–å’Œåˆ†ç¦»çš„è§†è§‰ç‰¹å¾ï¼Œç§°ä¹‹ä¸ºç”Ÿæˆå±‚æ¬¡åŒ–ç‰¹å¾ï¼ˆGH-Featï¼‰ã€‚å¹¶è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒGH-Featä¸ä»…æœ‰åˆ©äºç”Ÿæˆæ€§ä»»åŠ¡ï¼Œæ›´é‡è¦çš„æ˜¯æœ‰åˆ©äºåˆ†è¾¨æ€§ä»»åŠ¡ï¼ŒåŒ…æ‹¬äººè„¸éªŒè¯ã€å…³é”®ç‚¹æ£€æµ‹ã€layout predictionã€è¿ç§»å­¦ä¹ ã€style mixingã€å›¾åƒç¼–è¾‘ç­‰ã€‚
- [Teachers Do More Than Teach: Compressing Image-to-Image Models](https://arxiv.org/abs/2103.03467)<br>:star:[code](https://github.com/snap-research/CAT)
- [PISE: Person Image Synthesis and Editing with Decoupled GAN](https://arxiv.org/abs/2103.04023)<br>:star:[code](https://github.com/Zhangjinso/PISE)
- [LOHO: Latent Optimization of Hairstyles via Orthogonalization](https://arxiv.org/abs/2103.03891)<br>:star:[code](https://github.com/dukebw/LOHO)
- [HumanGAN: A Generative Model of Humans Images](https://arxiv.org/abs/2103.06902)
- [HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms](https://arxiv.org/abs/2011.11731)<br>:star:[code](https://github.com/mahmoudnafifi/HistoGAN)
- [DivCo: Diverse Conditional Image Synthesis via Contrastive Generative Adversarial Network](https://arxiv.org/abs/2103.07893)<br>:star:[code](https://github.com/ruiliu-ai/DivCo)
* [pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis](https://arxiv.org/abs/2012.00926)<br>:open_mouth:oral:house:[project](https://marcoamonteiro.github.io/pi-GAN-website/):tv:[video](https://youtu.be/0HCdof9BGtw)<br>æ›´å¤šï¼š[æ–¯å¦ç¦å­¦è€…æå‡ºå‘¨æœŸæ€§éšå¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆÏ€-GANæˆ–pi-GANï¼‰ï¼Œç”¨äºé«˜è´¨é‡çš„3Dæ„ŸçŸ¥å›¾åƒåˆæˆ](https://zhuanlan.zhihu.com/p/336155077)<br>æ–¯å¦ç¦å¤§å­¦
* [ReMix: Towards Image-to-Image Translation with Limited Data](https://arxiv.org/abs/2103.16835)
* [Unsupervised Disentanglement of Linear-Encoded Facial Semantics](https://arxiv.org/abs/2103.16605)
* [Content-Aware GAN Compression](https://arxiv.org/abs/2104.02244)
* [Regularizing Generative Adversarial Networks under Limited Data](https://arxiv.org/abs/2104.03310)<br>:star:[code](https://github.com/google/lecam-gan):house:[project](https://hytseng0509.github.io/lecam-gan/)
* [Where and What? Examining Interpretable Disentangled Representations](https://arxiv.org/abs/2104.05622)<br>:star:[code](https://github.com/zhuxinqimac/PS-SC)
* [Few-shot Image Generation via Cross-domain Correspondence](https://arxiv.org/abs/2104.06820)<br>:house:[project](https://utkarshojha.github.io/)
* [DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort](https://arxiv.org/abs/2104.06490)<br>:open_mouth:oral
* [Surrogate Gradient Field for Latent Space Manipulation](https://arxiv.org/abs/2104.09065)
* [StylePeople: A Generative Model of Fullbody Human Avatars](https://arxiv.org/abs/2104.08363)<br>:house:[project](https://saic-violet.github.io/style-people/)
* [Ensembling with Deep Generative Views](https://arxiv.org/abs/2104.14551)<br>:star:[code](https://github.com/chail/gan-ensembling):house:[project](https://chail.github.io/gan-ensembling/)
* [Continuous Face Aging via Self-estimated Residual Age Embedding](https://arxiv.org/abs/2105.00020)
* [Blur, Noise, and Compression Robust Generative Adversarial Networks](https://arxiv.org/abs/2003.07849)
* [Adaptive Weighted Discriminator for Training Generative Adversarial Networks](https://arxiv.org/abs/2012.03149)<br>:star:[code](https://github.com/vasily789/adaptive-weighted-gans)
* [DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_DatasetGAN_Efficient_Labeled_Data_Factory_With_Minimal_Human_Effort_CVPR_2021_paper.pdf)<br>:house:[project](https://nv-tlabs.github.io/datasetGAN/)
* [House-GAN++: Generative Adversarial Layout Refinement Network towards Intelligent Computational Agent for Professional Architects](https://openaccess.thecvf.com/content/CVPR2021/papers/Nauata_House-GAN_Generative_Adversarial_Layout_Refinement_Network_towards_Intelligent_Computational_Agent_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/ennauata/houseganpp):house:[project](https://ennauata.github.io/houseganpp/page.html)
* [Roof-GAN: Learning To Generate Roof Geometry and Relations for Residential Houses](https://openaccess.thecvf.com/content/CVPR2021/papers/Qian_Roof-GAN_Learning_To_Generate_Roof_Geometry_and_Relations_for_Residential_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/yi-ming-qian/roofgan)
* [Exploring Adversarial Fake Images on Face Manifold](https://arxiv.org/abs/2101.03272)
* [Hyper-LifelongGAN: Scalable Lifelong Learning for Image Conditioned Generation](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhai_Hyper-LifelongGAN_Scalable_Lifelong_Learning_for_Image_Conditioned_Generation_CVPR_2021_paper.pdf)
* [GANmut: Learning Interpretable Conditional Space for Gamut of Emotions](https://openaccess.thecvf.com/content/CVPR2021/papers/dApolito_GANmut_Learning_Interpretable_Conditional_Space_for_Gamut_of_Emotions_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/stefanodapolito/GANmut)
* [StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation](https://arxiv.org/abs/2011.12799)
* [Positional Encoding As Spatial Inductive Bias in GANs](https://arxiv.org/abs/2012.05217)<br>:star:[code](https://github.com/open-mmlab/mmgeneration):house:[project](https://nbei.github.io/gan-pos-encoding.html)
* [Partition-Guided GANs](https://arxiv.org/abs/2104.00816)
* [3D Shape Generation With Grid-Based Implicit Functions](https://openaccess.thecvf.com/content/CVPR2021/papers/Ibing_3D_Shape_Generation_With_Grid-Based_Implicit_Functions_CVPR_2021_paper.pdf)
* [Linear Semantics in Generative Adversarial Networks](https://arxiv.org/abs/2104.00487)<br>:star:[code](https://github.com/AtlantixJJ/LinearGAN):house:[project](https://atlantixjj.github.io/LinearSemanticsGAN/):tv:[video](https://youtu.be/xcQqUJqu5WM)
* [Cross-Modal Contrastive Learning for Text-to-Image Generation](https://arxiv.org/abs/2101.04702)
* [Lifting 2D StyleGAN for 3D-Aware Face Generation](https://arxiv.org/abs/2011.13126)
* [Unsupervised Learning of Depth and Depth-of-Field Effect From Natural Images With Aperture Rendering Generative Adversarial Networks](https://openaccess.thecvf.com/content/CVPR2021/papers/Kaneko_Unsupervised_Learning_of_Depth_and_Depth-of-Field_Effect_From_Natural_Images_CVPR_2021_paper.pdf)<br>:open_mouth:oral:house:[project](http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/ar-gan/)
* [Training Generative Adversarial Networks in One Stage](https://arxiv.org/abs/2103.00430)<br>:star:[code](https://github.com/zju-vipa/OSGAN)
* [Self-Supervised Video GANs: Learning for Appearance Consistency and Motion Coherency](https://openaccess.thecvf.com/content/CVPR2021/papers/Hyun_Self-Supervised_Video_GANs_Learning_for_Appearance_Consistency_and_Motion_Coherency_CVPR_2021_paper.pdf) 
* [Closed-Form Factorization of Latent Semantics in GANs](https://arxiv.org/abs/2007.06600)<br>:open_mouth:oral:star:[code](https://github.com/genforce/sefa):house:[project](https://genforce.github.io/sefa/):tv:[video](https://youtu.be/OFHW2WbXXIQ)
* [Discovering Interpretable Latent Space Directions of GANs Beyond Binary Attributes](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Discovering_Interpretable_Latent_Space_Directions_of_GANs_Beyond_Binary_Attributes_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/BERYLSHEEP/AdvStyle)
* [Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement](https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Normalized_Avatar_Synthesis_Using_StyleGAN_and_Perceptual_Refinement_CVPR_2021_paper.pdf)
* [L2M-GAN: Learning To Manipulate Latent Space Semantics for Facial Attribute Editing](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_L2M-GAN_Learning_To_Manipulate_Latent_Space_Semantics_for_Facial_Attribute_CVPR_2021_paper.pdf)
* [Spatially-invariant Style-codes Controlled Makeup Transfer](https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_Spatially-Invariant_Style-Codes_Controlled_Makeup_Transfer_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/makeuptransfer/SCGAN)
* æ— ç›‘ç£å›¾åƒåˆæˆ
  * [Posterior Promoted GAN With Distribution Discriminator for Unsupervised Image Synthesis](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Posterior_Promoted_GAN_With_Distribution_Discriminator_for_Unsupervised_Image_Synthesis_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/bioinf-jku/TTUR)
* å›¾åƒåˆ°å›¾åƒç¿»è¯‘
  * [Memory-guided Unsupervised Image-to-image Translation](https://arxiv.org/abs/2104.05170)
  * [Image-to-image Translation via Hierarchical Style Disentanglement](https://arxiv.org/abs/2103.01456)<br>:open_mouth:oral:star:[code](https://github.com/imlixinyang/HiSD)<br>åœ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸Šå®ç°å±‚æ¬¡é£æ ¼è§£è€¦
  * [CoMoGAN: continuous model-guided image-to-image translation](https://arxiv.org/abs/2103.06879)<br>:open_mouth:oral:star:[code](https://github.com/cv-rits/CoMoGAN)
  * [Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation](https://arxiv.org/abs/2008.00951)<br>:star:[code](https://github.com/eladrich/pixel2style2pixel):house:[project](https://eladrich.github.io/pixel2style2pixel/)<br>
* å›¾åƒç¼–è¾‘
  * [StyleMapGAN: Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing](https://arxiv.org/abs/2104.14754)<br>:star:[code](https://github.com/naver-ai/StyleMapGAN):tv:[video](https://www.youtube.com/watch?v=qCapNyRA_Ng)
  * [Navigating the GAN Parameter Space for Semantic Image Editing](https://arxiv.org/abs/2011.13786)<br>:star:[code](https://github.com/yandex-research/navigan)
* äººè„¸å›¾åƒåˆæˆ
  * [TediGAN: Text-Guided Diverse Face Image Generation and Manipulation](https://arxiv.org/abs/2012.03308)<br>:star:[code](https://github.com/IIGROUP/TediGAN):tv:[video](https://www.youtube.com/watch?v=L8Na2f5viAM)


<a name="14"/> 

## 14.Few-Shot/Zero-Shot Learning,Domain Generalization/Adaptation(å°/é›¶æ ·æœ¬å­¦ä¹ ï¼ŒåŸŸé€‚åº”ï¼ŒåŸŸæ³›åŒ–)

* å°æ ·æœ¬å­¦ä¹ 
  * [Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning](https://arxiv.org/abs/2103.01315)<br>
  * [Learning Dynamic Alignment via Meta-filter for Few-shot Learning](https://arxiv.org/abs/2103.13582)<br>[ä½œè€…ä¸»é¡µ](https://yanweifu.github.io/page3.html)<br>é€šè¿‡å…ƒå·ç§¯æ ¸å®ç°åŸºäºåŠ¨æ€å¯¹é½çš„å°æ ·æœ¬å­¦ä¹ <br>è§£è¯»ï¼š[17](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [ECKPN: Explicit Class Knowledge Propagation Network for Transductive Few-shot Learning](https://arxiv.org/abs/2106.08523)
  * [Mutual CRF-GNN for Few-Shot Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_Mutual_CRF-GNN_for_Few-Shot_Learning_CVPR_2021_paper.pdf)
  * [Rethinking Class Relations: Absolute-Relative Supervised and Unsupervised Few-Shot Learning](https://arxiv.org/abs/2001.03919)
  * [Pareto Self-Supervised Training for Few-Shot Learning](https://arxiv.org/abs/2104.07841)
  * [Reinforced Attention for Few-Shot Learning and Beyond](https://arxiv.org/abs/2104.04192)
  * [Using Shape to Categorize: Low-Shot Learning with an Explicit Shape Bias](https://arxiv.org/abs/2101.07296)<br>:house:[project](https://rehg-lab.github.io/publication-pages/lowshot-shapebias/)
  * [Prototype Completion With Primitive Knowledge for Few-Shot Learning](https://arxiv.org/abs/2009.04960)<br>:star:[code](https://github.com/zhangbq-research/Prototype_Completion_for_FSL)
* åŸŸæ³›åŒ–
  * [FSDR: Frequency Space Domain Randomization for Domain Generalization](https://arxiv.org/abs/2103.02370)<br>å— JPEG å°†ç©ºé—´å›¾åƒè½¬æ¢ä¸ºå¤šä¸ªé¢‘ç‡åˆ†é‡(FCs)çš„å¯å‘ï¼Œæå‡ºé¢‘ç‡ç©ºé—´åŸŸéšæœºåŒ–(FSDR)ï¼Œé€šè¿‡ä¿ç•™åŸŸå˜é‡FCs(DIFs)å’ŒåªéšæœºåŒ–åŸŸå˜é‡FCs(DVFs)æ¥éšæœºåŒ–é¢‘ç‡ç©ºé—´çš„å›¾åƒã€‚
  * [Domain Generalization via Inference-time Label-Preserving Target Projections](https://arxiv.org/abs/2103.01134)
  * [Adaptive Methods for Real-World Domain Generalization](https://arxiv.org/abs/2103.15796)<br>:open_mouth: Oral
  * [Progressive Domain Expansion Network for Single Domain Generalization](https://arxiv.org/abs/2103.16050)<br>:star:[code](https://github.com/lileicv/PDEN)
  * [A Fourier-based Framework for Domain Generalization](https://arxiv.org/abs/2105.11120)<br>:open_mouth:oral:star:[code](https://github.com/MediaBrain-SJTU/FACT)
  * [Adversarially Adaptive Normalization for Single Domain Generalization](https://arxiv.org/abs/2106.01899)
  * [Generalization on Unseen Domains via Inference-Time Label-Preserving Target Projections](https://openaccess.thecvf.com/content/CVPR2021/papers/Pandey_Generalization_on_Unseen_Domains_via_Inference-Time_Label-Preserving_Target_Projections_CVPR_2021_paper.pdf)
  * [Uncertainty-Guided Model Generalization to Unseen Domains](https://arxiv.org/abs/2103.07531)<br>:star:[code](https://github.com/joffery/UMGUD)
  * [Open Domain Generalization with Domain-Augmented Meta-Learning](https://arxiv.org/abs/2104.03620)
* é›¶æ ·æœ¬å­¦ä¹ 
  * [Goal-Oriented Gaze Estimation for Zero-Shot Learning](https://arxiv.org/abs/2103.03433)<br>:star:[code](https://github.com/osierboy/GEM-ZSL)
  * [Contrastive Embedding for Generalized Zero-Shot Learning](https://arxiv.org/abs/2103.16173)<br>:star:[code](https://github.com/Hanzy1996/CE-GZSL)
  * [Open World Compositional Zero-Shot Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_CutPaste_Self-Supervised_Learning_for_Anomaly_Detection_and_Localization_CVPR_2021_paper.pdf)
  * [Learning Graph Embeddings for Compositional Zero-Shot Learning](https://arxiv.org/abs/2102.01987)<br>:star:[code](https://github.com/ExplainableML/czsl)
  * [Hardness Sampling for Self-Training Based Transductive Zero-Shot Learning](https://arxiv.org/abs/2106.00264)<br>:star:[code](https://github.com/flywithcloud/STHS)
* åŸŸé€‚åº”
  * [Dynamic Transfer for Multi-Source Domain Adaptation](https://arxiv.org/abs/2103.10583)<br>:star:[code](https://github.com/liyunsheng13/DRT)  
  * [Transferable Semantic Augmentation for Domain Adaptation](https://arxiv.org/abs/2103.12562)<br>:star:[code](https://github.com/BIT-DA/TSA)
  * [MetaAlign: Coordinating Domain Alignment and Classification for Unsupervised Domain Adaptation](https://arxiv.org/abs/2103.13575)
  * [DRANet: Disentangling Representation and Adaptation Networks for Unsupervised Cross-Domain Adaptation](https://arxiv.org/abs/2103.13447)
  * [Dynamic Domain Adaptation for Efficient Inference](https://arxiv.org/abs/2103.16403)<br>:star:[code](https://github.com/BIT-DA/DDA)
  * [Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation](https://arxiv.org/abs/2103.16765)<br>:house:[project](http://xyue.io/pcs-fuda/)
  * [Domain Consensus Clustering for Universal Domain Adaptation](http://reler.net/papers/guangrui_cvpr2021.pdf)<br>:star:[code](https://github.com/Solacex/Domain-Consensus-Clustering)
  * [Divergence Optimization for Noisy Universal Domain Adaptation](https://arxiv.org/abs/2104.00246)
  * [Curriculum Graph Co-Teaching for Multi-Target Domain Adaptation](https://arxiv.org/abs/2104.00808)<br>:star:[code](https://github.com/Evgeneus/Graph-Domain-Adaptaion):house:[project](https://roysubhankar.github.io/graph-coteaching-adaptation/)
  * [Unsupervised Multi-source Domain Adaptation Without Access to Source Data](https://arxiv.org/abs/2104.01845)
  * [Domain Adaptation with Auxiliary Target Domain-Oriented Classifier](https://arxiv.org/pdf/2007.04171.pdf)<br>:star:[code](https://github.com/tim-learn/ATDOC)
  * [Cross-Domain Adaptive Clustering for Semi-Supervised Domain Adaptation](https://arxiv.org/abs/2104.09415)
  * [Generalized Domain Adaptation](https://arxiv.org/abs/2106.01656)<br>:star:[code](https://github.com/nttcslab/Generalized-Domain-Adaptation)
  * [Multi-Target Domain Adaptation with Collaborative Consistency Learning](https://arxiv.org/abs/2106.03418)<br>:star:[code](https://github.com/junpan19/MTDA)
  * [Wasserstein Barycenter for Multi-Source Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2021/papers/Montesuma_Wasserstein_Barycenter_for_Multi-Source_Domain_Adaptation_CVPR_2021_paper.pdf)
  * [Conditional Bures Metric for Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Conditional_Bures_Metric_for_Domain_Adaptation_CVPR_2021_paper.pdf)
  * [Partial Feature Selection and Alignment for Multi-Source Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2021/papers/Fu_Partial_Feature_Selection_and_Alignment_for_Multi-Source_Domain_Adaptation_CVPR_2021_paper.pdf)
  * [Transferable Query Selection for Active Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2021/papers/Fu_Transferable_Query_Selection_for_Active_Domain_Adaptation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/thuml/Transferable-Query-Selection)
  * [Learning Invariant Representations and Risks for Semi-Supervised Domain Adaptation](https://arxiv.org/abs/2010.04647)<br>:star:[code](https://github.com/Luodian/Learning-Invariant-Representations-and-Risks)
  * æ— ç›‘ç£åŸŸé€‚åº”
    * [Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation](https://arxiv.org/abs/2106.04151)<br>:star:[code](https://github.com/lijin118/CGDM)
    * [Instance Level Affinity-Based Transfer for Unsupervised Domain Adaptation](https://arxiv.org/abs/2104.01286)<br>:star:[code](https://github.com/astuti/ILA-DA)
    * [Learning to Relate Depth and Semantics for Unsupervised Domain Adaptation](https://arxiv.org/abs/2105.07830)<br>:star:[code](https://github.com/susaha/ctrl-uda)
    * [PixMatch: Unsupervised Domain Adaptation via Pixelwise Consistency Training](https://arxiv.org/abs/2105.08128) 
    * [FixBi: Bridging Domain Spaces for Unsupervised Domain Adaptation](https://arxiv.org/abs/2011.09230)<br>:star:[code](https://github.com/NaJaeMin92/FixBi)
    * [Dynamic Weighted Learning for Unsupervised Domain Adaptation](http://arxiv.org/abs/2103.13814)
 

<a name="13"/> 

## 13.Image/Video Retrieval(å›¾åƒ/è§†é¢‘æ£€ç´¢)
* [Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers](https://arxiv.org/abs/2103.16553)
* [Convolutional Hough Matching](https://arxiv.org/abs/2103.16831)<br>:open_mouth:oral:house:[project](http://cvlab.postech.ac.kr/research/CHM/)
* [T2VLAD: Global-Local Sequence Alignment for Text-Video Retrieval](https://arxiv.org/abs/2104.10054)
* [M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training](https://arxiv.org/abs/2006.02635)
* [VIGOR: Cross-View Image Geo-localization beyond One-to-one Retrieval](https://arxiv.org/abs/2011.12172)<br>:star:[code](https://github.com/Jeff-Zilence/VIGOR)
* å›¾åƒæ£€ç´¢
  * [Probabilistic Embeddings for Cross-Modal Retrieval](https://arxiv.org/abs/2101.05068)<br>:star:[code](https://github.com/naver-ai/pcme)
  * [QAIR: Practical Query-efficient Black-Box Attacks for Image Retrieval](https://arxiv.org/abs/2103.02927)
  * [More Photos are All You Need: Semi-Supervised Learning for Fine-Grained Sketch Based Image Retrieval](https://arxiv.org/abs/2103.13990)<br>:star:[code](https://github.com/AyanKumarBhunia/semisupervised-FGSBIR)
  * [StyleMeUp: Towards Style-Agnostic Sketch-Based Image Retrieval](https://arxiv.org/abs/2103.15706)
  * [Prototype-supervised Adversarial Network for Targeted Attack of Deep Hashing](https://arxiv.org/abs/2105.07553)<br>:star:[code](https://github.com/xunguangwang/ProS-GAN)
  * [CoSMo: Content-Style Modulation for Image Retrieval With Text Feedback](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_CoSMo_Content-Style_Modulation_for_Image_Retrieval_With_Text_Feedback_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/postBG/CosMo.pytorch)
  * [Efficient Object Embedding for Spliced Image Retrieval](https://arxiv.org/abs/1905.11903)
  * [You See What I Want You To See: Exploring Targeted Black-Box Transferability Attack for Hash-Based Image Retrieval Systems](https://openaccess.thecvf.com/content/CVPR2021/papers/Xiao_You_See_What_I_Want_You_To_See_Exploring_Targeted_CVPR_2021_paper.pdf)
* è§†é¢‘æ£€ç´¢
  * [On Semantic Similarity in Video Retrieval](https://arxiv.org/abs/2103.10095)<br>:star:[code](https://github.com/mwray/Semantic-Video-Retrieval):house:[project](https://mwray.github.io/SSVR/):tv:[video](https://youtu.be/pS9qa_B771I)
* è§†è§‰æœç´¢
  * [Compatibility-aware Heterogeneous Visual Search](https://arxiv.org/abs/2105.06047)
* è·¨æ¨¡æ€æ£€ç´¢
  * [Multi-Modal Relational Graph for Cross-Modal Video Moment Retrieval](https://openaccess.thecvf.com/content/CVPR2021/papers/Zeng_Multi-Modal_Relational_Graph_for_Cross-Modal_Video_Moment_Retrieval_CVPR_2021_paper.pdf)<br>:house:[project](https://cvpr-2021.wixsite.com/mmrg)
  * [Learning Cross-Modal Retrieval With Noisy Labels](https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Learning_Cross-Modal_Retrieval_With_Noisy_Labels_CVPR_2021_paper.pdf)
  * [Cross-Modal Center Loss for 3D Cross-Modal Retrieval](https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf)
* æ£€ç´¢(ä¸‰ç»´å½¢çŠ¶æ£€ç´¢å’Œå˜å½¢çš„è”åˆå­¦ä¹ )
  * [Joint Learning of 3D Shape Retrieval and Deformation](https://arxiv.org/abs/2101.07889)

<a name="12"/> 

## 12.Image Quality Assessment(å›¾åƒè´¨é‡è¯„ä¼°)

* å›¾åƒæ¢å¤Image Restoration
  * [Multi-Stage Progressive Image Restoration](https://arxiv.org/abs/2102.02808)<br>:star:[code](https://github.com/swz30/MPRNet)<br>
  * [See through Gradients: Image Batch Recovery via GradInversion](https://arxiv.org/abs/2104.07586)
  * [Controllable Image Restoration for Under-Display Camera in Smartphones](https://openaccess.thecvf.com/content/CVPR2021/papers/Kwon_Controllable_Image_Restoration_for_Under-Display_Camera_in_Smartphones_CVPR_2021_paper.pdf)
  * [Zero-Shot Single Image Restoration Through Controlled Perturbation of Koschmieder's Model](https://openaccess.thecvf.com/content/CVPR2021/papers/Kar_Zero-Shot_Single_Image_Restoration_Through_Controlled_Perturbation_of_Koschmieders_Model_CVPR_2021_paper.pdf)<br>:house:[project](https://aupendu.github.io/zero-restore)
  * [High-Quality Stereo Image Restoration From Double Refraction](https://openaccess.thecvf.com/content/CVPR2021/papers/Kim_High-Quality_Stereo_Image_Restoration_From_Double_Refraction_CVPR_2021_paper.pdf)
  * [Image Restoration for Under-Display Camera](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Image_Restoration_for_Under-Display_Camera_CVPR_2021_paper.pdf)
  * æ¼«ç”»ä¿®å¤
    * [Exploiting Aliasing for Manga Restoration](https://arxiv.org/abs/2105.06830)
* å»é˜´å½±Shadow Removal
  * [Auto-Exposure Fusion for Single-Image Shadow Removal](https://arxiv.org/abs/2103.01255)<br>:star:[code](https://github.com/tsingqguo/exposure-fusion-shadow-removal)<br>
  * [From Shadow Generation to Shadow Removal](https://arxiv.org/abs/2103.12997)<br>:star:[code](https://github.com/hhqweasd/G2R-ShadowNet)
  * [No Shadow Left Behind: Removing Objects and Their Shadows Using Approximate Lighting and Geometry](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_No_Shadow_Left_Behind_Removing_Objects_and_Their_Shadows_Using_CVPR_2021_paper.pdf)
* å»æ¨¡ç³ŠDeblurring
  * [DeFMO: Deblurring and Shape Recovery of Fast Moving Objects](https://arxiv.org/abs/2012.00595)<br>:star:[code](https://github.com/rozumden/DeFMO):tv:[video](https://www.youtube.com/watch?v=pmAynZvaaQ4)<br>
  * [ARVo: Learning All-Range Volumetric Correspondence for Video Deblurring](https://arxiv.org/abs/2103.04260)
  * [Explore Image Deblurring via Blur Kernel Space](https://arxiv.org/abs/2104.00317)
  * [Towards Rolling Shutter Correction and Deblurring in Dynamic Scenes](https://arxiv.org/abs/2104.01601)<br>:star:[code](https://github.com/zzh-tech/RSCD)
  * [Learning a Non-Blind Deblurring Network for Night Blurry Images](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Learning_a_Non-Blind_Deblurring_Network_for_Night_Blurry_Images_CVPR_2021_paper.pdf)
  * [Ultra-High-Definition Image Dehazing via Multi-Guided Bilateral Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Ultra-High-Definition_Image_Dehazing_via_Multi-Guided_Bilateral_Learning_CVPR_2021_paper.pdf) 
  * [Test-Time Fast Adaptation for Dynamic Scene Deblurring via Meta-Auxiliary Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Chi_Test-Time_Fast_Adaptation_for_Dynamic_Scene_Deblurring_via_Meta-Auxiliary_Learning_CVPR_2021_paper.pdf)
  * [Blind Deblurring for Saturated Images](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Blind_Deblurring_for_Saturated_Images_CVPR_2021_paper.pdf)
  * [Explore Image Deblurring via Encoded Blur Kernel Space](https://openaccess.thecvf.com/content/CVPR2021/papers/Tran_Explore_Image_Deblurring_via_Encoded_Blur_Kernel_Space_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/VinAIResearch/blur-kernel-space-exploring)
  * [Learning Spatially-Variant MAP Models for Non-Blind Image Deblurring](https://openaccess.thecvf.com/content/CVPR2021/papers/Dong_Learning_Spatially-Variant_MAP_Models_for_Non-Blind_Image_Deblurring_CVPR_2021_paper.pdf)
* å»åå°„Reflection Removal
  * [Robust Reflection Removal with Reflection-free Flash-only Cues](https://arxiv.org/abs/2103.04273)<br>:star:[code](https://github.com/ChenyangLEI/flash-reflection-removal)
  * [Single Image Reflection Removal With Absorption Effect](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Single_Image_Reflection_Removal_With_Absorption_Effect_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/q-zh/absorption)
  * [Panoramic Image Reflection Removal](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Panoramic_Image_Reflection_Removal_CVPR_2021_paper.pdf)
* å»é›¾
  * [Learning to Restore Hazy Video: A New Real-World Dataset and A New Method](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Restore_Hazy_Video_A_New_Real-World_Dataset_and_CVPR_2021_paper.pdf)<br>å­¦ä¹ å¤åŸæœ‰é›¾è§†é¢‘ï¼šä¸€ç§æ–°çš„çœŸå®æ•°æ®é›†åŠç®—æ³•<br>è§£è¯»ï¼š[9](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [Contrastive Learning for Compact Single Image Dehazing](https://arxiv.org/abs/2104.09367)<br>:star:[code](https://github.com/GlassyWu/AECR-Net)<br>åŸºäºå¯¹æ¯”å­¦ä¹ çš„ç´§å‡‘å›¾åƒå»é›¾æ–¹æ³•<br>è§£è¯»ï¼š[5](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [PSD: Principled Synthetic-to-Real Dehazing Guided by Physical Priors](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_PSD_Principled_Synthetic-to-Real_Dehazing_Guided_by_Physical_Priors_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/zychen-ustc/PSD-Principled-Synthetic-to-Real-Dehazing-Guided-by-Physical-Priors)
* å»å™ªDenoising
  * [Neighbor2Neighbor: Self-Supervised Denoising from Single Noisy Images](https://arxiv.org/abs/2101.02824)<br>:star:[code](https://github.com/TaoHuang2018/Neighbor2Neighbor)<br>è§£è¯»ï¼š[CVPR 2021 | Neighbor2Neighborï¼šä»…éœ€å™ªå£°å›¾åƒå³å¯è®­ç»ƒä»»æ„é™å™ªç½‘ç»œçš„æ–¹æ³•](https://mp.weixin.qq.com/s/Eg7vbjTILSd1Si3HSyz3CA)
  * [NBNet: Noise Basis Learning for Image Denoising with Subspace Projection](https://arxiv.org/abs/2012.15028)<br>:star:[code](https://github.com/megvii-research/NBNet)<br>ç²—è§£ï¼š[9](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
  * [Invertible Denoising Network: A Light Solution for Real Noise Removal](https://arxiv.org/abs/2104.10546)<br>:star:[code](https://github.com/Yang-Liu1082/InvDN)
  * [FBI-Denoiser: Fast Blind Image Denoiser for Poisson-Gaussian Noise](https://arxiv.org/abs/2105.10967)<br>:star:[code](https://github.com/csm9493/FBI-Denoiser)
  * [Recorrupted-to-Recorrupted: Unsupervised Deep Learning for Image Denoising](https://openaccess.thecvf.com/content/CVPR2021/papers/Pang_Recorrupted-to-Recorrupted_Unsupervised_Deep_Learning_for_Image_Denoising_CVPR_2021_paper.pdf)
  * [The Neural Tangent Link Between CNN Denoisers and Non-Local Filters](https://arxiv.org/abs/2006.02379)<br>:star:[code](https://gitlab.com/Tachella/neural_tangent_denoiser)
  * [Deep Denoising of Flash and No-Flash Pairs for Photography in Low-Light Environments](https://arxiv.org/abs/2012.05116)<br>:house:[project](https://www.cse.wustl.edu/~zhihao.xia/deepfnf/)
  * [Adaptive Consistency Prior Based Deep Network for Image Denoising](https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Adaptive_Consistency_Prior_Based_Deep_Network_for_Image_Denoising_CVPR_2021_paper.pdf)
  * [EventZoom: Learning To Denoise and Super Resolve Neuromorphic Events](https://openaccess.thecvf.com/content/CVPR2021/papers/Duan_EventZoom_Learning_To_Denoise_and_Super_Resolve_Neuromorphic_Events_CVPR_2021_paper.pdf)<br>:house:[project](https://sites.google.com/view/EventZoom):tv:[video](https://youtu.be/D7BH1DKVJcQ)
  * [Extreme Low-Light Environment-Driven Image Denoising Over Permanently Shadowed Lunar Regions With a Physical Noise Model](https://openaccess.thecvf.com/content/CVPR2021/papers/Moseley_Extreme_Low-Light_Environment-Driven_Image_Denoising_Over_Permanently_Shadowed_Lunar_Regions_CVPR_2021_paper.pdf)
  * [Guided Integrated Gradients: An Adaptive Path Method for Removing Noise](https://openaccess.thecvf.com/content/CVPR2021/papers/Kapishnikov_Guided_Integrated_Gradients_An_Adaptive_Path_Method_for_Removing_Noise_CVPR_2021_paper.pdf)
  * [Effective Snapshot Compressive-Spectral Imaging via Deep Denoising and Total Variation Priors](https://openaccess.thecvf.com/content/CVPR2021/papers/Qiu_Effective_Snapshot_Compressive-Spectral_Imaging_via_Deep_Denoising_and_Total_Variation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/ucker/SCI-TV-FFDNet)
  * [Deep Convolutional Dictionary Learning for Image Denoising](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Deep_Convolutional_Dictionary_Learning_for_Image_Denoising_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/natezhenghy/DCDicL_denoising)
  * [Learning An Explicit Weighting Scheme for Adapting Complex HSI Noise](https://openaccess.thecvf.com/content/CVPR2021/papers/Rui_Learning_an_Explicit_Weighting_Scheme_for_Adapting_Complex_HSI_Noise_CVPR_2021_paper.pdf)
  * [Pseudo 3D Auto-Correlation Network for Real Image Denoising](https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Pseudo_3D_Auto-Correlation_Network_for_Real_Image_Denoising_CVPR_2021_paper.pdf)
* å»é›¨Deraining
  * [Semi-Supervised Video Deraining with Dynamic Rain Generator](https://arxiv.org/abs/2103.07939)
  * [Closing the Loop: Joint Rain Generation and Removal via Disentangled Image Translation](https://arxiv.org/abs/2103.13660)
  * [Robust Representation Learning With Feedback for Single Image Deraining](https://arxiv.org/abs/2101.12463)<br>:star:[code](https://github.com/LI-Hao-SJTU/DerainRLNet)
  * [Multi-Decoding Deraining Network and Quasi-Sparsity Based Training](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Multi-Decoding_Deraining_Network_and_Quasi-Sparsity_Based_Training_CVPR_2021_paper.pdf)
  * [Image De-Raining via Continual Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Image_De-Raining_via_Continual_Learning_CVPR_2021_paper.pdf)
  * [From Rain Generation to Rain Removal](https://arxiv.org/abs/2008.03580)<br>:star:[code](https://github.com/hongwang01/VRGNet)
  * [Memory Oriented Transfer Learning for Semi-Supervised Image Deraining](https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Memory_Oriented_Transfer_Learning_for_Semi-Supervised_Image_Deraining_CVPR_2021_paper.pdf)
  * [Removing Raindrops and Rain Streaks in One Go](https://openaccess.thecvf.com/content/CVPR2021/papers/Quan_Removing_Raindrops_and_Rain_Streaks_in_One_Go_CVPR_2021_paper.pdf)
  * æ§åˆ¶é›¨é‡
    * [Controlling the Rain: From Removal to Rendering](https://openaccess.thecvf.com/content/CVPR2021/papers/Ni_Controlling_the_Rain_From_Removal_to_Rendering_CVPR_2021_paper.pdf)
* æ›å…‰æ ¡æ­£
  * [Learning Multi-Scale Photo Exposure Correction](https://arxiv.org/abs/2003.11596)<br>:star:[code](https://github.com/mahmoudnafifi/Exposure_Correction)
* å›¾åƒä¿®å¤Image Inpainting
  * [Generating Diverse Structure for Image Inpainting With Hierarchical VQ-VAE](https://arxiv.org/abs/2103.10022)<br>:star:[code](https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting)
  * [TransFill: Reference-guided Image Inpainting by Merging Multiple Color and Spatial Transformations](https://arxiv.org/abs/2103.15982)<br>:house:[project](https://yzhouas.github.io/projects/TransFill/index.html)
  * [Image Inpainting with External-internal Learning and Monochromic Bottleneck](https://arxiv.org/abs/2104.09068)<br>:star:[code](https://github.com/Tengfei-Wang/external-internal-inpainting)
  * [PD-GAN: Probabilistic Diverse GAN for Image Inpainting](https://arxiv.org/abs/2105.02201)<br>:star:[code](https://github.com/KumapowerLIU/PD-GAN)
  * [Image Inpainting Guided by Coherence Priors of Semantics and Textures](https://arxiv.org/abs/2012.08054)
* å›¾åƒç¼–è¾‘
  * [DeFLOCNet: Deep Image Editing via Flexible Low-level Controls](https://arxiv.org/abs/2103.12723)<br>:star:[code](https://github.com/KumapowerLIU/DeFLOCNet)
* å›¾åƒå‹ç¼©
  * [Attention-guided Image Compression by Deep Reconstruction of Compressive Sensed Saliency Skeleton](https://arxiv.org/abs/2103.15368)
  * [Slimmable Compressive Autoencoders for Practical Neural Image Compression](https://arxiv.org/abs/2103.15726)<br>:star:[code](https://github.com/FireFYF/SlimCAE)
  * [Checkerboard Context Model for Efficient Learned Image Compression](https://arxiv.org/abs/2103.15306)
  * [Learning Scalable â„“âˆ-constrained Near-lossless Image Compression via Joint Lossy Image and Residual Compression](https://arxiv.org/abs/2103.17015)<br>:star:[code](https://github.com/BYchao100/Scalable-Near-lossless-Image-Compression)
  * [Deep Homography for Efficient Stereo Image Compression](http://buaamc2.net/pdf/cvpr21hesic.pdf)<br>:star:[code](https://github.com/ywz978020607/HESIC)<br>åˆ†äº«ä¼š
  * [iVPF: Numerical Invertible Volume Preserving Flow for Efficient Lossless Compression](https://arxiv.org/abs/2103.16211)
  * [What's in the Image? Explorable Decoding of Compressed Images](https://openaccess.thecvf.com/content/CVPR2021/papers/Bahat_Whats_in_the_Image_Explorable_Decoding_of_Compressed_Images_CVPR_2021_paper.pdf)
  * [Learning Scalable lY=-Constrained Near-Lossless Image Compression via Joint Lossy Image and Residual Compression](https://openaccess.thecvf.com/content/CVPR2021/papers/Bai_Learning_Scalable_lY-Constrained_Near-Lossless_Image_Compression_via_Joint_Lossy_Image_CVPR_2021_paper.pdf)
  * [Asymmetric Gained Deep Image Compression With Continuous Rate Adaptation](https://openaccess.thecvf.com/content/CVPR2021/papers/Cui_Asymmetric_Gained_Deep_Image_Compression_With_Continuous_Rate_Adaptation_CVPR_2021_paper.pdf)
* de-rendering
  * [De-rendering the World's Revolutionary Artefacts](https://arxiv.org/abs/2104.03954)<br>:house:[project](https://sorderender.github.io/):tv:[video](https://www.youtube.com/watch?v=pxkYyyw02H0)
  * [How To Exploit the Transferability of Learned Image Compression to Conventional Codecs](https://arxiv.org/abs/2012.01874)
* æ¶ˆé™¤å›¾åƒä¼ªå½±
  * [Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network](https://arxiv.org/abs/2104.09556)<br>:star:[code](https://github.com/jnjaby/DISCNet):house:[project](https://jnjaby.github.io/projects/UDC/) 
* å›¾åƒå¯¹é½
  * [Deep Lucas-Kanade Homography for Multimodal Image Alignment](https://arxiv.org/abs/2104.11693)<br>:star:[code](https://github.com/placeforyiming/CVPR21-Deep-Lucas-Kanade-Homography)
* å›¾åƒå’Œè°åŒ–
  * [Region-aware Adaptive Instance Normalization for Image Harmonization](https://arxiv.org/abs/2106.02853)<br>:star:[code](https://github.com/junleen/RainNet)
  * [Intrinsic Image Harmonization](https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_Intrinsic_Image_Harmonization_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/zhenglab/IntrinsicHarmony)
* å›¾åƒå¢å¼º
  * [CAMERAS: Enhanced Resolution and Sanity Preserving Class Activation Mapping for Image Saliency](https://openaccess.thecvf.com/content/CVPR2021/papers/Jalwana_CAMERAS_Enhanced_Resolution_and_Sanity_Preserving_Class_Activation_Mapping_for_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/VisMIL/CAMERAS)
  * [Retinex-Inspired Unrolling With Cooperative Prior Architecture Search for Low-Light Image Enhancement](https://arxiv.org/abs/2012.05609)<br>:star:[code](https://github.com/dut-media-lab/RUAS):house:[project](http://dutmedia.org/RUAS/) 
  * [Debiased Subjective Assessment of Real-World Image Enhancement](https://openaccess.thecvf.com/content/CVPR2021/papers/Cao_Debiased_Subjective_Assessment_of_Real-World_Image_Enhancement_CVPR_2021_paper.pdf)
  * [Learning Temporal Consistency for Low Light Video Enhancement From Single Images](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_Temporal_Consistency_for_Low_Light_Video_Enhancement_From_Single_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/zkawfanx/StableLLVE)
* Image Stabilizationé˜²æŠ–
  * [Digital Gimbal: End-to-end Deep Image Stabilization with Learnable Exposure Times](http://arxiv.org/abs/2012.04515)
* å»æ•£ç„¦æ¨¡ç³Š
  * [Iterative Filter Adaptive Network for Single Image Defocus Deblurring](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Iterative_Filter_Adaptive_Network_for_Single_Image_Defocus_Deblurring_CVPR_2021_paper.pdf)
* å»é®æŒ¡
  * [Human De-Occlusion: Invisible Perception and Recovery for Humans](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Human_De-Occlusion_Invisible_Perception_and_Recovery_for_Humans_CVPR_2021_paper.pdf)<br>:house:[project](https://sydney0zq.github.io/ahp/)
* å¢å¼ºå¤œé—´å¯è§†åº¦
  * [Nighttime Visibility Enhancement by Increasing the Dynamic Range and Suppression of Light Effects](https://openaccess.thecvf.com/content/CVPR2021/papers/Sharma_Nighttime_Visibility_Enhancement_by_Increasing_the_Dynamic_Range_and_Suppression_CVPR_2021_paper.pdf)
* å›¾åƒè¡¥å…¨
  * [Prior Based Human Completion](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_Prior_Based_Human_Completion_CVPR_2021_paper.pdf)
* image steganography(å›¾ç‰‡éšå†™æœ¯)
  * [Large-Capacity Image Steganography Based on Invertible Neural Networks](https://openaccess.thecvf.com/content/CVPR2021/papers/Lu_Large-Capacity_Image_Steganography_Based_on_Invertible_Neural_Networks_CVPR_2021_paper.pdf)
* Image Blending
  * [Bridging the Visual Gap: Wide-Range Image Blending](https://arxiv.org/abs/2103.15149)<br>:star:[code](https://github.com/julia0607/Wide-Range-Image-Blending) 
* å›¾åƒçŸ«æ­£
  * [Progressively Complementary Network for Fisheye Image Rectification Using Appearance Flow](https://arxiv.org/abs/2103.16026)<br>:star:[code](https://github.com/uof1745-cmd/PCN)
* Defocus Blur Detection(æ£€æµ‹ç”±æ•£ç„¦å¯¼è‡´çš„æ¨¡ç³ŠåŒºåŸŸ)
  * [Self-Generated Defocus Blur Detection via Dual Adversarial Discriminators](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_Self-Generated_Defocus_Blur_Detection_via_Dual_Adversarial_Discriminators_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/shangcai1/SG)
* åœºæ™¯æ¢å¤ï¼ˆä¸åŒå¤©æ°”ã€æˆåƒï¼‰
  * [Rank-One Prior: Toward Real-Time Scene Recovery](https://arxiv.org/abs/2103.17126)
  * [ZeroScatter: Domain Transfer for Long Distance Imaging and Vision Through Scattering Media](https://arxiv.org/abs/2102.05847)<br>:house:[project](https://light.princeton.edu/publication/zeroscatter/):tv:[video](https://youtu.be/sE3M0pYvhZE)
* Image cropping(å›¾ç‰‡è£å‰ª)
  * [Composing Photos Like a Photographer](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Composing_Photos_Like_a_Photographer_CVPR_2021_paper.pdf)
* Image Stitching(å›¾åƒæ‹¼æ¥)
  * [Leveraging Line-Point Consistence To Preserve Structures for Wide Parallax Image Stitching](https://openaccess.thecvf.com/content/CVPR2021/papers/Jia_Leveraging_Line-Point_Consistence_To_Preserve_Structures_for_Wide_Parallax_Image_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/dut-media-lab/Image-Stitching)  
* æ·±åº¦ä¼°è®¡+å›¾åƒä¿®å¤
  * [Dual Pixel Exploration: Simultaneous Depth Estimation and Image Restoration](https://arxiv.org/abs/2012.00301)
* Image extrapolation
  * [OCONet: Image Extrapolation by Object Completion](https://openaccess.thecvf.com/content/CVPR2021/papers/Bowen_OCONet_Image_Extrapolation_by_Object_Completion_CVPR_2021_paper.pdf)
* å›¾åƒç¼–è¾‘
  * [Learning by Planning: Language-Guided Global Image Editing](https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Learning_by_Planning_Language-Guided_Global_Image_Editing_CVPR_2021_paper.pdf)
* å›¾åƒè´¨é‡
  * [Quality-Agnostic Image Recognition via Invertible Decoder](https://openaccess.thecvf.com/content/CVPR2021/papers/Kim_Quality-Agnostic_Image_Recognition_via_Invertible_Decoder_CVPR_2021_paper.pdf)
  * [Troubleshooting Blind Image Quality Models in the Wild](https://arxiv.org/abs/2105.06747)<br>:star:[code](https://github.com/wangzhihua520/troubleshooting_BIQA)
* HDR Deghosting(HDRå»ä¼ªå½±)
  * [Labeled From Unlabeled: Exploiting Unlabeled Data for Few-Shot Deep HDR Deghosting](https://openaccess.thecvf.com/content/CVPR2021/papers/Prabhakar_Labeled_From_Unlabeled_Exploiting_Unlabeled_Data_for_Few-Shot_Deep_HDR_CVPR_2021_paper.pdf)
* å›¾åƒå¢äº®
  * [Restoring Extremely Dark Images in Real Time](https://openaccess.thecvf.com/content/CVPR2021/papers/Lamba_Restoring_Extremely_Dark_Images_in_Real_Time_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/MohitLamba94/Restoring-Extremely-Dark-Images-In-Real-Time) 
* å›¾åƒé™è´¨
  * [DeFlow: Learning Complex Image Degradations from Unpaired Data with Conditional Flows](https://arxiv.org/abs/2101.05796)<br>:open_mouth:oral:star:[code](https://github.com/volflow/DeFlow)
* Specular highlight æ£€æµ‹ä¸å»é™¤
  * [A Multi-Task Network for Joint Specular Highlight Detection and Removal](https://openaccess.thecvf.com/content/CVPR2021/papers/Fu_A_Multi-Task_Network_for_Joint_Specular_Highlight_Detection_and_Removal_CVPR_2021_paper.pdf) 

<a name="11"/> 

## 11. Face(äººè„¸æŠ€æœ¯)

* [Towards High Fidelity Face Relighting with Realistic Shadows](https://arxiv.org/abs/2104.00825)<br>:star:[code](https://github.com/andrewhou1/Shadow-Mask-Face-Relighting)
* [IronMask: Modular Architecture for Protecting Deep Face Template](https://arxiv.org/abs/2104.02239)
* [Everything's Talkin': Pareidolia Face Reenactment](https://arxiv.org/abs/2104.03061)<br>:star:[code](https://github.com/Linsen13/EverythingTalking):house:[project](https://wywu.github.io/projects/ETT/ETT.html):tv:[video](https://youtu.be/lVYZ3IAVM_U)
* äººè„¸è¯†åˆ«
  * [A 3D GAN for Improved Large-pose Facial Recognition](https://arxiv.org/pdf/2012.10545.pdf)<br>
  * [When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework](https://arxiv.org/abs/2103.01520)<br>:open_mouth:oral:star:[code](https://github.com/Hzzone/MTLFace)<br>
  * [MagFace: A Universal Representation for Face Recognition and Quality Assessment](https://arxiv.org/abs/2103.06627)<br>:open_mouth:oral:star:[code](https://github.com/IrvingMeng/MagFace)<br>äººè„¸è¯†åˆ«+è´¨é‡ï¼Œä»Šå¹´çš„Oral presentationã€‚ ä»£ç å¾…æ•´ç†
  * [WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition](https://arxiv.org/abs/2103.04098)<br>:house:[project](https://www.face-benchmark.org/)
  * [ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis](https://arxiv.org/abs/2103.05630)<br>:open_mouth:oral:house:[project](https://yinanhe.github.io/projects/forgerynet.html):tv:[video](https://youtu.be/e8XIL3Di2Y8) 
  * [Spherical Confidence Learning for Face Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Spherical_Confidence_Learning_for_Face_Recognition_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/MathsShen/SCF/)<br>:open_mouth:oral<br>åŸºäºè¶…çƒæµå½¢ç½®ä¿¡åº¦å­¦ä¹ çš„äººè„¸è¯†åˆ«
  * [CRFace: Confidence Ranker for Model-Agnostic Face Detection Refinement](https://arxiv.org/abs/2103.07017)
  * [Cross-Domain Similarity Learning for Face Recognition in Unseen Domains](https://arxiv.org/abs/2103.07503)
  * [HLA-Face: Joint High-Low Adaptation for Low Light Face Detection](https://arxiv.org/abs/2104.01984)<br>:house:[project](https://daooshee.github.io/HLA-Face-Website/)
  * [FACESEC: A Fine-grained Robustness Evaluation Framework for Face Recognition Systems](https://arxiv.org/abs/2104.04107)
  * [Dynamic Class Queue for Large Scale Face Recognition In the Wild](https://arxiv.org/abs/2105.11113)<br>:star:[code](https://github.com/bilylee/DCQ)
  * [Consistent Instance False Positive Improves Fairness in Face Recognition](https://arxiv.org/abs/2106.05519)<br>:star:[code](https://github.com/Tencent/TFace)<br>åŸºäºå®ä¾‹è¯¯æŠ¥ä¸€è‡´æ€§çš„äººè„¸è¯†åˆ«å…¬å¹³æ€§æå‡æ–¹æ³•<br>è§£è¯»ï¼š[7](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [VirFace: Enhancing Face Recognition via Unlabeled Shallow Data](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_VirFace_Enhancing_Face_Recognition_via_Unlabeled_Shallow_Data_CVPR_2021_paper.pdf)
  * [Variational Prototype Learning for Deep Face Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_Variational_Prototype_Learning_for_Deep_Face_Recognition_CVPR_2021_paper.pdf)
  * [Mitigating Face Recognition Bias via Group Adaptive Classifier](https://arxiv.org/abs/2006.07576)<br>:star:[code](https://github.com/gongsixue/GAC)
  * [Pseudo Facial Generation With Extreme Poses for Face Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Pseudo_Facial_Generation_With_Extreme_Poses_for_Face_Recognition_CVPR_2021_paper.pdf)
  * [Improving Transferability of Adversarial Patches on Face Recognition With Generative Models](https://openaccess.thecvf.com/content/CVPR2021/papers/Xiao_Improving_Transferability_of_Adversarial_Patches_on_Face_Recognition_With_Generative_CVPR_2021_paper.pdf)
  * [Clusformer: A Transformer Based Clustering Approach to Unsupervised Large-Scale Face and Visual Landmark Recognition](https://openaccess.thecvf.com/content/CVPR2021/papers/Nguyen_Clusformer_A_Transformer_Based_Clustering_Approach_to_Unsupervised_Large-Scale_Face_CVPR_2021_paper.pdf)
* åˆæˆäººè„¸ï¼ˆDeepfake/Face Forgeryï¼‰æ£€æµ‹
  * [Multi-attentional Deepfake Detection](https://arxiv.org/abs/2103.02406)<br>:star:[code](https://github.com/yoctta/multiple-attention)
  * [Frequency-aware Discriminative Feature Learning Supervised by Single-Center Loss for Face Forgery Detection](https://arxiv.org/abs/2103.09096)
  * [MagDR: Mask-guided Detection and Reconstruction for Defending Deepfakes](https://arxiv.org/abs/2103.14211)
  * [Face Forensics in the Wild](https://arxiv.org/abs/2103.16076)<br>:open_mouth:oral:star:[code](https://github.com/tfzhou/FFIW)
  * [Improving the Efficiency and Robustness of Deepfakes Detection through Precise Geometric Features](https://arxiv.org/abs/2104.04480)<br>:star:[code](https://github.com/frederickszk/LRNet)
  * [Lips Don't Lie: A Generalisable and Robust Approach To Face Forgery Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Haliassos_Lips_Dont_Lie_A_Generalisable_and_Robust_Approach_To_Face_CVPR_2021_paper.pdf)
  * [Representative Forgery Mining for Fake Face Detection](https://arxiv.org/abs/2104.06609)<br>:star:[code](https://github.com/crywang/RFM)
  * [Exploring Adversarial Fake Images on Face Manifold](https://arxiv.org/abs/2101.03272)
  * [Spatial-Phase Shallow Learning: Rethinking Face Forgery Detection in Frequency Domain](https://arxiv.org/abs/2103.01856)
  * [Frequency-Aware Discriminative Feature Learning Supervised by Single-Center Loss for Face Forgery Detection](https://arxiv.org/abs/2103.09096)
  * [Generalizing Face Forgery Detection With High-Frequency Features](https://arxiv.org/abs/2103.12376)
  * [Face Forgery Detection by 3D Decomposition](https://arxiv.org/abs/2011.09737)
* äººè„¸è´¨é‡è¯„ä¼°
  * [SDD-FIQA: Unsupervised Face Image Quality Assessment with Similarity Distribution Distance](https://arxiv.org/abs/2103.05977)<br>:star:[code](https://github.com/Slinene/SDD-FIQA)<br>åŸºäºç›¸ä¼¼åº¦åˆ†å¸ƒè·ç¦»çš„æ— ç›‘ç£äººè„¸è´¨é‡è¯„ä¼°<br>è§£è¯»ï¼š[6](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* 3Däººè„¸é‡å»º
  * [3DCaricShop: A Dataset and A Baseline Method for Single-view 3D Caricature Face Reconstruction](https://arxiv.org/abs/2103.08204)<br>:star:[code](https://github.com/qiuyuda/3DCaricShop):house:[project](https://qiuyuda.github.io/3DCaricShop/)
  * [Riggable 3D Face Reconstruction via In-Network Optimization](https://arxiv.org/abs/2104.03493)<br>:star:[code](https://github.com/zqbai-jeremy/INORig)<br>æœ¬æ–‡é€šè¿‡ä¸€ä¸ªåµŒå…¥äº†ç½‘ç»œå†…ä¼˜åŒ–çš„ç«¯åˆ°ç«¯å¯è®­ç»ƒç½‘ç»œï¼Œè§£å†³äº†ä»å•ç›® RGB å›¾åƒä¸­ riggable 3D äººè„¸é‡å»ºã€‚å¹¶ä¸”è¾¾åˆ°äº†æœ€å…ˆè¿›çš„é‡å»ºç²¾åº¦ï¼Œåˆç†çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åº”ç”¨äºæ ‡å‡†çš„ face rig åº”ç”¨ï¼Œå¦‚é‡å®šä½ã€‚
  * [Pixel Codec Avatars](https://arxiv.org/abs/2104.04638)<br>:open_mouth:oral
  * [Inverting Generative Adversarial Renderer for Face Reconstruction](https://arxiv.org/pdf/2105.02431.pdf)<br>:star:[code](https://github.com/WestlyPark/StyleRenderer)<br>è§£è¯»ï¼š[å•†æ±¤ã€æ¸¯ä¸­æ–‡å®ç°å•ç›®äººè„¸é‡å»ºæ–°çªç ´ï¼š åŸºäºç”Ÿæˆç½‘ç»œçš„æ¸²æŸ“å™¨ï¼å‡ ä½•å½¢çŠ¶æ›´ç²¾å‡†ï¼æ¸²æŸ“æ•ˆæœæ›´çœŸå®ï¼](https://mp.weixin.qq.com/s/H2zdQGVBFY4N0x4MmLf55g)
  * [Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection](https://arxiv.org/abs/2106.07852)<br>åœ¨å¼€æ”¾çš„äººåƒé›†åˆä¸­å­¦ä¹ 3Däººè„¸çš„èšåˆä¸ç‰¹å¼‚åŒ–é‡å»º<br>:open_mouth:oral:star:[code](https://github.com/TencentYoutuResearch/3DFaceReconstruction-LAP)
  * [Monocular Reconstruction of Neural Face Reflectance Fields](https://arxiv.org/abs/2008.10247)<br>:house:[project](http://gvv.mpi-inf.mpg.de/projects/FaceReflectanceFields/)
  * [Learning Complete 3D Morphable Face Models From Images and Videos](https://arxiv.org/abs/2010.01679)<br>:house:[project](https://gvv.mpi-inf.mpg.de/projects/LeMoMo/)
* äººè„¸è¡¨æƒ…è¯†åˆ«
  * [Affective Processes: stochastic modelling of temporal context for emotion and facial expression recognition](https://arxiv.org/abs/2103.13372)<br>
  * [Dive into Ambiguity: Latent Distribution Mining and Pairwise Uncertainty Estimation for Facial Expression Recognition](https://arxiv.org/abs/2104.00232)
  * [Feature Decomposition and Reconstruction Learning for Effective Facial Expression Recognition](https://arxiv.org/abs/2104.05160)
  * [Learning a Facial Expression Embedding Disentangled from Identity](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_a_Facial_Expression_Embedding_Disentangled_From_Identity_CVPR_2021_paper.pdf)
* äººè„¸èšç±» 
  * [Structure-Aware Face Clustering on a Large-Scale Graph with 10^7 Nodes](https://arxiv.org/abs/2103.13225)<br>:star:[code](https://github.com/sstzal/STAR-FC):house:[project](https://sstzal.github.io/STAR-FC/)
* äººè„¸ç¼–è¾‘
  * [High-Fidelity and Arbitrary Face Editing](https://arxiv.org/abs/2103.15814)
* äººè„¸è·Ÿè¸ª
  * [High-fidelity Face Tracking for AR/VR via Deep Lighting Adaptation](https://arxiv.org/abs/2103.15876)<br>:house:[project](https://www.cs.rochester.edu/u/lchen63/):tv:[video](https://www.youtube.com/watch?v=dtz1LgZR8cc)
* å¹¿è§’äººè„¸çŸ«æ­£
  * [Practical Wide-Angle Portraits Correction with Deep Structured Models](https://arxiv.org/abs/2104.12464)<br>:star:[code](https://github.com/TanJing94/Deep_Portraits_Correction)<br>ç²—è§£ï¼š[7](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* äººè„¸æ´»ä½“æ£€æµ‹
  * [Cross Modal Focal Loss for RGBD Face Anti-Spoofing](https://arxiv.org/abs/2103.00948)<br>:star:[code](https://gitlab.idiap.ch/bob/bob.paper.cross_modal_focal_loss_cvpr2021)
* éŸ³é¢‘é©±åŠ¨åˆæˆèµ‹æœ‰æƒ…æ„Ÿçš„äººè„¸
  * [Audio-Driven Emotional Video Portraits](https://arxiv.org/abs/2104.07452)<br>:star:[code](https://github.com/jixinya/EVP/):house:[project](https://jixinya.github.io/projects/evp/)
* æ¢è„¸
  * [Information Bottleneck Disentanglement for Identity Swapping](https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_Information_Bottleneck_Disentanglement_for_Identity_Swapping_CVPR_2021_paper.pdf)<br>åˆ†äº«ä¼š
  * [One Shot Face Swapping on Megapixels](https://arxiv.org/abs/2105.04932)<br>:sunflower:[dataset](https://github.com/zyainfal/One-Shot-Face-Swapping-on-Megapixels)
* äººè„¸ä¿®å¤
  * [FaceInpainter: High Fidelity Face Adaptation to Heterogeneous Domains](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_FaceInpainter_High_Fidelity_Face_Adaptation_to_Heterogeneous_Domains_CVPR_2021_paper.pdf)<br>åˆ†äº«ä¼š
  * [Progressive Semantic-Aware Style Transformation for Blind Face Restoration](https://arxiv.org/abs/2009.08709)<br>:star:[code](https://github.com/chaofengc/PSFRGAN)
  * [GAN Prior Embedded Network for Blind Face Restoration in the Wild](https://arxiv.org/abs/2105.06070)<br>:star:[code](https://github.com/yangxy/GPEN)
  * [Towards Real-World Blind Face Restoration With Generative Facial Prior](https://arxiv.org/abs/2101.04061)<br>:star:[code](https://github.com/TencentARC/GFPGAN)
* äººè„¸åŠ¨ç”»
  * [Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation](https://arxiv.org/abs/2104.11116)<br>:star:[code](https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS):house:[project](https://hangz-nju-cuhk.github.io/projects/PC-AVS):tv:[video](https://www.youtube.com/watch?v=lNQQHIggnUg)<br>è§£è¯»ï¼š[ â€œä»¥éŸ³åŠ¨äººâ€ï¼šå§¿æ€å¯æ§çš„è¯­éŸ³é©±åŠ¨è¯´è¯äººè„¸](https://mp.weixin.qq.com/s/uYC3HgBiiH_X_vemuyk2Gg)
* 3D Talking Faces
  * [LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization](https://arxiv.org/abs/2106.04185)<br>:tv:[video](https://www.youtube.com/watch?v=L1StbX9OznY)
* äººè„¸è®¤è¯
  * [Privacy-Preserving Image Features via Adversarial Affine Subspace Embeddings](https://arxiv.org/abs/2006.06634)
* äººè„¸çº¹ç†è¡¥å…¨
  * [OSTeC: One-Shot Texture Completion](https://arxiv.org/abs/2012.15370)<br>:star:[code](https://github.com/barisgecer/OSTeC)
* äººè„¸å¯¹é½
  * [img2pose: Face Alignment and Detection via 6DoF, Face Pose Estimation](https://arxiv.org/abs/2012.07791)<br>:star:[code](https://github.com/vitoralbiero/img2pose) 
* äººè„¸è€é¾„åŒ–
  * [Continuous Face Aging via Self-Estimated Residual Age Embedding](https://arxiv.org/abs/2105.00020)
* Facial Action Unit Detection(é¢éƒ¨è¿åŠ¨å•å…ƒæ£€æµ‹)
  * [Hybrid Message Passing With Performance-Driven Structures for Facial Action Unit Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Hybrid_Message_Passing_With_Performance-Driven_Structures_for_Facial_Action_Unit_CVPR_2021_paper.pdf) 
  * [Exploiting Semantic Embedding and Visual Feature for Facial Action Unit Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Exploiting_Semantic_Embedding_and_Visual_Feature_for_Facial_Action_Unit_CVPR_2021_paper.pdf)
  * [Dynamic Probabilistic Graph Convolution for Facial Action Unit Intensity Estimation](https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Dynamic_Probabilistic_Graph_Convolution_for_Facial_Action_Unit_Intensity_Estimation_CVPR_2021_paper.pdf)
* äººè„¸é‡å»º
  * [Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction](https://arxiv.org/abs/2012.03065)<br>:open_mouth:oral:star:[code](https://github.com/gafniguy/4D-Facial-Avatars):house:[project](https://gafniguy.github.io/4D-Facial-Avatars/):tv:[video](https://www.youtube.com/watch?v=m7oROLdQnjk)
* äººè„¸å±æ€§è¯†åˆ«
  * [Learning Spatial-Semantic Relationship for Facial Attribute Recognition With Limited Labeled Data](https://openaccess.thecvf.com/content/CVPR2021/papers/Shu_Learning_Spatial-Semantic_Relationship_for_Facial_Attribute_Recognition_With_Limited_Labeled_CVPR_2021_paper.pdf)
* äººè„¸æ¨¡ç³ŠåŒ–
  * [Perceptual Indistinguishability-Net (PI-Net): Facial Image Obfuscation with Manipulable Semantics](https://arxiv.org/abs/2104.01753)
* äººè„¸ç”Ÿæˆ
  * [Flow-Guided One-Shot Talking Face Generation With a High-Resolution Audio-Visual Dataset](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Flow-Guided_One-Shot_Talking_Face_Generation_With_a_High-Resolution_Audio-Visual_Dataset_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/MRzzm/HDTF)
  
<a name="10"/> 

## 10.Neural Architecture Search(ç¥ç»æ¶æ„æœç´¢)

- [AttentiveNAS: Improving Neural Architecture Search via Attentive](https://arxiv.org/pdf/2011.09011.pdf)<br>
- [HourNAS: Extremely Fast Neural Architecture Search Through an Hourglass Lens](https://arxiv.org/pdf/2005.14446.pdf)<br>:star:[code](https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/HourNAS)
- [ReNAS: Relativistic Evaluation of Neural Architecture Search](https://arxiv.org/pdf/1910.01523.pdf)<br>
- [OPANAS: One-Shot Path Aggregation Network Architecture Search for Object](https://arxiv.org/abs/2103.04507)
- [Towards Improving the Consistency, Efficiency, and Flexibility of Differentiable Neural Architecture Search](https://arxiv.org/abs/2101.11342)<br>åŒ—äº¬å¤§å­¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æœºå™¨å­¦ä¹ ç ”ç©¶ä¸­å¿ƒ
- [Contrastive Neural Architecture Search with Neural Architecture Comparators](https://arxiv.org/abs/2103.05471)<br>:star:[code](https://arxiv.org/abs/2103.05471)
- [Searching by Generating: Flexible and Efficient One-Shot NAS with Architecture Generator](https://arxiv.org/abs/2103.07289)<br>:star:[code](https://github.com/eric8607242/SGNAS)
- [Prioritized Architecture Sampling with Monto-Carlo Tree Search](https://arxiv.org/abs/2103.11922)<br>:star:[code](https://github.com/xiusu/NAS-Bench-Macro)
* [One-Shot Neural Ensemble Architecture Search by Diversity-Guided Search Space Shrinking](https://arxiv.org/abs/2104.00597)<br>:star:[code](https://github.com/researchmm/NEAS)
* [NetAdaptV2: Efficient Neural Architecture Search with Fast Super-Network Training and Architecture Optimization](https://arxiv.org/abs/2104.00031)<br>:house:[project](http://web.mit.edu/netadapt/)
* [Neural Architecture Search with Random Labels](https://arxiv.org/abs/2101.11834)<br>ç²—è§£ï¼š[1](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)<br>è§£è¯»ï¼š[åŸºäºéšæœºæ ‡ç­¾çš„ç¥ç»æ¶æ„æœç´¢](https://mp.weixin.qq.com/s/hNwwzgmiR505AMomenUoFg)
* [Landmark Regularization: Ranking Guided Super-Net Training in Neural Architecture Search](https://arxiv.org/abs/2104.05309)<br>:star:[code](https://github.com/kcyu2014/nas-landmarkreg)
* [ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search](https://arxiv.org/abs/2105.10154)
* [TransNAS-Bench-101: Improving Transferability and Generalizability of Cross-Task Neural Architecture Search](https://arxiv.org/abs/2105.11871)<br>:star:[code](https://www.noahlab.com.hk/opensource/vega/page/doc.html?path=datasets/transnasbench101):sunflower:[dataset](https://download.mindspore.cn/dataset/TransNAS-Bench-101/)
* [HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers](https://arxiv.org/abs/2106.06560)<br>:open_mouth:oral:star:[code](https://github.com/dingmyu/HR-NAS) 
* [DOTS: Decoupling Operation and Topology in Differentiable Architecture Search](https://arxiv.org/abs/2010.00969)<br>:star:[code](https://github.com/guyuchao/DOTS)
* [NPAS: A Compiler-Aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration](https://arxiv.org/abs/2012.00596)
* [DSRNA: Differentiable Search of Robust Neural Architectures](https://arxiv.org/abs/2012.06122)
* [Rethinking Graph Neural Architecture Search From Message-Passing](https://arxiv.org/abs/2103.14282)<br>:star:[code](https://github.com/phython96/GNAS-MP)
* [FP-NAS: Fast Probabilistic Neural Architecture Search](https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_FP-NAS_Fast_Probabilistic_Neural_Architecture_Search_CVPR_2021_paper.pdf)
* [FBNetV3: Joint Architecture-Recipe Search Using Predictor Pretraining](https://arxiv.org/abs/2006.02049)
* [AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling](https://arxiv.org/abs/2011.09011)<br>:star:[code](https://github.com/facebookresearch/AttentiveNAS)



<a name="9"/> 

## 9.Object Tracking(ç›®æ ‡è·Ÿè¸ª)

- [Rotation Equivariant Siamese Networks for Tracking](https://arxiv.org/abs/2012.13078)<br>:star:[code](https://github.com/dkgupta90/re-siamnet)
- [LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search](https://arxiv.org/abs/2104.14545)<br>:star:[code](https://github.com/researchmm/LightTrack)<br>LightTrackï¼šç”¨ç¥ç»æ¶æ„æœç´¢å¾—åˆ°çš„è½»é‡çº§è·Ÿè¸ªç½‘ç»œï¼Œç²¾åº¦è¶…è¿‡SiamRPN++ å’Œ Oceanï¼Œé€Ÿåº¦å¿«12å€ï¼Œå‚æ•°é‡åªæœ‰1/13ï¼ŒFlopsä»…æœ‰1/38ã€‚ä»£ç å°†å¼€æºã€‚
- [Track, Check, Repeat: An EM Approach to Unsupervised Tracking](https://arxiv.org/abs/2104.03424)<br>:house:[project](http://www.cs.cmu.edu/~aharley/em_cvpr21/):tv:[video](https://youtu.be/Jg2f5fkgxZo)
- [Learning To Filter: Siamese Relation Network for Robust Tracking](https://arxiv.org/abs/2104.00829)<br>:star:[code](https://github.com/hqucv/siamrn)
- [Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box Estimation](https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_Alpha-Refine_Boosting_Tracking_Performance_by_Precise_Bounding_Box_Estimation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/MasterBin-IIAU/AlphaRefine)
* [CapsuleRRT: Relationships-Aware Regression Tracking via Capsules](https://openaccess.thecvf.com/content/CVPR2021/papers/Ma_CapsuleRRT_Relationships-Aware_Regression_Tracking_via_Capsules_CVPR_2021_paper.pdf)
* [Siamese Natural Language Tracker: Tracking by Natural Language Descriptions With Siamese Trackers](https://arxiv.org/abs/1912.02048)<br>:star:[code](https://github.com/fredfung007/snlt)
* [MeanShift++: Extremely Fast Mode-Seeking With Applications to Segmentation and Object Tracking](https://openaccess.thecvf.com/content/CVPR2021/papers/Jang_MeanShift_Extremely_Fast_Mode-Seeking_With_Applications_to_Segmentation_and_Object_CVPR_2021_paper.pdf)
* [Learning To Fuse Asymmetric Feature Maps in Siamese Trackers](https://arxiv.org/abs/2012.02776)<br>:star:[code](https://github.com/wencheng256/SiamBAN-ACM)
* å¤šç›®æ ‡è·Ÿè¸ª
  * [Probabilistic Tracklet Scoring and Inpainting for Multiple Object Tracking](https://arxiv.org/abs/2012.02337)<br>:star:[code](https://github.com/fatemeh-slh/ArTIST)
  * [Track to Detect and Segment: An Online Multi-Object Tracker](https://arxiv.org/abs/2103.08808)<br>:star:[code](https://github.com/JialianW/TraDeS):house:[project](https://jialianwu.com/projects/TraDeS.html):tv:[video](https://youtu.be/oGNtSFHRZJA)<br>TraDeS ï¼šCVPR 2021å¤šç›®æ ‡è·Ÿè¸ªç®—æ³•ï¼Œæ”¹è¿›äº†ç›®å‰è”åˆæ£€æµ‹ä¸è·Ÿè¸ªçš„åœ¨çº¿æ–¹æ³•ï¼Œä½¿ç”¨è·Ÿè¸ªçº¿ç´¢è¾…åŠ©æ£€æµ‹ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†å®ç°äº†å¤§å¹…ç²¾åº¦æå‡ï¼Œä½œè€…æ¥è‡ªçº½çº¦å·ç«‹å¤§å­¦ã€‚ä»£ç å·²å¼€æºã€‚
  * [Multiple Object Tracking with Correlation Learning](https://arxiv.org/abs/2104.03541)<br>æå‡º CorrTrackerï¼Œä¸€ä¸ªç»Ÿä¸€çš„å…³è”è·Ÿè¸ªå™¨ï¼Œå¯ä»¥å¯†é›†å»ºæ¨¡ç›®æ ‡ä¹‹é—´çš„å…³è”ï¼Œå¹¶é€šè¿‡å…³è”ä¼ é€’ä¿¡æ¯ã€‚åœ¨ MOT17 ä¸Šè·å¾—æœ€å…ˆè¿›çš„ MOTA 76.5% å’Œ IDF1 73.6%ã€‚
  * [Learning a Proposal Classifier for Multiple Object Tracking](https://arxiv.org/abs/2103.07889)<br>:star:[code](https://github.com/daip13/LPC_MOT)
  * [Learnable Graph Matching: Incorporating Graph Partitioning with Deep Feature Learning for Multiple Object Tracking](https://arxiv.org/abs/2103.16178)<br>:star:[code](https://github.com/jiaweihe1996/GMTracker)
  * [Online Multiple Object Tracking with Cross-Task Synergy](https://arxiv.org/abs/2104.00380)<br>:star:[code](https://github.com/songguocode/TADAM)
  * [SiamMOT: Siamese Multi-Object Tracking](https://arxiv.org/abs/2105.11595)<br>:star:[code](https://github.com/amazon-research/siam-mot)
  * [DyGLIP: A Dynamic Graph Model with Link Prediction for Accurate Multi-Camera Multiple Object Tracking](https://arxiv.org/abs/2106.06856)<br>:star:[code](https://github.com/uark-cviu/DyGLIP)
  * [Quasi-Dense Similarity Learning for Multiple Object Tracking](https://arxiv.org/abs/2006.06664)<br>:open_mouth:oral:star:[code](https://github.com/SysCV/qdtrack)
  * [Discriminative Appearance Modeling With Multi-Track Pooling for Real-Time Multi-Object Tracking](http://arxiv.org/abs/2101.12159)<br>:star:[code](https://github.com/chkim403/blstm-mtp)
  * [GMOT-40: A Benchmark for Generic Multiple Object Tracking](https://openaccess.thecvf.com/content/CVPR2021/papers/Bai_GMOT-40_A_Benchmark_for_Generic_Multiple_Object_Tracking_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/Spritea/GMOT40)
  * [Distractor-Aware Fast Tracking via Dynamic Convolutions and MOT Philosophy](https://arxiv.org/abs/2104.12041)<br>:star:[code](https://github.com/hqucv/dmtrack)
  * [Improving Multiple Object Tracking With Single Object Tracking](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Improving_Multiple_Object_Tracking_With_Single_Object_Tracking_CVPR_2021_paper.pdf)
  * 3Då¤šç›®æ ‡è·Ÿè¸ª
    * [Seeing Behind Objects for 3D Multi-Object Tracking in RGB-D Sequences](https://openaccess.thecvf.com/content/CVPR2021/papers/Muller_Seeing_Behind_Objects_for_3D_Multi-Object_Tracking_in_RGB-D_Sequences_CVPR_2021_paper.pdf)
* è§†è§‰ç›®æ ‡è·Ÿè¸ª
  * [IoU Attack: Towards Temporally Coherent Black-Box Adversarial Attack for Visual Object Tracking](https://arxiv.org/abs/2103.14938)<br>:star:[code](https://github.com/VISION-SJTU/IoUattack)
  * [Learning to Track Instances without Video Annotations](https://arxiv.org/abs/2104.00287)<br>:open_mouth:oral:house:[project](https://oasisyang.github.io/projects/semi-track/index.html):tv:[video](https://youtu.be/-S7xtk-7pGk)
  * [Progressive Unsupervised Learning for Visual Object Tracking](https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Progressive_Unsupervised_Learning_for_Visual_Object_Tracking_CVPR_2021_paper.pdf)
* å•ç›®æ ‡è·Ÿè¸ª
  * [Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark](https://arxiv.org/abs/2103.16746)<br>:house:[project](https://sites.google.com/view/langtrackbenchmark/):tv:[video](https://www.youtube.com/watch?v=7lvVDlkkff0)
  * [SiamGAT: Graph Attention Tracking](https://arxiv.org/abs/2011.11204)<br>:star:[code](https://github.com/ohhhyeahhh/SiamGAT)
 * è§†è§‰è·Ÿè¸ª
  * [STMTrack: Template-Free Visual Tracking With Space-Time Memory Networks](https://arxiv.org/abs/2104.00324)<br>:star:[code](https://github.com/fzh0917/STMTrack)
* å§¿åŠ¿è·Ÿè¸ª
  * [TesseTrack: End-to-End Learnable Multi-Person Articulated 3D Pose Tracking](https://openaccess.thecvf.com/content/CVPR2021/papers/Reddy_TesseTrack_End-to-End_Learnable_Multi-Person_Articulated_3D_Pose_Tracking_CVPR_2021_paper.pdf)<br>:house:[project](http://www.cs.cmu.edu/~ILIM/projects/IM/TesseTrack/)
* è¡Œäººè·Ÿè¸ª
  * [Improving Multiple Pedestrian Tracking by Track Management and Occlusion Handling](https://openaccess.thecvf.com/content/CVPR2021/papers/Stadler_Improving_Multiple_Pedestrian_Tracking_by_Track_Management_and_Occlusion_Handling_CVPR_2021_paper.pdf)
 
<a name="8"/> 

## 8.Image Segmentation(å›¾åƒåˆ†å‰²)

- [Information-Theoretic Segmentation by Inpainting Error Maximization](https://arxiv.org/abs/2012.07287)<br>
- [Simultaneously Localize, Segment and Rank the Camouflaged Objects](https://arxiv.org/abs/2103.04011)<br>:star:[code](https://github.com/JingZhang617/COD-Rank-Localize-and-Segment)
- [Capturing Omni-Range Context for Omnidirectional Segmentation](https://arxiv.org/abs/2103.05687)<br>:star:[code](https://github.com/elnino9ykl/WildPASS)
- [Boundary IoU: Improving Object-Centric Image Segmentation Evaluation](https://arxiv.org/abs/2103.16562)<br>:star:[code](https://github.com/bowenc0221/boundary-iou-api):house:[project](https://bowenc0221.github.io/boundary-iou/)
* [Locate then Segment: A Strong Pipeline for Referring Image Segmentation](https://arxiv.org/abs/2103.16284)
* [InverseForm: A Loss Function for Structured Boundary-Aware Segmentation](https://arxiv.org/abs/2104.02745)<br>:open_mouth:oral
* [Omnimatte: Associating Objects and Their Effects in Video](https://arxiv.org/abs/2105.06993)<br>:open_mouth:oral:house:[project](https://omnimatte.github.io/)
* [Unsupervised Part Segmentation through Disentangling Appearance and Shape](https://arxiv.org/abs/2105.12405)
* [Encoder Fusion Network With Co-Attention Embedding for Referring Image Segmentation](https://arxiv.org/abs/2105.01839)
* [Bottom-Up Shift and Reasoning for Referring Image Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Bottom-Up_Shift_and_Reasoning_for_Referring_Image_Segmentation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/incredibleXM/BUSNet)
* [DCNAS: Densely Connected Neural Architecture Search for Semantic Image Segmentation](https://arxiv.org/abs/2003.11883)
* [Unsupervised Part Segmentation Through Disentangling Appearance and Shape](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Unsupervised_Part_Segmentation_Through_Disentangling_Appearance_and_Shape_CVPR_2021_paper.pdf)
* [ATSO: Asynchronous Teacher-Student Optimization for Semi-Supervised Image Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Huo_ATSO_Asynchronous_Teacher-Student_Optimization_for_Semi-Supervised_Image_Segmentation_CVPR_2021_paper.pdf)
* [DyStaB: Unsupervised Object Segmentation via Dynamic-Static Bootstrapping](https://arxiv.org/abs/2008.07012)
* å®ä¾‹åˆ†å‰²
  * [Zero-Shot Instance Segmentation](https://arxiv.org/abs/2104.06601)<br>:star:[code](https://github.com/zhengye1995/Zero-shot-Instance-Segmentation)<br>åˆ›æ–°å¥‡æ™ºé¦–æ¬¡æå‡ºé›¶æ ·æœ¬å®ä¾‹åˆ†å‰²ï¼ŒåŠ©åŠ›è§£å†³å·¥ä¸šåœºæ™¯æ•°æ®ç“¶é¢ˆéš¾é¢˜
  * [Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers](https://arxiv.org/abs/2103.12340)<br>:star:[code](https://github.com/lkeab/BCNet)<br>è§£è¯»ï¼š[åŒå›¾å±‚å®ä¾‹åˆ†å‰²ï¼Œå¤§å¹…æå‡é®æŒ¡å¤„ç†æ€§èƒ½](https://mp.weixin.qq.com/s/LL3uw1ZVxio20wrvzM8Vag)
  * [Weakly Supervised Instance Segmentation for Videos with Temporal Mask Consistency](https://arxiv.org/abs/2103.12886)
  * [FAPIS: A Few-shot Anchor-free Part-based Instance Segmenter](https://arxiv.org/abs/2104.00073)
  * [Weakly-supervised Instance Segmentation via Class-agnostic Learning with Salient Images](https://arxiv.org/abs/2104.01526)<br>:star:[code](https://github.com/hustvl/BoxCaseg)
  * [Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation](https://arxiv.org/abs/2104.05239)<br>:star:[code](https://github.com/tinyalpha/BPR)
  * [RefineMask: Towards High-Quality Instance Segmentation with Fine-Grained Features](https://arxiv.org/abs/2104.08569)<br>:star:[code](https://github.com/zhanggang001/RefineMask)
  * [A^2-FPN: Attention Aggregation based Feature Pyramid Network for Instance Segmentation](https://arxiv.org/abs/2105.03186)
  * [Incremental Few-Shot Instance Segmentation](https://arxiv.org/abs/2105.05312)<br>:star:[code](https://github.com/danganea/iMTFA)
  * [Simple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation](https://arxiv.org/abs/2012.07177)<br>:star:[code](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/copy_paste)
  * [A2-FPN: Attention Aggregation Based Feature Pyramid Network for Instance Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_A2-FPN_Attention_Aggregation_Based_Feature_Pyramid_Network_for_Instance_Segmentation_CVPR_2021_paper.pdf)
  * [Point Cloud Instance Segmentation Using Probabilistic Embeddings](https://arxiv.org/abs/1912.00145)<br>:house:[project](http://1zb.github.io/publication/prob-embed/)
  * [FAPIS: A Few-Shot Anchor-Free Part-Based Instance Segmenter](https://arxiv.org/abs/2104.00073)
  * [DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_DCT-Mask_Discrete_Cosine_Transform_Mask_Representation_for_Instance_Segmentation_CVPR_2021_paper.pdf)
  * [Robust Instance Segmentation Through Reasoning About Multi-Object Occlusion](https://arxiv.org/abs/2012.02107)<br>:star:[code](https://github.com/XD7479/Multi-Object-Occlusion)
  * [Deeply Shape-Guided Cascade for Instance Segmentation](https://arxiv.org/abs/1911.11263)<br>:star:[code](https://github.com/hding2455/DSC)
  * [ColorRL: Reinforced Coloring for End-to-End Instance Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Tuan_ColorRL_Reinforced_Coloring_for_End-to-End_Instance_Segmentation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/anhtuanhsgs/ColorRL)
  * [Seesaw Loss for Long-Tailed Instance Segmentation](https://arxiv.org/abs/2008.10032)<br>:star:[code](https://github.com/open-mmlab/mmdetection)
  * [Unsupervised Discovery of the Long-Tail in Instance Segmentation Using Hierarchical Self-Supervision](https://arxiv.org/abs/2104.01257)
  * [DyCo3D: Robust Instance Segmentation of 3D Point Clouds through Dynamic Convolution](https://arxiv.org/abs/2011.13328)<br>:star:[code](https://github.com/aim-uofa/DyCo3D)
  * [BoxInst: High-Performance Instance Segmentation With Box Annotations](https://arxiv.org/abs/2012.02310)<br>:star:[code](https://github.com/aim-uofa/AdelaiDet/)
* å…¨æ™¯åˆ†å‰²
  * [4D Panoptic LiDAR Segmentation](https://arxiv.org/abs/2102.12472)<br>:star:[code](https://github.com/mehmetaygun/4d-pls)
  * [Cross-View Regularization for Domain Adaptive Panoptic Segmentation](https://arxiv.org/abs/2103.02584)<br>:open_mouth:oral<br>ç”¨äºåŸŸè‡ªé€‚åº”å…¨æ™¯åˆ†å‰²çš„è·¨è§†å›¾æ­£åˆ™åŒ–æ–¹æ³•<br>
  * [Part-aware Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/de_Geus_Part-Aware_Panoptic_Segmentation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/pmeletis/panoptic_parts)
  * [Toward Joint Thing-and-Stuff Mining for Weakly Supervised Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_Toward_Joint_Thing-and-Stuff_Mining_for_Weakly_Supervised_Panoptic_Segmentation_CVPR_2021_paper.pdf)<br>è”åˆç‰©ä½“å’Œç‰©è´¨æŒ–æ˜çš„å¼±ç›‘ç£å…¨æ™¯åˆ†å‰²<br>è§£è¯»ï¼š[15](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [Panoptic-PolarNet: Proposal-free LiDAR Point Cloud Panoptic Segmentation](https://arxiv.org/abs/2103.14962)<br>:star:[code](https://github.com/edwardzhou130/Panoptic-PolarNet)
  * [Fully Convolutional Networks for Panoptic Segmentation](https://arxiv.org/abs/2012.00720)<br>:open_mouth:oral:star:[code](https://github.com/yanwei-li/PanopticFCN)<br>ç²—è§£ï¼š[11](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
  * [Panoptic Segmentation Forecasting](https://arxiv.org/abs/2104.03962)<br>:star:[code](https://github.com/nianticlabs/panoptic-forecasting)
  * [Exemplar-Based Open-Set Panoptic Segmentation Network](https://arxiv.org/abs/2105.08336)<br>:star:[code](https://github.com/jd730/EOPSN):house:[project](https://cv.snu.ac.kr/research/EOPSN/)
  * [Hierarchical Lovasz Embeddings for Proposal-free Panoptic Segmentation](https://arxiv.org/abs/2106.04555)
  * [VIP-DeepLab: Learning Visual Perception With Depth-Aware Video Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Qiao_VIP-DeepLab_Learning_Visual_Perception_With_Depth-Aware_Video_Panoptic_Segmentation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/joe-siyuan-qiao/ViP-DeepLab)
  * [Learning To Associate Every Segment for Video Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Woo_Learning_To_Associate_Every_Segment_for_Video_Panoptic_Segmentation_CVPR_2021_paper.pdf)
  * [LiDAR-Based Panoptic Segmentation via Dynamic Shifting Network](https://arxiv.org/abs/2011.11964)<br>:star:[code](https://github.com/hongfz16/DS-Net)
  * [LPSNet: A Lightweight Solution for Fast Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_LPSNet_A_Lightweight_Solution_for_Fast_Panoptic_Segmentation_CVPR_2021_paper.pdf)
  * [Improving Panoptic Segmentation at All Scales](https://arxiv.org/abs/2012.07717)
* è¯­ä¹‰åˆ†å‰²
  * [PLOP: Learning without Forgetting for Continual Semantic Segmentation](https://arxiv.org/abs/2011.11390)<br>:star:[code](https://github.com/arthurdouillard/CVPR2021_PLOP)
  * [Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges](https://arxiv.org/abs/2009.03137)<br>:sunflower:[dataset](https://github.com/QingyongHu/SensatUrban):tv:[video](https://www.youtube.com/watch?v=IG0tTdqB3L8)<br>
  * [Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation](https://arxiv.org/abs/2103.04717)
  * [Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation](https://arxiv.org/abs/2103.04705)
  * [Differentiable Multi-Granularity Human Representation Learning for Instance-Aware Human Semantic Parsing](https://arxiv.org/abs/2103.04570)<br>:open_mouth:oral:star:[code](https://github.com/tfzhou/MG-HumanParsing)
  * [Learning Statistical Texture for Semantic Segmentation](https://arxiv.org/abs/2103.04133)
  * [MetaCorrection: Domain-aware Meta Loss Correction for Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2103.05254)<br>:star:[code](https://github.com/cyang-cityu/MetaCorrection)<br>è¯­ä¹‰åˆ†å‰²ä¸­çš„æ— ç›‘ç£åŸŸé€‚åº”çš„åŸŸæ„ŸçŸ¥å…ƒæŸå¤±æ ¡æ­£
  * [Continual Semantic Segmentation via Repulsion-Attraction of Sparse and Disentangled Latent Representations](https://arxiv.org/abs/2103.06342)<br>:star:[code](https://github.com/LTTM/SDR):tv:[video](https://lttm.dei.unipd.it/paper_data/SDR/)
  * [Semantic Segmentation for Real Point Cloud Scenes via Bilateral Augmentation and Adaptive Fusion](https://arxiv.org/abs/2103.07074)<br>:star:[code](https://github.com/ShiQiu0419/BAAF-Net)
  * [Rethinking BiSeNet For Real-time Semantic Segmentation](https://arxiv.org/abs/2104.13188)<br>:star:[code](https://github.com/MichaelFan01/STDC-Seg)
  * [BBAM: Bounding Box Attribution Map for Weakly Supervised Semantic and Instance Segmentation](https://arxiv.org/abs/2103.08907)<br>:star:[code](https://github.com/jbeomlee93/BBAM)
  * [Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2103.08896)<br>:star:[code](https://github.com/jbeomlee93/AdvCAM)
  * [Cross-Dataset Collaborative Learning for Semantic Segmentation](https://arxiv.org/abs/2103.11351)
  * [Coarse-to-Fine Domain Adaptive Semantic Segmentation with Photometric Alignment and Category-Center Regularization](https://arxiv.org/abs/2103.13041)
  * [Non-Salient Region Object Mining for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2103.14581)<br>:star:[code](https://github.com/NUST-Machine-Intelligence-Laboratory/nsrom)
  * [Source-Free Domain Adaptation for Semantic Segmentation](https://arxiv.org/abs/2103.16372)
  * [PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering](https://arxiv.org/abs/2103.17070)<br>:star:[code](https://github.com/janghyuncho/PiCIE)
  * [Background-Aware Pooling and Noise-Aware Loss for Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2104.00905)<br>:house:[project](https://cvlab.yonsei.ac.kr/projects/BANA/)
  * [Progressive Semantic Segmentation](https://arxiv.org/abs/2104.03778)<br>:star:[code](https://github.com/VinAIResearch/MagNet)
  * [Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization](https://arxiv.org/abs/2104.05833)<br>:house:[project](https://nv-tlabs.github.io/semanticGAN/)
  * [DANNet: A One-Stage Domain Adaptation Network for Unsupervised Nighttime Semantic Segmentation](https://arxiv.org/abs/2104.10834)<br>:open_mouth:oral:star:[code](https://github.com/W-zx-Y/DANNet)<br>å®ç°å¤œé—´è¯­ä¹‰åˆ†å‰²æœ€å…ˆè¿›æ€§èƒ½ï¼Œå·²å¼€æºã€‚
  * [Self-supervised Augmentation Consistency for Adapting Semantic Segmentation](https://arxiv.org/abs/2105.00097)<br>:star:[code](https://github.com/visinf/da-sac)
  * [Railroad is not a Train: Saliency as Pseudo-pixel Supervision for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2105.08965)<br>:star:[code](https://github.com/halbielee/EPS)
  * [Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision](https://arxiv.org/abs/2106.01226)<br>:star:[code](https://github.com/charlesCXK/TorchSemiSeg)
  * [Cluster, Split, Fuse, and Update: Meta-Learning for Open Compound Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2012.08278)
  * [Semi-Supervised Semantic Segmentation With Directional Context-Aware Consistency](https://openaccess.thecvf.com/content/CVPR2021/papers/Lai_Semi-Supervised_Semantic_Segmentation_With_Directional_Context-Aware_Consistency_CVPR_2021_paper.pdf)
  * [Scale-Aware Graph Neural Network for Few-Shot Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Xie_Scale-Aware_Graph_Neural_Network_for_Few-Shot_Semantic_Segmentation_CVPR_2021_paper.pdf) 
  * [Uncertainty Reduction for Model Adaptation in Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/S_Uncertainty_Reduction_for_Model_Adaptation_in_Semantic_Segmentation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/idiap/model-uncertainty-for-adaptation)
  * [HyperSeg: Patch-Wise Hypernetwork for Real-Time Semantic Segmentation](https://arxiv.org/abs/2012.11582)<br>:star:[code](https://github.com/YuvalNirkin/hyperseg):house:[project](https://nirkin.com/hyperseg/)
  * [Complete & Label: A Domain Adaptation Approach to Semantic Segmentation of LiDAR Point Clouds](https://arxiv.org/abs/2007.08488)
  * [Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2101.10979)<br>:star:[code](https://github.com/microsoft/ProDA)
  * [Few-Shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2006.12052)<br>:star:[code](https://github.com/Na-Z/attMPTI)
  * [Anti-Aliasing Semantic Reconstruction for Few-Shot Semantic Segmentation](https://arxiv.org/abs/2106.00184)<br>:star:[code](https://github.com/Bibkiller/ASR)
  * [Three Ways to Improve Semantic Segmentation with Self-Supervised Depth Estimation](https://arxiv.org/abs/2012.10782)<br>:star:[code](https://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth)
  * [(AF)2-S3Net: Attentive Feature Fusion With Adaptive Feature Selection for Sparse Semantic Segmentation Network](https://openaccess.thecvf.com/content/CVPR2021/papers/Cheng_AF2-S3Net_Attentive_Feature_Fusion_With_Adaptive_Feature_Selection_for_Sparse_CVPR_2021_paper.pdf)
  * [One Thing One Click: A Self-Training Approach for Weakly Supervised 3D Semantic Segmentation](https://arxiv.org/abs/2104.02246)
  * [Exploit Visual Dependency Relations for Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Exploit_Visual_Dependency_Relations_for_Semantic_Segmentation_CVPR_2021_paper.pdf)
  * [Revisiting Superpixels for Active Learning in Semantic Segmentation With Realistic Annotation Costs](https://openaccess.thecvf.com/content/CVPR2021/papers/Cai_Revisiting_Superpixels_for_Active_Learning_in_Semantic_Segmentation_With_Realistic_CVPR_2021_paper.pdf)
  * [ABMDRNet: Adaptive-Weighted Bi-Directional Modality Difference Reduction Network for RGB-T Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_ABMDRNet_Adaptive-Weighted_Bi-Directional_Modality_Difference_Reduction_Network_for_RGB-T_Semantic_CVPR_2021_paper.pdf)
  * [CGA-Net: Category Guided Aggregation for Point Cloud Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Lu_CGA-Net_Category_Guided_Aggregation_for_Point_Cloud_Semantic_Segmentation_CVPR_2021_paper.pdf)
* åœºæ™¯ç†è§£/åœºæ™¯è§£æ
  * [Bidirectional Projection Network for Cross Dimension Scene Understanding](https://arxiv.org/abs/2103.14326)<br>:open_mouth:oral:star:[code](https://github.com/wbhu/BPNet)
  * [RobustNet: Improving Domain Generalization in Urban-Scene Segmentation via Instance Selective Whitening](https://arxiv.org/abs/2103.15597)<br>:open_mouth:oral:star:[code](https://github.com/shachoi/RobustNet)
  * [CoCoNets: Continuous Contrastive 3D Scene Representations](https://arxiv.org/abs/2104.03851)<br>:house:[project](https://mihirp1998.github.io/project_pages/coconets/):tv:[video](https://youtu.be/n_own_d7Fh8)<br>æ¥è‡ªCMUçš„å­¦è€…æå‡ºä¸€ç§3Dåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œåˆ©ç”¨è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ å’Œè¾“å…¥çš„RGBä¸RGBDåœºæ™¯æ•°æ®å­¦ä¹ è€Œæ¥ï¼Œè¿™ç§ç‰¹å¾è¡¨ç¤ºæ–¹æ³•åœ¨ç›®æ ‡è·Ÿè¸ªã€æ£€æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚
  * [RfD-Net: Point Scene Understanding by Semantic Instance Reconstruction](https://openaccess.thecvf.com/content/CVPR2021/papers/Nie_RfD-Net_Point_Scene_Understanding_by_Semantic_Instance_Reconstruction_CVPR_2021_paper.pdf)
  * [3D-to-2D Distillation for Indoor Scene Parsing](https://arxiv.org/abs/2104.02243)
  * [Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts](https://arxiv.org/abs/2012.09165)
  * åœºæ™¯å›¾åˆæˆ/åˆ†æ
    * [SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences](https://arxiv.org/abs/2103.14898)<br>:house:[project](https://shunchengwu.github.io/SceneGraphFusion)
    * [Probabilistic Modeling of Semantic Ambiguity for Scene Graph Generation](https://arxiv.org/abs/2103.05271)<br>åœºæ™¯å›¾ç”Ÿæˆ---åœºæ™¯è§£æ
    * [Exploiting Edge-Oriented Reasoning for 3D Point-based Scene Graph Analysis](https://arxiv.org/abs/2103.05558)<br>:house:[project](https://sggpoint.github.io/)<br>åˆ©ç”¨é¢å‘è¾¹ç¼˜çš„æ¨ç†è¿›è¡ŒåŸºäº3Dç‚¹çš„åœºæ™¯å›¾åˆ†æ---åœºæ™¯ç†è§£
    * [Fully Convolutional Scene Graph Generation](https://arxiv.org/abs/2103.16083)<br>:open_mouth:oral
    * [Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene Graph Generation](https://arxiv.org/abs/2104.00308)<br>:star:[code](https://github.com/Scarecrow0/BGNN-SGG)
    * [Linguistic Structures as Weak Supervision for Visual Scene Graph Generation](https://arxiv.org/abs/2105.13994)<br>:star:[code](https://github.com/yekeren/WSSGG)
    * [Energy-Based Learning for Scene Graph Generation](https://arxiv.org/abs/2103.02221)<br>:star:[code](https://github.com/mods333/energy-based-scene-graph)
  * 3D åœºæ™¯ç†è§£
    * [Holistic 3D Scene Understanding From a Single Image With Implicit Representation](https://arxiv.org/abs/2103.06422)<br>:star:[code](https://github.com/chengzhag/Implicit3DUnderstanding):house:[project](https://chengzhag.github.io/publication/im3d/):tv:[video](https://youtu.be/Kg0du7mFu60)
    * [Monte Carlo Scene Search for 3D Scene Understanding](https://arxiv.org/abs/2103.07969)<br>:house:[project](https://www.tugraz.at/index.php?id=50484):tv:[video](https://youtu.be/4kAfuymevUw)
    * [Exploring Data Efficient 3D Scene Understanding with Contrastive Scene Contexts](https://arxiv.org/abs/2012.09165)<br>:open_mouth:oral:house:[project](https://sekunde.github.io/project_efficient/):tv:[video](https://youtu.be/E70xToZLgs4)
* æŠ å›¾
  * [Real-Time High Resolution Background Matting](https://arxiv.org/abs/2012.07810)<br>:star:[code](https://github.com/PeterL1n/BackgroundMattingV2):house:[project](https://grail.cs.washington.edu/projects/background-matting-v2/):tv:[video](https://youtu.be/oMfPTeYDF9g)<br>æœ€æ–°å¼€æºæŠ å›¾æŠ€æœ¯ï¼Œå®æ—¶å¿«é€Ÿé«˜åˆ†è¾¨ç‡ï¼Œ4k(30fps)ã€ç°ä»£GPUï¼ˆ60fpsï¼‰<br>è§£è¯»ï¼š[å•å—GPUå®ç°4Kåˆ†è¾¨ç‡æ¯ç§’30å¸§ï¼Œåç››é¡¿å¤§å­¦å®æ—¶è§†é¢‘æŠ å›¾å†å‡çº§ï¼Œæ¯›å‘ç»†èŠ‚åˆ°ä½](https://mp.weixin.qq.com/s/0OJR3Y5cPfeHhdTdI3BgEA)<br>[æœ€æ–°å¼€æºæŠ å›¾æŠ€æœ¯ï¼Œå®æ—¶å¿«é€Ÿé«˜åˆ†è¾¨ç‡ï¼Œ4k(30fps)ã€ç°ä»£GPUï¼ˆ60fpsï¼‰](https://zhuanlan.zhihu.com/p/337028483)
  * [Mask Guided Matting via Progressive Refinement Network](https://arxiv.org/abs/2012.06722)<br>:star:[code](https://github.com/yucornetto/MGMatting)
  * [Semantic Image Matting](http://arxiv.org/abs/2104.08201)<br>:star:[code](https://github.com/nowsyn/SIM)
  * [Real-Time High-Resolution Background Matting](https://arxiv.org/abs/2012.07810)  
  * [Improved Image Matting via Real-Time User Clicks and Uncertainty Estimation](https://arxiv.org/abs/2012.08323)<br>:tv:[video](https://www.youtube.com/watch?v=pAXydeN-LpQ)
  * [Learning Affinity-Aware Upsampling for Deep Image Matting](https://arxiv.org/abs/2011.14288)
* é›·è¾¾åˆ†å‰²
  * [Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR Segmentation](https://arxiv.org/abs/2011.10033)<br>:open_mouth:oral:star:[code](https://github.com/xinge008/Cylinder3D)<br>åœ¨ SemanticKITTI æ¦œå•æ’åç¬¬ä¸€ï¼ˆuntil CVPR DDLï¼‰ï¼Œåœ¨ nuScenes ä¸­è·å¾— SOTAï¼Œå¹¶å¯¹å…¶ä»–åŸºäºæ¿€å…‰é›·è¾¾çš„ä»»åŠ¡ä¿æŒäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¿€å…‰é›·è¾¾å…¨æ™¯åˆ†å‰²å’Œæ¿€å…‰é›·è¾¾ä¸‰ç»´æ£€æµ‹ï¼Œå…¶ä¸­å°±åŸºäºæ­¤å·¥ä½œï¼Œåœ¨ SemanticKITTI å…¨æ™¯åˆ†å‰²æ¦œå•ä¹Ÿæ’åç¬¬ä¸€ã€‚
* è§†é¢‘ç›®æ ‡åˆ†å‰²
  * [Modular Interactive Video Object Segmentation:Interaction-to-Mask, Propagation and Difference-Aware Fusion](https://arxiv.org/abs/2103.07941)<br>:open_mouth:oral:star:[code](https://github.com/hkchengrex/MiVOS):house:[project](https://hkchengrex.github.io/MiVOS/):tv:[video](https://hkchengrex.github.io/MiVOS/video.html)
  * [Learning to Recommend Frame for Interactive Video Object Segmentation in the Wild](https://arxiv.org/abs/2103.10391)<br>:star:[code](https://github.com/svip-lab/IVOS-W)
  * [Efficient Regional Memory Network for Video Object Segmentation](https://arxiv.org/abs/2103.12934)<br>:star:[code](https://github.com/hzxie/RMNet):house:[project](https://infinitescript.com/project/rmnet)
  * [Learning Position and Target Consistency for Memory-based Video Object Segmentation](https://arxiv.org/abs/2104.04329)<br>åœ¨ DAVIS å’Œ Youtube-VOS åŸºå‡†ä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ DAVIS 2020 æŒ‘æˆ˜åŠç›‘ç£ VOS ä»»åŠ¡ä¸­æ’åç¬¬ä¸€ã€‚
  * [Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps](https://arxiv.org/abs/2104.10386)<br>:open_mouth:oral:star:[code](https://github.com/yuk6heo/GIS-RAmap)
  * [Reciprocal Transformations for Unsupervised Video Object Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Reciprocal_Transformations_for_Unsupervised_Video_Object_Segmentation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/OliverRensu/RTNet)
  * [Delving Deep Into Many-to-Many Attention for Few-Shot Video Object Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Delving_Deep_Into_Many-to-Many_Attention_for_Few-Shot_Video_Object_Segmentation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/scutpaul/DANet)
  * [Video Object Segmentation Using Global and Instance Embedding Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_Video_Object_Segmentation_Using_Global_and_Instance_Embedding_Learning_CVPR_2021_paper.pdf)
  * [SwiftNet: Real-Time Video Object Segmentation](https://arxiv.org/abs/2102.04604)<br>:star:[code](https://github.com/haochenheheda/SwiftNet)
  * [SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation](https://arxiv.org/abs/2101.08833)<br>:star:[code](https://github.com/dukebw/SSTVOS)
  * [Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion](https://arxiv.org/abs/2103.07941)<br>:star:[code](https://github.com/hkchengrex/MiVOS):house:[project](https://hkchengrex.github.io/MiVOS/):tv:[video](https://hkchengrex.github.io/MiVOS/video.html)
  * [Learning Dynamic Network Using a Reuse Gate Function in Semi-Supervised Video Object Segmentation](https://arxiv.org/abs/2012.11655)<br>:star:[code](https://github.com/HYOJINPARK/Reuse_VOS)
  * point set tracking
    * [Polygonal Point Set Tracking](https://arxiv.org/abs/2105.14584)
  * è§†é¢‘å¤šç›®æ ‡åˆ†å‰²
    * [Target-Aware Object Discovery and Association for Unsupervised Video Multi-Object Segmentation](https://arxiv.org/abs/2104.04782)
* è§†é¢‘å®ä¾‹åˆ†å‰²
  * [SG-Net: Spatial Granularity Network for One-Stage Video Instance Segmentation](https://arxiv.org/abs/2103.10284)<br>:star:[code](https://github.com/goodproj13/SG-Net):tv:[video](https://www.youtube.com/watch?v=zft0T3YUgpM)<br>æ–‡ç« ä»‹ç»ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å•é˜¶æ®µæ¡†æ¶ï¼šSG-Netï¼Œä¸ä¼ ç»Ÿçš„ä¸¤é˜¶æ®µæ¡†æ¶ç›¸æ¯”ï¼Œå¯ä»¥æœ‰æ•ˆæé«˜æ©ç è´¨é‡å’Œæ¨ç†é€Ÿåº¦ã€‚
  * [Spatial Feature Calibration and Temporal Fusion for Effective One-stage Video Instance Segmentation](https://arxiv.org/abs/2104.05606)<br>:star:[code](https://github.com/MinghanLi/STMask)
* å°æ ·æœ¬åˆ†å‰²
  * [Self-Guided and Cross-Guided Learning for Few-Shot Segmentation](https://arxiv.org/abs/2103.16129)<br>:star:[code](https://github.com/zbf1991/SCL)
  * [Adaptive Prototype Learning and Allocation for Few-Shot Segmentation](https://arxiv.org/abs/2104.01893)<br>:star:[code](https://github.com/Reagan1311/ASGNet)
  * [Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?](https://openaccess.thecvf.com/content/CVPR2021/papers/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation) 
* ä¼ªè£…ç›®æ ‡åˆ†å‰²
  * [Camouflaged Object Segmentation with Distraction Mining](https://arxiv.org/abs/2104.10475)<br>:house:[project](https://mhaiyang.github.io/CVPR2021_PFNet/index)
* è§†é¢‘æŠ å›¾
  * [Deep Video Matting via Spatio-Temporal Alignment and Aggregation](https://arxiv.org/abs/2104.11208)<br>:sunflower:[dataset](https://github.com/nowsyn/DVM)
* ç‚¹äº‘åˆ†å‰²
  * [Omni-supervised Point Cloud Segmentation via Gradual Receptive Field Component Reasoning](https://arxiv.org/abs/2105.10203)<br>:star:[code](https://github.com/azuki-miho/RFCR)
* è¯­ä¹‰éƒ¨åˆ†åˆ†å‰²
  * [Repurposing GANs for One-Shot Semantic Part Segmentation](https://arxiv.org/abs/2103.04379)<br>:open_mouth:oral:house:[project](https://repurposegans.github.io/)
* é•œåƒåˆ†å‰²
  * [Depth-Aware Mirror Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Mei_Depth-Aware_Mirror_Segmentation_CVPR_2021_paper.pdf)<br>:house:[project](https://mhaiyang.github.io/CVPR2021_PDNet/index):tv:[video](https://mhaiyang.github.io/CVPR2021_PDNet/report.mp4)
* è¿åŠ¨åˆ†å‰²
  * [Learning To Segment Rigid Motions From Two Frames](https://arxiv.org/abs/2101.03694)<br>:star:[code](https://github.com/gengshan-y/rigidmask)
* ç»†ç²’åº¦åˆ†å‰²
  * [Learning Fine-Grained Segmentation of 3D Shapes without Part Labels](https://arxiv.org/abs/2103.13030)
 
<a name="7"/> 

## 7.Object Detection(ç›®æ ‡æ£€æµ‹)

- [Multiple Instance Active Learning for Object Detection](https://arxiv.org/pdf/2104.02324.pdf)<br>:star:[code](https://github.com/yuantn/MIAL)<br>
- [Positive-Unlabeled Data Purification in the Wild for Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_Positive-Unlabeled_Data_Purification_in_the_Wild_for_Object_Detection_CVPR_2021_paper.pdf)<br>
- [Depth from Camera Motion and Object Detection](https://arxiv.org/abs/2103.01468)<br>:star:[github](https://github.com/griffbr/ODMD):tv:[video](https://www.youtube.com/watch?v=GruhbdJ2l7k)<br>é€šè¿‡ä½¿ç”¨â€œæ™®é€šæ‰‹æœºæ‘„åƒå¤´è¿åŠ¨+ç›®æ ‡æ£€æµ‹çš„åŒ…å›´æ¡†â€æ•°æ®ï¼Œè®¾è®¡RNNç½‘ç»œå®ç°äº†è¾¾åˆ°æœ€å…ˆè¿›ç²¾åº¦çš„ç›®æ ‡æ·±åº¦ä¼°è®¡ã€‚<br>
- [Towards Open World Object Detection](https://arxiv.org/abs/2103.02603)<br>:open_mouth:oral:star:[code](https://github.com/JosephKJ/OWOD)<br>
- [General Instance Distillation for Object Detection](https://arxiv.org/abs/2103.02340)<br>è¿‘å¹´æ¥ï¼ŒçŸ¥è¯†è’¸é¦å·²è¢«è¯æ˜æ˜¯æ¨¡å‹å‹ç¼©çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚å¯ä»¥ä½¿è½»é‡çº§çš„å­¦ç”Ÿæ¨¡å‹è·å¾—ä»ç¹ççš„æ•™å¸ˆæ¨¡å‹ä¸­æå–çš„çŸ¥è¯†ï¼Œä½†ä»¥å¾€çš„æ£€æµ‹è’¸é¦æ–¹æ³•å¯¹äºä¸åŒçš„æ£€æµ‹æ¡†æ¶çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ï¼Œè€Œä¸”ä¸¥é‡ä¾èµ–ground truthï¼ˆGTï¼‰ï¼Œå¿½ç•¥äº†å®ä¾‹ä¹‹é—´æœ‰ä»·å€¼çš„å…³ç³»ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œä½œè€…åœ¨æœ¬æ–‡ä¸­æå‡ºæ–°çš„åŸºäºåˆ¤åˆ«æ€§å®ä¾‹çš„æ£€æµ‹ä»»åŠ¡è’¸é¦æ–¹æ³•ï¼Œä¸è€ƒè™‘ GT åŒºåˆ†çš„æ­£è´Ÿï¼Œå‘½åä¸ºé€šç”¨å®ä¾‹è’¸é¦ï¼ˆGIDï¼‰ã€‚è¯¥æ–¹æ³•åŒ…å«ä¸€ä¸ªé€šç”¨å®ä¾‹é€‰æ‹©æ¨¡å—(GISM)ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨åŸºäºç‰¹å¾ã€åŸºäºå…³ç³»å’ŒåŸºäºå“åº”çš„çŸ¥è¯†è¿›è¡Œè’¸é¦ã€‚å®éªŒéªŒè¯ï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨å„ç§æ£€æµ‹æ¡†æ¶ä¸­å¯ä»¥å®ç°æ˜¾è‘—çš„ AP æ”¹è¿›ï¼Œç”šè‡³ä¼˜äºæ•™å¸ˆæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒRetinaNet ä¸ ResNet-50 åœ¨ COCO æ•°æ®é›†ä¸Šç”¨ GID å®ç°äº†39.1% çš„ mAPï¼Œæ¯”åŸºçº¿ 36.2% è¶…å‡ºäº† 2.9%ï¼Œç”šè‡³ä¼˜äºåŸºäº ResNet-101 çš„æ•™å¸ˆæ¨¡å‹ 38.1% çš„ APã€‚<br>
- [MeGA-CDA: Memory Guided Attention for Category-Aware Unsupervised Domain Adaptive Object Detection](https://arxiv.org/abs/2103.04224)<br>
- [Informative and Consistent Correspondence Mining for Cross-Domain Weakly Supervised Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Hou_Informative_and_Consistent_Correspondence_Mining_for_Cross-Domain_Weakly_Supervised_Object_CVPR_2021_paper.pdf)<br>:open_mouth:oral
* [You Only Look One-level Feature](https://arxiv.org/abs/2103.09460)<br>:star:[code](https://github.com/megvii-model/YOLOF)<br>[å¼€æº YOLOFï¼Œæ— éœ€ FPNï¼Œé€Ÿåº¦æ¯” YOLOv4 å¿«13%](https://zhuanlan.zhihu.com/p/357986047)<br>è§£è¯»ï¼š[ç›®æ ‡æ£€æµ‹ç®—æ³•YOLOFï¼šYou Only Look One-level Feature](https://mp.weixin.qq.com/s/0ChHOljrqrXk8yWi_S6ITg)
- [Sparse R-CNN: End-to-End Object Detection with Learnable Proposals](https://arxiv.org/abs/2011.12450)<br>:star:[code](https://github.com/PeizeSun/SparseR-CNN)
- [End-to-End Object Detection with Fully Convolutional Network](https://arxiv.org/abs/2012.03544)<br>:star:[code](https://github.com/Megvii-BaseDetection/DeFCN)<br>è§£è¯»ï¼š[ä¸¢å¼ƒTransformerï¼ŒFCNä¹Ÿå¯ä»¥å®ç°E2Eæ£€æµ‹](https://zhuanlan.zhihu.com/p/332281368)
- [Robust and Accurate Object Detection via Adversarial Learning](https://arxiv.org/abs/2103.13886)
* [Distilling Object Detectors via Decoupled Features](https://arxiv.org/abs/2103.14475)<br>:star:[code](https://github.com/ggjy/DeFeat.pytorch)
* [OTA: Optimal Transport Assignment for Object Detection](https://arxiv.org/abs/2103.14259)<br>:star:[code](https://github.com/Megvii-BaseDetection/OTA)
* [Scale-aware Automatic Augmentation for Object Detection](https://arxiv.org/abs/2103.17220)<br>:star:[code](https://github.com/Jia-Research-Lab/SA-AutoAug) 
* [A Closer Look at Fourier Spectrum Discrepancies for CNN-generated Images Detection](https://arxiv.org/abs/2103.17195)<br>:open_mouth:oral:house:[project](https://keshik6.github.io/Fourier-Discrepancies-CNN-Detection/)
* [IQDet: Instance-wise Quality Distribution Sampling for Object Detection](https://arxiv.org/abs/2104.06936)<br>ç²—è§£ï¼š[20](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* [Domain-Specific Suppression for Adaptive Object Detection](https://arxiv.org/abs/2105.03570)
* [PSRR-MaxpoolNMS: Pyramid Shifted MaxpoolNMS with Relationship Recovery](https://arxiv.org/abs/2105.12990)
* [Joint-DetNAS: Upgrade Your Detector with NAS, Pruning and Dynamic Distillation](https://arxiv.org/abs/2105.12971)
* [Dynamic Head: Unifying Object Detection Heads with Attentions](https://arxiv.org/abs/2106.08322)<br>:star:[code](https://github.com/microsoft/DynamicHead):tv:[video](https://user-images.githubusercontent.com/1438231/122347136-9282e900-cefe-11eb-8b36-ebe08736ec97.mp4)
* [Open-Vocabulary Object Detection Using Captions](https://arxiv.org/abs/2011.10678)<br>:open_mouth:oral:star:[code](https://github.com/alirezazareian/ovr-cnn) 
* [MobileDets: Searching for Object Detection Architectures for Mobile Accelerators](https://arxiv.org/abs/2004.14525)<br>:star:[code](https://github.com/tensorflow/models/tree/master/research/object_detection)
* [Layer-Wise Searching for 1-Bit Detectors](https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Layer-Wise_Searching_for_1-Bit_Detectors_CVPR_2021_paper.pdf)
* [OPANAS: One-Shot Path Aggregation Network Architecture Search for Object Detection](https://arxiv.org/abs/2103.04507)<br>:star:[code](https://github.com/VDIGPKU/OPANAS)
* [GAIA: A Transfer Learning System of Object Detection That Fits Your Needs](https://openaccess.thecvf.com/content/CVPR2021/papers/Bu_GAIA_A_Transfer_Learning_System_of_Object_Detection_That_Fits_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/GAIA-vision)
* [DetectoRS: Detecting Objects With Recursive Feature Pyramid and Switchable Atrous Convolution](https://arxiv.org/abs/2006.02334)<br>:star:[code](https://github.com/joe-siyuan-qiao/DetectoRS)
* [RankDetNet: Delving Into Ranking Constraints for Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_RankDetNet_Delving_Into_Ranking_Constraints_for_Object_Detection_CVPR_2021_paper.pdf)
* [AQD: Towards Accurate Quantized Object Detection](https://arxiv.org/abs/2007.06919)<br>:open_mouth:oral:star:[code](https://github.com/aim-uofa/model-quantization)
* [Class-Aware Robust Adversarial Training for Object Detection](https://arxiv.org/abs/2103.16148)
* [Scaled-YOLOv4: Scaling Cross Stage Partial Network](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Scaled-YOLOv4_Scaling_Cross_Stage_Partial_Network_CVPR_2021_paper.pdf)
* [Improved Handling of Motion Blur in Online Object Detection](https://arxiv.org/abs/2011.14448)<br>:house:[project](http://visual.cs.ucl.ac.uk/pubs/handlingMotionBlur/)
* [The Translucent Patch: A Physical and Universal Attack on Object Detectors](https://arxiv.org/abs/2012.12528)<br>:tv:[video](https://www.youtube.com/watch?v=n6P55eslyvA)
* [Unbiased Mean Teacher for Cross-Domain Object Detection](https://arxiv.org/abs/2003.00707)<br>:star:[code](https://github.com/kinredon/umt)
* [Interpolation-Based Semi-Supervised Learning for Object Detection](https://arxiv.org/abs/2006.02158)<br>:star:[code](https://github.com/soo89/ISD-SSD)
* [Neural Auto-Exposure for High-Dynamic Range Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Onzon_Neural_Auto-Exposure_for_High-Dynamic_Range_Object_Detection_CVPR_2021_paper.pdf)
* [Instant-Teaching: An End-to-End Semi-Supervised Object Detection Framework](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Instant-Teaching_An_End-to-End_Semi-Supervised_Object_Detection_Framework_CVPR_2021_paper.pdf)
* [Black-Box Explanation of Object Detectors via Saliency Maps](https://arxiv.org/abs/2006.03204)<br>:open_mouth:oral:house:[project](https://cs-people.bu.edu/vpetsiuk/drise/):tv:[video](https://youtu.be/AW063Nju9F4)
* å°æ ·æœ¬ç›®æ ‡æ£€æµ‹
  * [Semantic Relation Reasoning for Shot-Stable Few-Shot Object Detection](https://arxiv.org/abs/2103.01903)<br>é¦–ä¸ªç ”ç©¶å°‘æ ·æœ¬æ£€æµ‹ä»»åŠ¡çš„è¯­ä¹‰å…³ç³»æ¨ç†ï¼Œå¹¶è¯æ˜å®ƒå¯æå‡å¼ºåŸºçº¿çš„æ½œã€‚ <br> 
  * [Dense Relation Distillation with Context-aware Aggregation for Few-Shot Object Detection](https://arxiv.org/abs/2103.17115)<br>:star:[code](https://github.com/hzhupku/DCNet)<br>åŒ—äº¬å¤§å­¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æœºå™¨å­¦ä¹ ç ”ç©¶ä¸­å¿ƒ<br>
  * [FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding](https://arxiv.org/abs/2103.05950)<br>:star:[code](https://github.com/MegviiDetection/FSCE)
  * [Generalized Few-Shot Object Detection without Forgetting](https://arxiv.org/abs/2105.09491)<br>ç²—è§£ï¼š[16](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
  * [Accurate Few-Shot Object Detection With Support-Query Mutual Guidance and Hybrid Loss](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Accurate_Few-Shot_Object_Detection_With_Support-Query_Mutual_Guidance_and_Hybrid_CVPR_2021_paper.pdf)
  * [Hallucination Improves Few-Shot Object Detection](https://arxiv.org/abs/2105.01294)<br>:star:[code](https://github.com/pppplin/HallucFsDet)
  * [Few-Shot Object Detection via Classification Refinement and Distractor Retreatment](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Few-Shot_Object_Detection_via_Classification_Refinement_and_Distractor_Retreatment_CVPR_2021_paper.pdf)
  * [Transformation Invariant Few-Shot Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Transformation_Invariant_Few-Shot_Object_Detection_CVPR_2021_paper.pdf)
  * [Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Beyond_Max-Margin_Class_Margin_Equilibrium_for_Few-Shot_Object_Detection_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/Bohao-Lee/CME)
* å¤šç›®æ ‡æ£€æµ‹
  * [There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge](https://arxiv.org/abs/2103.01353)<br>:house:[project](http://multimodal-distill.cs.uni-freiburg.de/)<br>
* 3Dç›®æ ‡æ£€æµ‹
  * [Categorical Depth Distribution Network for Monocular 3D Object Detection](https://arxiv.org/abs/2103.01100)<br>:open_mouth:oral:star:[code](https://github.com/TRAILab/CaDDN)
  * [3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection](https://arxiv.org/abs/2012.04355)<br>:star:[code](https://github.com/thu17cyz/3DIoUMatch):house:[project](https://thu17cyz.github.io/3DIoUMatch/):tv:[video](https://youtu.be/nuARjhkQN2U)<br>æ›´å¤šï¼š[CVPR 2021|åˆ©ç”¨IoUé¢„æµ‹è¿›è¡ŒåŠç›‘ç£å¼3Dç›®æ ‡æ£€æµ‹](https://zhuanlan.zhihu.com/p/354618636)
  * [Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection](https://arxiv.org/abs/2103.16470)<br>:star:[code](https://github.com/fudan-zvg/DDMP)
  * [M3DSSD: Monocular 3D Single Stage Object Detector](https://arxiv.org/abs/2103.13164)<br>:star:[code](https://github.com/mumianyuxin/M3DSSD)
  * [GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection](https://arxiv.org/abs/2103.17202)<br>:star:[code](https://github.com/abhi1kumar/groomed_nms):tv:[video](https://www.youtube.com/watch?v=PWctKkyWrno)<br>æå‡ºå¹¶é›†æˆ GrooMeD-NMSï¼Œç”¨äºå•ç›®3Dç›®æ ‡æ£€æµ‹ã€‚è§£å†³äº†è®­ç»ƒå’Œæ¨ç†ç®¡é“ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œåœ¨ KITTI åŸºå‡†æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„å•ç›®3Dç›®æ ‡æ£€æµ‹ç»“æœï¼Œè¡¨ç°ä¸åŸºäºå•ç›®è§†é¢‘çš„æ–¹æ³•ç›¸å½“ã€‚
  * [LiDAR R-CNN: An Efficient and Universal 3D Object Detector](https://arxiv.org/abs/2103.15297)<br>:star:[code](https://github.com/tusimple/LiDAR_RCNN)
  * [Delving into Localization Errors for Monocular 3D Object Detection](https://arxiv.org/abs/2103.16237)<br>:star:[code](https://github.com/xinzhuma/monodle)
  * [HVPR: Hybrid Voxel-Point Representation for Single-stage 3D Object Detection](https://arxiv.org/abs/2104.00902)<br>:house:[project](https://cvlab.yonsei.ac.kr/projects/HVPR/)
  * [Objects are Different: Flexible Monocular 3D Object Detection](https://arxiv.org/abs/2104.02323)<br>:star:[code](https://github.com/zhangyp15/MonoFlex)
  * [Back-tracing Representative Points for Voting-based 3D Object Detection in Point Clouds](https://arxiv.org/abs/2104.06114)<br>:star:[code](https://github.com/cheng052/BRNet)
  * [PointAugmenting: Cross-Modal Augmentation for 3D Object Detection](https://vision.sjtu.edu.cn/files/cvpr21_pointaugmenting.pdf)<br>åˆ†äº«ä¼š
  * [SE-SSD: Self-Ensembling Single-Stage Object Detector From Point Cloud](https://arxiv.org/abs/2104.09804)<br>:star:[code](https://github.com/Vegeta2020/SE-SSD)<br>æå‡º Self-Ensembling Single-Stage object Detectorï¼ˆSE-SSDï¼‰ï¼Œç”¨äºåœ¨å®¤å¤–ç‚¹äº‘ä¸­è¿›è¡Œå‡†ç¡®å’Œæœ‰æ•ˆçš„ 3D ç›®æ ‡æ£€æµ‹ã€‚å…³é”®åœ¨äºåˆ©ç”¨ soft å’Œ hard targets ä¸æ‰€åˆ¶å®šçš„çº¦æŸæ¡ä»¶æ¥å…±åŒä¼˜åŒ–æ¨¡å‹ï¼Œè€Œä¸åœ¨æ¨ç†ä¸­å¼•å…¥é¢å¤–çš„è®¡ç®—ã€‚ä¸ä¹‹å‰çš„æ‰€æœ‰ä½œå“ç›¸æ¯”ï¼ŒSE-SSD è¾¾åˆ°äº†é¡¶çº§æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ KITTI åŸºå‡†ä¸­çš„æ±½è½¦æ£€æµ‹ä¸­è·å¾—äº†æœ€é«˜çš„ç²¾åº¦ï¼ˆåˆ†åˆ«åœ¨ BEV å’Œ 3D æ’è¡Œæ¦œä¸Šæ’åç¬¬ä¸€å’Œç¬¬äºŒï¼‰ï¼Œå¹¶å…·æœ‰è¶…é«˜çš„æ¨ç†é€Ÿåº¦ã€‚
  * [Offboard 3D Object Detection From Point Cloud Sequences](https://arxiv.org/abs/2103.05073)
  * [Monocular 3D Object Detection: An Extrinsic Parameter Free Approach](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Monocular_3D_Object_Detection_An_Extrinsic_Parameter_Free_Approach_CVPR_2021_paper.pdf)
  * [SRDAN: Scale-Aware and Range-Aware Domain Adaptation Network for Cross-Dataset 3D Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_SRDAN_Scale-Aware_and_Range-Aware_Domain_Adaptation_Network_for_Cross-Dataset_3D_CVPR_2021_paper.pdf)
  * [PVGNet: A Bottom-Up One-Stage 3D Object Detector With Integrated Multi-Level Features](https://openaccess.thecvf.com/content/CVPR2021/papers/Miao_PVGNet_A_Bottom-Up_One-Stage_3D_Object_Detector_With_Integrated_Multi-Level_CVPR_2021_paper.pdf)
  * [MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation](https://arxiv.org/abs/2103.12605)<br>:star:[code](https://github.com/tjiiv-cprg/MonoRUn)
  * [LiDAR-Aug: A General Rendering-Based Augmentation Framework for 3D Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Fang_LiDAR-Aug_A_General_Rendering-Based_Augmentation_Framework_for_3D_Object_Detection_CVPR_2021_paper.pdf)
  * [ST3D: Self-Training for Unsupervised Domain Adaptation on 3D Object Detection](http://arxiv.org/abs/2103.05346)<br>:star:[code](https://github.com/CVMI-Lab/ST3D)
  * [RangeIoUDet: Range Image Based Real-Time 3D Object Detector Optimized by Intersection Over Union](https://openaccess.thecvf.com/content/CVPR2021/papers/Liang_RangeIoUDet_Range_Image_Based_Real-Time_3D_Object_Detector_Optimized_by_CVPR_2021_paper.pdf)
  * [Center-Based 3D Object Detection and Tracking](https://arxiv.org/abs/2006.11275)<br>:star:[code](https://github.com/tianweiy/CenterPoint)
  * [3D Object Detection with Pointformer](https://arxiv.org/abs/2012.11409)<br>:star:[code](https://github.com/Vladimir2506/Pointformer)
  * [To the Point: Efficient 3D Object Detection in the Range Image With Graph Convolution Kernels](https://openaccess.thecvf.com/content/CVPR2021/papers/Chai_To_the_Point_Efficient_3D_Object_Detection_in_the_Range_CVPR_2021_paper.pdf)
  * [RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_RSN_Range_Sparse_Net_for_Efficient_Accurate_LiDAR_3D_Object_CVPR_2021_paper.pdf)
  * [3D-MAN: 3D Multi-Frame Attention Network for Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_3D-MAN_3D_Multi-Frame_Attention_Network_for_Object_Detection_CVPR_2021_paper.pdf)
* æ—‹è½¬ç›®æ ‡æ£€æµ‹
  * [Dense Label Encoding for Boundary Discontinuity Free Rotation Detection](https://arxiv.org/abs/2011.09670)<br>:star:[code](https://github.com/Thinklab-SJTU/DCL_RetinaNet_Tensorflow)
* å¼±ç›‘ç£ç›®æ ‡å®šä½
  * [Shallow Feature Matters for Weakly Supervised Object Localization](https://openaccess.thecvf.com/content/CVPR2021/papers/Wei_Shallow_Feature_Matters_for_Weakly_Supervised_Object_Localization_CVPR_2021_paper.pdf)
  * [Unveiling the Potential of Structure Preserving for Weakly Supervised Object Localization](https://arxiv.org/abs/2103.04523)<br>:star:[code](https://github.com/Panxjia/SPA_CVPR2021)<br>åŸºäºç»“æ„ä¿¡æ¯ä¿æŒçš„å¼±ç›‘ç£ç›®æ ‡å®šä½<br>è§£è¯»ï¼š[13](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [Strengthen Learning Tolerance for Weakly Supervised Object Localization](https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_Strengthen_Learning_Tolerance_for_Weakly_Supervised_Object_Localization_CVPR_2021_paper.pdf)<br>:house:[project](https://nwpu-brainlab.gitee.io/index_en)
* å¯†é›†ç›®æ ‡æ£€æµ‹
  * [Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection](https://arxiv.org/abs/2011.12885)<br>:star:[code](https://github.com/implus/GFocalV2)<br>è§£è¯»ï¼š[ç›®æ ‡æ£€æµ‹æ— ç—›æ¶¨ç‚¹ä¹‹ Generalized Focal Loss V2](https://mp.weixin.qq.com/s/H3LuCuqKCUNFldzqiPWQXg)
  * [VarifocalNet: An IoU-Aware Dense Object Detector](https://arxiv.org/abs/2008.13367)<br>:open_mouth:oral:star:[code](https://github.com/hyz-xmaster/VarifocalNet)
  * [Beyond Bounding-Box: Convex-Hull Feature Adaptation for Oriented and Densely Packed Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_Beyond_Bounding-Box_Convex-Hull_Feature_Adaptation_for_Oriented_and_Densely_Packed_CVPR_2021_paper.pdf)
* æ˜¾è‘—ç›®æ ‡æ£€æµ‹
  * [Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion](https://arxiv.org/abs/2103.11832)<br>:open_mouth:oral
  * [Weakly Supervised Video Salient Object Detection](https://arxiv.org/abs/2104.02391)<br>:star:[code](https://github.com/wangbo-zhao/WSVSOD)
  * [Uncertainty-aware Joint Salient Object and Camouflaged Object Detection](https://arxiv.org/abs/2104.02628)<br>:star:[code](https://github.com/JingZhang617/Joint_COD_SOD)
  * [Calibrated RGB-D Salient Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Calibrated_RGB-D_Salient_Object_Detection_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/jiwei0921/DCF)
  * [From Semantic Categories to Fixations: A Novel Weakly-Supervised Visual-Auditory Saliency Detection Approach](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_From_Semantic_Categories_to_Fixations_A_Novel_Weakly-Supervised_Visual-Auditory_Saliency_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/guotaowang/STANet)
  * co-saliency detection(ååŒæ˜¾è‘—ç›®æ ‡æ£€æµ‹)
    * [DeepACG: Co-Saliency Detection via Semantic-Aware Contrast Gromov-Wasserstein Distance](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_DeepACG_Co-Saliency_Detection_via_Semantic-Aware_Contrast_Gromov-Wasserstein_Distance_CVPR_2021_paper.pdf)
    * [Group Collaborative Learning for Co-Salient Object Detection](https://arxiv.org/abs/2104.01108)<br>:star:[code](https://github.com/fanq15/GCoNet)
* åŠç›‘ç£ç›®æ ‡æ£€æµ‹
  * [Data-Uncertainty Guided Multi-Phase Learning for Semi-Supervised Object Detection](https://arxiv.org/abs/2103.16368)
  * [Points As Queries: Weakly Semi-Supervised Object Detection by Points](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Points_As_Queries_Weakly_Semi-Supervised_Object_Detection_by_Points_CVPR_2021_paper.pdf)<br>ç²—è§£ï¼š[6](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
  * [Interactive Self-Training With Mean Teachers for Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Interactive_Self-Training_With_Mean_Teachers_for_Semi-Supervised_Object_Detection_CVPR_2021_paper.pdf)
  * [Humble Teachers Teach Better Students for Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_Humble_Teachers_Teach_Better_Students_for_Semi-Supervised_Object_Detection_CVPR_2021_paper.pdf)
* é•¿å°¾ç›®æ ‡æ£€æµ‹ 
  * [Adaptive Class Suppression Loss for Long-Tail Object Detection](https://arxiv.org/abs/2104.00885)<br>:star:[code](https://github.com/CASIA-IVA-Lab/ACSL)
  * [Equalization Loss v2: A New Gradient Balance Approach for Long-Tailed Object Detection](https://arxiv.org/abs/2012.08548)<br>:star:[code](https://github.com/tztztztztz/eqlv2)
* å•é˜¶ç›®æ ‡æ£€æµ‹
  * [I^3Net: Implicit Instance-Invariant Network for Adapting One-Stage Object Detectors](https://arxiv.org/abs/2103.13757)
* é˜´å½±æ£€æµ‹
  * [Triple-Cooperative Video Shadow Detection](https://arxiv.org/abs/2103.06533)<br>:star:[code](https://github.com/eraserNut/ViSha)
  * [Single-Stage Instance Shadow Detection with Bidirectional Relation Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Single-Stage_Instance_Shadow_Detection_With_Bidirectional_Relation_Learning_CVPR_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/stevewongv/SSIS)
* æ— ç›‘ç£ç›®æ ‡æ£€æµ‹
  * [Unsupervised Object Detection With LIDAR Clues](https://openaccess.thecvf.com/content/CVPR2021/papers/Tian_Unsupervised_Object_Detection_With_LIDAR_Clues_CVPR_2021_paper.pdf)
* åŸŸé€‚åº”ç›®æ ‡æ£€æµ‹
  * [RPN Prototype Alignment for Domain Adaptive Object Detector](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_RPN_Prototype_Alignment_for_Domain_Adaptive_Object_Detector_CVPR_2021_paper.pdf)
* glass surface detection
  * [Rich Context Aggregation With Reflection Prior for Glass Surface Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Lin_Rich_Context_Aggregation_With_Reflection_Prior_for_Glass_Surface_Detection_CVPR_2021_paper.pdf)
* ä¼ªè£…ç‰©ä½“æ£€æµ‹
  * [Mutual Graph Learning for Camouflaged Object Detection](http://arxiv.org/abs/2104.02613)<br>:star:[code](https://github.com/fanyang587/MGL)
* Any-Shotç›®æ ‡æ£€æµ‹ 
  * [UniT: Unified Knowledge Transfer for Any-Shot Object Detection and Segmentation](https://arxiv.org/abs/2006.07502)<br>:star:[code](https://github.com/ubc-vision/UniT)


<a name="6"/> 

## 6.Data Augmentation(æ•°æ®å¢å¹¿)

- [KeepAugment: A Simple Information-Preserving Data Augmentation](https://arxiv.org/pdf/2011.11778.pdf)<br>
* [SuperMix: Supervising the Mixing Data Augmentation](https://arxiv.org/abs/2003.05034)<br>:star:[code](https://github.com/alldbi/SuperMix)
* [On Feature Normalization and Data Augmentation](https://arxiv.org/abs/2002.11102)<br>:star:[code](https://github.com/Boyiliee/MoEx)
* [StyleMix: Separating Content and Style for Enhanced Data Augmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_StyleMix_Separating_Content_and_Style_for_Enhanced_Data_Augmentation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/alsdml/StyleMix)

<a name="5"/> 

## 5.Anomaly Detection(å¼‚å¸¸æ£€æµ‹)

- [Multiresolution Knowledge Distillation for Anomaly Detection](https://arxiv.org/abs/2011.11108)<br>:star:[code](https://github.com/Niousha12/Knowledge_Distillation_AD)
- [PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation](https://arxiv.org/abs/2010.05903)<br>:star:[code](github.com/talreiss/PANDA)
- [Glancing at the Patch: Anomaly Localization with Global and Local Feature Comparison](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Glancing_at_the_Patch_Anomaly_Localization_With_Global_and_Local_CVPR_2021_paper.pdf)
* é©¾é©¶åœºæ™¯ä¸‹çš„åƒç´ å¼‚å¸¸æ£€æµ‹
  * [Pixel-Wise Anomaly Detection in Complex Driving Scenes](https://arxiv.org/abs/2103.05445)<br>:star:[code](https://github.com/giandbt/SynBoost)

<a name="4"/> 

## 4.Weakly Supervised/Semi-Supervised/Self-supervised/Unsupervised Learning(è‡ª/åŠ/å¼±ç›‘ç£å­¦ä¹ )

* å¼±ç›‘ç£
  * [Weakly Supervised Learning of Rigid 3D Scene Flow](https://arxiv.org/pdf/2102.08945.pdf)<br>:open_mouth:oral:star:[code](https://github.com/zgojcic/Rigid3DSceneFlow):house:[project](https://3dsceneflow.github.io/)<br>
  * [Relation-aware Instance Refinement for Weakly Supervised Visual Grounding](https://arxiv.org/abs/2103.12989)<br>:star:[code](https://github.com/youngfly11/ReIR-WeaklyGrounding.pytorch)
* åŠç›‘ç£
  * [Adaptive Consistency Regularization for Semi-Supervised Transfer Learning](https://arxiv.org/abs/2103.02193)<br>:star:[code](https://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning)<br>
  * [SSLayout360: Semi-Supervised Indoor Layout Estimation from 360âˆ˜ Panorama](https://arxiv.org/abs/2103.13696)
  * [CReST: A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Wei_CReST_A_Class-Rebalancing_Self-Training_Framework_for_Imbalanced_Semi-Supervised_Learning_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/google-research/crest) 
  * [AlphaMatch: Improving Consistency for Semi-Supervised Learning With Alpha-Divergence](https://arxiv.org/abs/2011.11779)
* è‡ªç›‘ç£
  * [Self-supervised Geometric Perception](https://arxiv.org/abs/2103.03114)<br>:open_mouth:oral:star:[code](https://github.com/theNded/SGP)<br>ä½œè€…ç§° SGP æ˜¯ç¬¬ä¸€ä¸ªåœ¨å‡ ä½•æ„ŸçŸ¥ä¸­è¿›è¡Œç‰¹å¾å­¦ä¹ çš„é€šç”¨æ¡†æ¶ï¼Œä¸éœ€è¦ä»»ä½•æ¥è‡ª ground-truth å‡ ä½•æ ‡ç­¾çš„ç›‘ç£ã€‚SGPä»¥EMæ–¹å¼è¿è¡Œï¼Œå®ƒè¿­ä»£æ‰§è¡Œå‡ ä½•æ¨¡å‹çš„é²æ£’ä¼°è®¡ä»¥ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶åœ¨å™ªå£°ä¼ªæ ‡ç­¾çš„ç›‘ç£ä¸‹è¿›è¡Œç‰¹å¾å­¦ä¹ ã€‚å°† SGP åº”ç”¨äºç›¸æœºå§¿åŠ¿ä¼°è®¡å’Œç‚¹äº‘é…å‡†ï¼Œå¹¶è¯æ˜åœ¨å¤§è§„æ¨¡çœŸå®æ•°æ®é›†ä¸­ï¼ŒSGP çš„æ€§èƒ½ç­‰åŒäºç”šè‡³ä¼˜äºç›‘ç£çš„æƒå¨ã€‚
  * [Vectorization and Rasterization: Self-Supervised Learning for Sketch and Handwriting](https://arxiv.org/abs/2103.13716)<br>:star:[code](https://github.com/AyanKumarBhunia/Self-Supervised-Learning-for-Sketch)
  * [Self-supervised Motion Learning from Static Images](https://arxiv.org/abs/2104.00240)
  * [SOLD2: Self-supervised Occlusion-aware Line Description and Detection](https://arxiv.org/abs/2104.03362)<br>:open_mouth:oral:star:[code](https://github.com/cvg/SOLD2)
  * [All Labels Are Not Created Equal: Enhancing Semi-supervision via Label Grouping and Co-training](https://arxiv.org/abs/2104.05248)<br>:star:[code](https://github.com/islam-nassar/semco)
  * [Global Transport for Fluid Reconstruction with Learned Self-Supervision](https://arxiv.org/abs/2104.06031)<br>:open_mouth:oral:star:[code](https://github.com/tum-pbs/Global-Flow-Transport)
  * [Task Programming: Learning Data Efficient Behavior Representations](https://arxiv.org/abs/2011.13917)<br>:open_mouth:oral:star:[code](https://github.com/neuroethology/TREBA):house:[project](https://sites.google.com/view/task-programming)
  * [Audio-Visual Instance Discrimination with Cross-Modal Agreement](https://arxiv.org/abs/2004.12943)
  * [Safe Local Motion Planning With Self-Supervised Freespace Forecasting](https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Safe_Local_Motion_Planning_With_Self-Supervised_Freespace_Forecasting_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/peiyunh/ff)
  * [Back to Event Basics: Self-Supervised Learning of Image Reconstruction for Event Cameras via Photometric Constancy](https://arxiv.org/abs/2009.08283)<br>:star:[code](https://github.com/tudelft/ssl_e2vid):house:[project](https://mavlab.tudelft.nl/ssl_e2v/)
  * [Exponential Moving Average Normalization for Self-Supervised and Semi-Supervised Learning](https://arxiv.org/abs/2101.08482)
  * [How Well Do Self-Supervised Models Transfer?](https://arxiv.org/abs/2011.13377)<br>:star:[code](https://github.com/linusericsson/ssl-transfer)
  * [The Lottery Tickets Hypothesis for Supervised and Self-Supervised Pre-Training in Computer Vision Models](https://arxiv.org/abs/2012.06908)<br>:star:[code](https://github.com/VITA-Group/CV_LTH_Pre-training)
  * [OBoW: Online Bag-of-Visual-Words Generation for Self-Supervised Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Gidaris_OBoW_Online_Bag-of-Visual-Words_Generation_for_Self-Supervised_Learning_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/valeoai/obow)
  * [SSLayout360: Semi-Supervised Indoor Layout Estimation From 360deg Panorama](https://openaccess.thecvf.com/content/CVPR2021/papers/Tran_SSLayout360_Semi-Supervised_Indoor_Layout_Estimation_From_360deg_Panorama_CVPR_2021_paper.pdf)
  * [Instance Localization for Self-supervised Detection Pretraining](https://arxiv.org/pdf/2102.08318.pdf)<br>:star:[code](https://github.com/limbo0000/InstanceLoc)<br>
  * [CASTing Your Model: Learning to Localize Improves Self-Supervised Representations](https://arxiv.org/abs/2012.04630)<br>:star:[code](https://github.com/salesforce/CAST/)
  * [Self-supervised Motion Learning from Static Images](https://arxiv.org/abs/2104.00240)
  * [SPSG: Self-Supervised Photometric Scene Generation From RGB-D Scans](https://openaccess.thecvf.com/content/CVPR2021/papers/Dai_SPSG_Self-Supervised_Photometric_Scene_Generation_From_RGB-D_Scans_CVPR_2021_paper.pdf)
  * [SelfAugment: Automatic Augmentation Policies for Self-Supervised Learning](https://arxiv.org/abs/2009.07724)
* æ— ç›‘ç£
  * [A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning](https://arxiv.org/abs/2104.14558)<br>:star:[code](https://github.com/facebookresearch/SlowFast)
* [Unsupervised Visual Representation Learning by Tracking Patches in Video](https://arxiv.org/abs/2105.02545)<br>:star:[code](https://github.com/microsoft/CtP)
* [SMURF: Self-Teaching Multi-Frame Unsupervised RAFT with Full-Image Warping](https://arxiv.org/abs/2105.07014)<br>:star:[code](https://github.com/google-research/google-research/tree/master/smurf)
* [PAUL: Procrustean Autoencoder for Unsupervised Lifting](https://arxiv.org/abs/2103.16773)
* [Progressive Stage-Wise Learning for Unsupervised Feature Representation Enhancement](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Progressive_Stage-Wise_Learning_for_Unsupervised_Feature_Representation_Enhancement_CVPR_2021_paper.pdf)
* [VDSM: Unsupervised Video Disentanglement With State-Space Modeling and Deep Mixtures of Experts](https://arxiv.org/abs/2103.07292)
* [Unsupervised Feature Learning by Cross-Level Instance-Group Discrimination](https://arxiv.org/abs/2008.03813)<br>:star:[code](https://github.com/frank-xwang/CLD-UnsupervisedLearning)
* [Recurrent Multi-View Alignment Network for Unsupervised Surface Registration](https://arxiv.org/abs/2011.12104)<br>:star:[code](https://github.com/WanquanF/RMA-Net)
* [Feature-Level Collaboration: Joint Unsupervised Learning of Optical Flow, Stereo Depth and Camera Motion](https://openaccess.thecvf.com/content/CVPR2021/papers/Chi_Feature-Level_Collaboration_Joint_Unsupervised_Learning_of_Optical_Flow_Stereo_Depth_CVPR_2021_paper.pdf)

<a name="3"/> 

## 3.Point Cloud(ç‚¹äº‘)

- [Style-based Point Generator with Adversarial Rendering for Point Cloud Completion](https://arxiv.org/abs/2103.02535)<br>
- [MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan Synchronization](https://arxiv.org/abs/2101.06605)<br>:open_mouth:oral:star:[code](https://github.com/huangjh-pub/multibody-sync)
- [TPCN: Temporal Point Cloud Networks for Motion Forecasting](https://arxiv.org/abs/2103.03067)<br>ç”¨äºè¿åŠ¨é¢„æµ‹çš„æ—¶ç©ºç‚¹äº‘ç½‘ç»œ<br>
- [How Privacy-Preserving are Line Clouds? Recovering Scene Details from 3D Lines](https://arxiv.org/abs/2103.05086)<br>:star:[code](https://github.com/kunalchelani/Line2Point)
- [PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds](https://arxiv.org/abs/2103.14635)<br>:star:[code](https://github.com/CVMI-Lab/PAConv)
- [Point2Skeleton: Learning Skeletal Representations from Point Clouds](https://arxiv.org/abs/2012.00230)<br>:open_mouth:oral:star:[code](https://github.com/clinplayer/Point2Skeleton):house:[project](https://enigma-li.github.io/projects/point2skeleton/point2skeleton.html)
- [FESTA: Flow Estimation via Spatial-Temporal Attention for Scene Point Clouds](https://arxiv.org/abs/2104.00798)
- [RPSRNet: End-to-End Trainable Rigid Point Set Registration Network using Barnes-Hut 2D-Tree Representation](https://arxiv.org/abs/2104.05328) 
- [Point Cloud Upsampling via Disentangled Refinement](https://arxiv.org/abs/2106.04779)<br>:star:[code](https://github.com/liruihui/Dis-PU)
- [Regularization Strategy for Point Cloud via Rigidly Mixed Sample](https://arxiv.org/abs/2102.01929)<br>:star:[code](https://github.com/dogyoonlee/RSMix)
- [Verifiability and Predictability: Interpreting Utilities of Network Architectures for Point Cloud Processing](https://arxiv.org/abs/1911.09053)
* [Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos](https://openaccess.thecvf.com/content/CVPR2021/papers/Fan_Point_4D_Transformer_Networks_for_Spatio-Temporal_Modeling_in_Point_Cloud_CVPR_2021_paper.pdf)
* [PointNetLK Revisited](https://arxiv.org/abs/2008.09527)<br>:open_mouth:oral:star:[code](https://github.com/Lilac-Lee/PointNetLK_Revisited)
* [PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds](https://openaccess.thecvf.com/content/CVPR2021/papers/Wei_PV-RAFT_Point-Voxel_Correlation_Fields_for_Scene_Flow_Estimation_of_Point_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/weiyithu/PV-RAFT)
* ç‚¹äº‘é…å‡†
  * [PREDATOR: Registration of 3D Point Clouds with Low Overlap](https://arxiv.org/pdf/2011.13005.pdf)<br>:open_mouth:oral:star:[code](https://github.com/ShengyuH/OverlapPredator):house:[project](https://overlappredator.github.io/)<br>
  * [SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration](https://arxiv.org/abs/2011.12149)<br>:star:[code](https://github.com/QingyongHu/SpinNet)
  * [Robust Point Cloud Registration Framework Based on Deep Graph Matching](https://arxiv.org/abs/2103.04256)<br>:star:[code](https://github.com/fukexue/RGM)
  * [PointDSC: Robust Point Cloud Registration using Deep Spatial Consistency](https://arxiv.org/abs/2103.05465)<br>:star:[code](https://github.com/XuyangBai/PointDSC)
  * [ReAgent: Point Cloud Registration using Imitation and Reinforcement Learning](https://arxiv.org/abs/2103.15231)<br>:star:[code](https://github.com/dornik/reagent)
  * [DeepI2P: Image-to-Point Cloud Registration via Deep Classification](https://arxiv.org/abs/2104.03501)<br>:star:[code](https://github.com/lijx10/DeepI2P)
  * [StickyPillars: Robust and Efficient Feature Matching on Point Clouds Using Graph Neural Networks](https://openaccess.thecvf.com/content/CVPR2021/papers/Fischer_StickyPillars_Robust_and_Efficient_Feature_Matching_on_Point_Clouds_Using_CVPR_2021_paper.pdf)
  * [UnsupervisedR&R: Unsupervised Point Cloud Registration via Differentiable Rendering](https://openaccess.thecvf.com/content/CVPR2021/papers/Banani_UnsupervisedRR_Unsupervised_Point_Cloud_Registration_via_Differentiable_Rendering_CVPR_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/mbanani/unsupervisedRR)
* ç‚¹äº‘è¡¥å…¨
  * [Cycle4Completion: Unpaired Point Cloud Completion using Cycle Transformation with Missing Region Coding](https://arxiv.org/abs/2103.07838)<br>:star:[code](https://github.com/diviswen/Cycle4Completion)
  * [Denoise and Contrast for Category Agnostic Shape Completion](https://arxiv.org/abs/2103.16671)<br>:star:[code](https://github.com/antoalli/Deco)
  * [Variational Relational Point Completion Network](https://arxiv.org/abs/2104.10154)<br>:open_mouth:oral:star:[code](https://github.com/paul007pl/VRCNet):house:[project](https://paul007pl.github.io/projects/VRCNet.html)
  * [Unsupervised 3D Shape Completion through GAN Inversion](https://arxiv.org/abs/2104.13366)<br>:star:[code](https://github.com/junzhezhang/shape-inversion):house:[project](https://junzhezhang.github.io/projects/ShapeInversion/)
  * [PMP-Net: Point Cloud Completion by Learning Multi-Step Point Moving Paths](https://openaccess.thecvf.com/content/CVPR2021/papers/Wen_PMP-Net_Point_Cloud_Completion_by_Learning_Multi-Step_Point_Moving_Paths_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/diviswen/PMP-Net)
  * [View-Guided Point Cloud Completion](https://arxiv.org/abs/2104.05666)
* ç‚¹äº‘å…³é”®ç‚¹æ£€æµ‹
  * [Skeleton Merger: an Unsupervised Aligned Keypoint Detector](https://arxiv.org/abs/2103.10814)<br>:star:[code](https://github.com/eliphatfs/SkeletonMerger)
* 3Dç‚¹äº‘
  * [Diffusion Probabilistic Models for 3D Point Cloud Generation](https://arxiv.org/abs/2103.01458)<br>:open_mouth:oral:star:[code](https://github.com/luost26/diffusion-point-cloud)<br>
  * [PointGuard: Provably Robust 3D Point Cloud Classification](https://arxiv.org/abs/2103.03046)
  * [Equivariant Point Network for 3D Point Cloud Analysis](https://arxiv.org/abs/2103.14147)<br>:star:[code](https://github.com/nintendops/EPN_PointCloud)
  * [CorrNet3D: Unsupervised End-to-End Learning of Dense Correspondence for 3D Point Clouds](https://arxiv.org/abs/2012.15638)<br>:star:[code](https://github.com/ZENGYIMING-EAMON/CorrNet3D)
  * [Self-Supervised Learning on 3D Point Clouds by Learning Discrete Generative Models](https://openaccess.thecvf.com/content/CVPR2021/papers/Eckart_Self-Supervised_Learning_on_3D_Point_Clouds_by_Learning_Discrete_Generative_CVPR_2021_paper.pdf)
  * [PWCLO-Net: Deep LiDAR Odometry in 3D Point Clouds Using Hierarchical Embedding Mask Optimization](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_PWCLO-Net_Deep_LiDAR_Odometry_in_3D_Point_Clouds_Using_Hierarchical_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/IRMVLab/PWCLONet)
  * 3Dç‚¹äº‘ç”Ÿæˆ
    * [Learning Progressive Point Embeddings for 3D Point Cloud Generation](https://openaccess.thecvf.com/content/CVPR2021/papers/Wen_Learning_Progressive_Point_Embeddings_for_3D_Point_Cloud_Generation_CVPR_2021_paper.pdf)
* ç‚¹äº‘å‹ç¼©
  * [VoxelContext-Net: An Octree based Framework for Point Cloud Compression](https://arxiv.org/abs/2105.02158)
* ç‚¹äº‘è¯†åˆ«
  * [3D Spatial Recognition without Spatially Labeled 3D](https://arxiv.org/abs/2105.06461)<br>:house:[project](https://facebookresearch.github.io/WyPR/)
* ç‚¹äº‘åˆ†å‰²
  * [SCF-Net: Learning Spatial Contextual Features for Large-Scale Point Cloud Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Fan_SCF-Net_Learning_Spatial_Contextual_Features_for_Large-Scale_Point_Cloud_Segmentation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/leofansq/SCF-Net)

<a name="2"/> 

## 2.Graph Neural Networks(å›¾å·ç§¯ç½‘ç»œGNNã€GCNã€GMN)

- [Sequential Graph Convolutional Network for Active Learning](https://arxiv.org/pdf/2006.10219.pdf)<br>
- [Quantifying Explainers of Graph Neural Networks in Computational Pathology](https://arxiv.org/abs/2011.12646)<br>:star:[code](https://github.com/histocartography/patho-quant-explainer)
- [Binary Graph Neural Networks](https://arxiv.org/abs/2012.15823)<br>:star:[code](https://github.com/mbahri/binary_gnn)
- [Amalgamating Knowledge from Heterogeneous Graph Neural Networks](https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Amalgamating_Knowledge_From_Heterogeneous_Graph_Neural_Networks_CVPR_2021_paper.pdf)
* GCN
  * [SelfSAGCN: Self-Supervised Semantic Alignment for Graph Convolution Network](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_SelfSAGCN_Self-Supervised_Semantic_Alignment_for_Graph_Convolution_Network_CVPR_2021_paper.pdf)
  * [Bi-GCN: Binary Graph Convolutional Network](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Bi-GCN_Binary_Graph_Convolutional_Network_CVPR_2021_paper.pdf)
  * [PU-GCN: Point Cloud Upsampling Using Graph Convolutional Networks](https://openaccess.thecvf.com/content/CVPR2021/papers/Qian_PU-GCN_Point_Cloud_Upsampling_Using_Graph_Convolutional_Networks_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/guochengqian/PU-GCN)
  * [A Hyperbolic-to-Hyperbolic Graph Convolutional Network](https://arxiv.org/abs/2104.06942)<br>:open_mouth:oral
  * [TSGCNet: Discriminative Geometric Feature Learning With Two-Stream Graph Convolutional Network for 3D Dental Model Segmentation](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_TSGCNet_Discriminative_Geometric_Feature_Learning_With_Two-Stream_Graph_Convolutional_Network_CVPR_2021_paper.pdf)
  * [Hierarchical Layout-Aware Graph Convolutional Network for Unified Aesthetics Assessment](https://openaccess.thecvf.com/content/CVPR2021/papers/She_Hierarchical_Layout-Aware_Graph_Convolutional_Network_for_Unified_Aesthetics_Assessment_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/days1011/HLAGCN)
* Graph Matching Networks(GMN)
  * [LayoutGMN: Neural Graph Matching for Structural Layout Similarity](http://arxiv.org/abs/2012.06547)
  * [Scene Essence](https://openaccess.thecvf.com/content/CVPR2021/papers/Qiu_Scene_Essence_CVPR_2021_paper.pdf)



<a name="1"/> 

## 1.Unkown(æœªåˆ†ç±»)



- [Reconsidering Representation Alignment for Multi-view Clustering](https://arxiv.org/abs/2103.07738)<br>:star:[code](https://github.com/DanielTrosten/mvc)
- [Self-supervised Simultaneous Multi-Step Prediction of Road Dynamics and Cost Map](https://arxiv.org/abs/2103.01039)<br>
- [Neural Geometric Level of Detail:Real-time Rendering with Implicit 3D Surfaces](https://arxiv.org/abs/2101.10994)<br>:open_mouth:Oral:star:[code](https://github.com/nv-tlabs/nglod):house:[project](https://nv-tlabs.github.io/nglod/)<br>
- [Data-Free Model Extraction](https://arxiv.org/abs/2011.14779)<br>:star:[code](https://github.com/cake-lab/datafree-model-extraction)<br>
- [Continual Adaptation of Visual Representations via Domain Randomization and Meta-learning](https://arxiv.org/abs/2012.04324)<br>:open_mouth:oral
- [PatchmatchNet: Learned Multi-View Patchmatch Stereo](https://arxiv.org/abs/2012.01411)<br>:open_mouth:oral:star:[code](https://github.com/FangjinhuaWang/PatchmatchNet)
- [Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning](https://arxiv.org/abs/2012.11552)<br>:star:[code](https://github.com/valeoai/obow):house:[project](https://valeoai.github.io/blog/publications/obow/)<br>
- [Semantic Palette: Guiding Scene Generation with Class Proportions](https://arxiv.org/abs/2106.01629)

- [Multi-Objective Interpolation Training for Robustness to Label Noise](https://arxiv.org/abs/2012.04462)<br>:star:[code](https://github.com/DiegoOrtego/LabelNoiseMOIT)
- [Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations](https://arxiv.org/abs/2011.12854)<br>:star:[code](https://github.com/ml-research/NeSyXIL)
- [Simpler Certified Radius Maximization by Propagating Covariances](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhen_Simpler_Certified_Radius_Maximization_by_Propagating_Covariances_CVPR_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/zhenxingjian/Propagating_Covariance):tv:[video](https://www.youtube.com/watch?v=m1ya2oNf5iE)
- [Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food](https://arxiv.org/abs/2103.03375)<br>:star:[code](https://github.com/google-research-datasets/Nutrition5k)
- [Discovering Hidden Physics Behind Transport Dynamics](https://arxiv.org/abs/2011.12222)<br>:open_mouth:oral
- [Soft-IntroVAE: Analyzing and Improving the Introspective Variational Autoencoder](https://arxiv.org/abs/2012.13253)<br>:open_mouth:oral:star:[code](https://github.com/taldatech/soft-intro-vae-pytorch):house:[project](https://taldatech.github.io/soft-intro-vae-web/)
- [Deep Gradient Projection Networks for Pan-sharpening](https://arxiv.org/abs/2103.04584)<br>:star:[code](https://github.com/xsxjtu/GPPNN)
- [Consensus Maximisation Using Influences of Monotone Boolean Functions](https://arxiv.org/abs/2103.04200)<br>:open_mouth:oral:star:[code](https://github.com/RuwanT/MBF-MaxCon)
* [Forecasting Irreversible Disease via Progression Learning](https://arxiv.org/abs/2012.11107)
* [Causal Hidden Markov Model for Time Series Disease Forecasting](https://arxiv.org/abs/2103.16391)<br>:star:[code](https://github.com/LilJing/causal_hmm):house:[project](https://sites.google.com/view/causal-hmm)
- [Knowledge Evolution in Neural Networks](https://arxiv.org/abs/2103.05152)<br>:open_mouth:oral:star:[code](https://github.com/ahmdtaha/knowledge_evolution)
* [RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_RSTNet_Captioning_With_Adaptive_Attention_on_Visual_and_Non-Visual_Words_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/zhangxuying1004/RSTNet)<br>RSTNet: åŸºäºå¯åŒºåˆ†è§†è§‰è¯å’Œéè§†è§‰è¯çš„è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶çš„å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹<br>è§£è¯»ï¼š[14](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* [Removing the Background by Adding the Background: Towards a Background Robust Self-supervised Video Representation Learning](https://arxiv.org/abs/2009.05769)<br>é€šè¿‡æ·»åŠ èƒŒæ™¯æ¥å»é™¤èƒŒæ™¯å½±å“ï¼šèƒŒæ™¯é²æ£’çš„è‡ªç›‘ç£è§†é¢‘è¡¨å¾å­¦ä¹ <br>è§£è¯»ï¼š[11](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* [Representative Batch Normalization with Feature Calibration](https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_Representative_Batch_Normalization_With_Feature_Calibration_CVPR_2021_paper.pdf)<br>:open_mouth:oral:star:[code](https://github.com/ShangHua-Gao/RBN):house:[project](http://mmcheng.net/rbn/)<br>[ä½œè€…ä¸»é¡µ](https://duoli.org/)<br>åŸºäºç‰¹å¾æ ¡å‡†çš„è¡¨å¾æ‰¹è§„èŒƒåŒ–æ–¹æ³•è§£è¯»ï¼š[4](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* [Involution: Inverting the Inherence of Convolution for Visual Recognition](https://arxiv.org/abs/2103.06255)<br>:star:[code](https://github.com/d-li14/involution)<br>è§£è¯»ï¼š[CVPR'21 | involutionï¼šè¶…è¶Šconvolutionå’Œself-attentionçš„ç¥ç»ç½‘ç»œæ–°ç®—å­](https://mp.weixin.qq.com/s/Kn7QJdldLhyBfYS1KiCdcA)
* [Spatially Consistent Representation Learning](https://arxiv.org/abs/2103.06122)<br>:star:[code](https://github.com/kakaobrain/scrl)
* [Limitations of Post-Hoc Feature Alignment for Robustness](https://arxiv.org/abs/2103.05898)<br>:star:[code](https://github.com/collin-burns/feature-alignment)
* [AutoDO: Robust AutoAugment for Biased Data with Label Noise via Scalable Probabilistic Implicit Differentiation](https://arxiv.org/abs/2103.05863)<br>:star:[code](https://github.com/gudovskiy/autodo)
* [Augmentation Strategies for Learning with Noisy Labels](https://arxiv.org/abs/2103.02130)<br>:star:[code](https://github.com/KentoNishi/Augmentation-for-LNL)
* [PGT: A Progressive Method for Training Models on Long Videos](https://arxiv.org/abs/2103.11313)<br>:open_mouth:oral:star:[code](https://github.com/BoPang1996/PGT)
* [Generic Perceptual Loss for Modeling Structured Output Dependencies](https://arxiv.org/abs/2103.10571)
* [Masksembles for Uncertainty Estimation](https://arxiv.org/abs/2012.08334)<br>:star:[code](https://github.com/nikitadurasov/masksembles):house:[project](https://nikitadurasov.github.io/projects/masksembles/)
* [Student-Teacher Learning from Clean Inputs to Noisy Inputs](https://arxiv.org/abs/2103.07600)
* [Scene-Intuitive Agent for Remote Embodied Visual Grounding](https://arxiv.org/abs/2103.12944)
* [Meta-Mining Discriminative Samples for Kinship Verification](https://arxiv.org/abs/2103.15108)<br>
* [Learning Probabilistic Ordinal Embeddings for Uncertainty-Aware Regression](https://arxiv.org/abs/2103.13629)<br>:star:[code](https://github.com/Li-Wanhua/POEs):tv:[video](https://www.youtube.com/watch?v=zCTPRxxlZsI&t=427s)<br>è®ºæ–‡å…¬å¼€
* [Diverse Branch Block: Building a Convolution as an Inception-like Unit](https://arxiv.org/abs/2103.13425)<br>:star:[code](https://github.com/DingXiaoH/DiverseBranchBlock)
* [OTCE: A Transferability Metric for Cross-Domain Cross-Task Representations](https://arxiv.org/abs/2103.13843)
* [Disentangled Cycle Consistency for Highly-realistic Virtual Try-On](https://arxiv.org/abs/2103.09479)<br>:star:[code](https://github.com/ChongjianGE/DCTON)
* [Stylized Neural Painting](https://arxiv.org/abs/2011.08114)<br>:star:[code](https://github.com/jiupinjia/stylized-neural-painting):house:[project](https://jiupinjia.github.io/neuralpainter/):tv:[video](https://youtu.be/oerb-nwrXhk)<br>é£æ ¼åŒ–çš„ç¥ç»ç»˜ç”»,Stylized Neural Painting,æå‡º image-to-painting ç¿»è¯‘æ–¹æ³•ï¼Œç”Ÿæˆç”ŸåŠ¨é€¼çœŸã€é£æ ¼å¯æ§çš„ç»˜ç”»è‰ºæœ¯ä½œå“ 
* [Confluent Vessel Trees with Accurate Bifurcations](https://arxiv.org/abs/2103.14268)<br>:star:[code](https://vision.cs.uwaterloo.ca/code/)
* [Repopulating Street Scenes](https://arxiv.org/abs/2103.16183)
* [Can We Characterize Tasks Without Labels or Features?](https://openaccess.thecvf.com/content/CVPR2021/papers/Wallace_Can_We_Characterize_Tasks_Without_Labels_or_Features_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/BramSW/task_characterization_cvpr_2021/)
* [Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding](https://arxiv.org/abs/2103.16848)
* [Online Learning of a Probabilistic and Adaptive Scene Representation](https://arxiv.org/abs/2103.16832)
* [Generative Modelling of BRDF Textures from Flash Images](https://arxiv.org/abs/2102.11861)<br>:star:[code](https://github.com/henzler/neuralmaterial):house:[project](https://henzler.github.io/publication/neuralmaterial/)
* [PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting](https://arxiv.org/abs/2104.00674)<br>:house:[project](https://kai-46.github.io/PhySG-website/)<br>ä½œè€…å‘æ˜çš„é€†å‘æ¸²æŸ“ç®—æ³•PhySGï¼Œå¯ä»¥ä»ä¸€ç»„RGBè¾“å…¥å›¾åƒä¸­é‡å»ºç‰©ä½“å‡ ä½•å›¾å½¢ã€æè´¨å’Œå…‰ç…§ï¼Œå…¨ç¨‹ç«¯åˆ°ç«¯è¿è¡Œã€‚
* [Self-supervised Video Representation Learning by Context and Motion Decoupling](https://arxiv.org/abs/2104.00862)
* [Dynamic Region-Aware Convolution](https://arxiv.org/abs/2003.12243)<br>ç²—è§£ï¼š[14](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* [Meta Pseudo Labels](https://arxiv.org/pdf/2003.10580.pdf)<br>:star:[code](https://github.com/google-research/google-research/tree/master/meta_pseudo_labels):tv:[video](https://www.youtube.com/watch?v=yhItocvAaq0)
* [PQA: Perceptual Question Answering](https://arxiv.org/abs/2104.03589)
* [CondenseNet V2: Sparse Feature Reactivation for Deep Networks](https://arxiv.org/abs/2104.04382)<br>:star:[code](https://github.com/jianghaojun/CondenseNetV2) 
* [CFNet: Cascade and Fused Cost Volume for Robust Stereo Matching](https://arxiv.org/abs/2104.04314)<br>:star:[code](https://github.com/gallenszl/CFNet)
* [Neural Camera Simulators](https://arxiv.org/abs/2104.05237)<br>:star:[code](https://github.com/ken-ouyang/neural_image_simulator)
* [Simpler Certified Radius Maximization by Propagating Covariances](https://arxiv.org/abs/2104.05888)<br>:open_mouth:oral:star:[code](https://github.com/zhenxingjian/Propagating_Covariance):tv:[video](https://www.youtube.com/watch?v=m1ya2oNf5iE)
* [Lighting, Reflectance and Geometry Estimation from 360âˆ˜ Panoramic Stereo](https://arxiv.org/abs/2104.09886)<br>:star:[code](https://github.com/junxuan-li/LRG_360Panoramic)
* [MetricOpt: Learning to Optimize Black-Box Evaluation Metrics](https://arxiv.org/abs/2104.10631)<br>:open_mouth:oral
* [Deep Stable Learning for Out-Of-Distribution](http://pengcui.thumedialab.com/papers/DeepStableLearning.pdf)<br>åˆ†äº«ä¼š
* [Learning a Self-Expressive Network for Subspace Clustering](http://www.pris.net.cn/wp-content/uploads/2021/04/SENet-CVPR-2021.pdf)<br>åˆ†äº«ä¼š
* [Heterogeneous Grid Convolution for Adaptive, Efficient, and Controllable Computation](https://arxiv.org/abs/2104.11176)
* [Extreme Rotation Estimation using Dense Correlation Volumes](https://arxiv.org/abs/2104.13530)<br>:house:[project](https://ruojincai.github.io/ExtremeRotation/)
* [Decoupled Dynamic Filter Networks](https://arxiv.org/abs/2104.14107)<br>:house:[project](https://thefoxofsky.github.io/project_pages/ddf):tv:[video](https://youtu.be/QecJD5HUF7U)
* [MongeNet: Efficient Sampler for Geometric Deep Learning](https://arxiv.org/abs/2104.14554)<br>:star:[code](https://github.com/lebrat/MongeNet):house:[project](https://lebrat.github.io/MongeNet/):tv:[video](https://youtu.be/RfmZBbSEiz4)
* [Multi-Perspective LSTM for Joint Visual Representation Learning](https://arxiv.org/abs/2105.02802)<br>:star:[code](https://github.com/arsm/MPLSTM)
* [Quantum Permutation Synchronization](https://arxiv.org/pdf/2101.07755.pdf)
* [A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts](https://arxiv.org/abs/2105.00290)
* [DriveGAN: Towards a Controllable High-Quality Neural Simulation](https://arxiv.org/abs/2104.15060)<br>:open_mouth:oral
* [Faster Meta Update Strategy for Noise-Robust Deep Learning](https://arxiv.org/abs/2104.15092)<br>:star:[code](https://github.com/youjiangxu/FaMUS)
* [NeRD: Neural 3D Reflection Symmetry Detector](https://arxiv.org/abs/2105.03211)<br>:star:[code](https://github.com/zhou13/nerd)
* [SSAN: Separable Self-Attention Network for Video Representation Learning](https://arxiv.org/abs/2105.13033)
* [Scene-aware Generative Network for Human Motion Synthesis](https://arxiv.org/abs/2105.14804)
* [Stochastic Whitening Batch Normalization](https://arxiv.org/abs/2106.04413)
* [CLCC: Contrastive Learning for Color Constancy](https://arxiv.org/abs/2106.04989)<br>:star:[code](https://github.com/howardyclo/clcc-cvpr21)
* [Magic Layouts: Structural Prior for Component Detection in User Interface Designs](https://arxiv.org/abs/2106.07615)
* [GIRAFFE: Representing Scenes As Compositional Generative Neural Feature Fields](https://arxiv.org/abs/2011.12100)<br>:open_mouth:oral:star:[code](https://github.com/autonomousvision/giraffe):house:[project](https://m-niemeyer.github.io/project-pages/giraffe/index.html)
* [Polygonal Building Extraction by Frame Field Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Girard_Polygonal_Building_Extraction_by_Frame_Field_Learning_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/Lydorn/Polygonization-by-Frame-Field-Learning)
* [MP3: A Unified Model To Map, Perceive, Predict and Plan](https://arxiv.org/abs/2101.06806)
* [NewtonianVAE: Proportional Control and Goal Identification From Pixels via Physical Latent Spaces](https://arxiv.org/abs/2006.01959)
* [Fast End-to-End Learning on Protein Surfaces](https://www.biorxiv.org/content/10.1101/2020.12.28.424589v1)
* [Flow Guided Transformable Bottleneck Networks for Motion Retargeting](https://arxiv.org/abs/2106.07771)
* [Polka Lines: Learning Structured Illumination and Reconstruction for Active Stereo](https://arxiv.org/abs/2011.13117)
* [Patch2Pix: Epipolar-Guided Pixel-Level Correspondences](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Patch2Pix_Epipolar-Guided_Pixel-Level_Correspondences_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/GrumpyZhou/patch2pix):tv:[video](https://www.youtube.com/watch?v=Qxkyjsgi8xY)
* [Pixel-Aligned Volumetric Avatars](https://openaccess.thecvf.com/content/CVPR2021/papers/Raj_Pixel-Aligned_Volumetric_Avatars_CVPR_2021_paper.pdf)
* [Learnable Motion Coherence for Correspondence Pruning](https://arxiv.org/abs/2011.14563)<br>:star:[code](https://github.com/liuyuan-pal/LMCNet):house:[project](https://liuyuan-pal.github.io/LMCNet/)
* [DualGraph: A Graph-Based Method for Reasoning About Label Noise](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_DualGraph_A_Graph-Based_Method_for_Reasoning_About_Label_Noise_CVPR_2021_paper.pdf)
* [Automatic Correction of Internal Units in Generative Neural Networks](https://arxiv.org/abs/2104.06118)
* [Adaptive Rank Estimate in Robust Principal Component Analysis](https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Adaptive_Rank_Estimate_in_Robust_Principal_Component_Analysis_CVPR_2021_paper.pdf)
* [Cluster-Wise Hierarchical Generative Model for Deep Amortized Clustering](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Cluster-Wise_Hierarchical_Generative_Model_for_Deep_Amortized_Clustering_CVPR_2021_paper.pdf)
* [3D AffordanceNet: A Benchmark for Visual Object Affordance Understanding](https://arxiv.org/abs/2103.16397)
* [Ranking Neural Checkpoints](https://arxiv.org/abs/2011.11200)
* [On Focal Loss for Class-Posterior Probability Estimation: A Theoretical Perspective](https://arxiv.org/abs/2011.09172)
* [Learning Deep Latent Variable Models by Short-Run MCMC Inference With Optimal Transport Correction](https://openaccess.thecvf.com/content/CVPR2021/papers/An_Learning_Deep_Latent_Variable_Models_by_Short-Run_MCMC_Inference_With_CVPR_2021_paper.pdf)
* [Learning the Best Pooling Strategy for Visual Semantic Embedding](https://arxiv.org/abs/2011.04305)<br>:star:[code](https://github.com/woodfrog/vse_infty):house:[project](https://vse-infty.github.io/)
* [Backdoor Attacks Against Deep Learning Systems in the Physical World](https://arxiv.org/abs/2006.14580)
* [Relevance-CAM: Your Model Already Knows Where To Look](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Relevance-CAM_Your_Model_Already_Knows_Where_To_Look_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/mongeoroo/Relevance-CAM)
* [On Robustness and Transferability of Convolutional Neural Networks](https://arxiv.org/abs/2007.08558)
* [Square Root Bundle Adjustment for Large-Scale Reconstruction](https://arxiv.org/abs/2103.01843)<br>:house:[project](https://vision.in.tum.de/research/vslam/rootba):tv:[video](https://youtu.be/kAhmjNL8B-U)
* [Crossing Cuts Polygonal Puzzles: Models and Solvers](https://openaccess.thecvf.com/content/CVPR2021/papers/Harel_Crossing_Cuts_Polygonal_Puzzles_Models_and_Solvers_CVPR_2021_paper.pdf)
* [Sparse Multi-Path Corrections in Fringe Projection Profilometry](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Sparse_Multi-Path_Corrections_in_Fringe_Projection_Profilometry_CVPR_2021_paper.pdf)
* [Understanding the Behaviour of Contrastive Loss](https://arxiv.org/abs/2012.09740)
* [Dual Contradistinctive Generative Autoencoder](https://arxiv.org/abs/2011.10063)<br>:star:[code](https://github.com/mlpc-ucsd/DC-VAE)
* [Metadata Normalization](https://arxiv.org/abs/2104.09052)<br>:star:[code](https://github.com/mlu355/MetadataNorm)
* [End-to-End Rotation Averaging With Multi-Source Propagation](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_End-to-End_Rotation_Averaging_With_Multi-Source_Propagation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/sfu-gruvi-3dv/msp_rot_avg)
* [UV-Net: Learning From Boundary Representations](https://openaccess.thecvf.com/content/CVPR2021/papers/Jayaraman_UV-Net_Learning_From_Boundary_Representations_CVPR_2021_paper.pdf)
* [Mixed-Privacy Forgetting in Deep Networks](http://arxiv.org/abs/2012.13431)
* [Double Low-Rank Representation With Projection Distance Penalty for Clustering](https://openaccess.thecvf.com/content/CVPR2021/papers/Fu_Double_Low-Rank_Representation_With_Projection_Distance_Penalty_for_Clustering_CVPR_2021_paper.pdf)
* [Lighting, Reflectance and Geometry Estimation From 360deg Panoramic Stereo](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Lighting_Reflectance_and_Geometry_Estimation_From_360deg_Panoramic_Stereo_CVPR_2021_paper.pdf)
* [Building Reliable Explanations of Unreliable Neural Networks: Locally Smoothing Perspective of Model Interpretation](https://arxiv.org/abs/2103.14332)
* [DAT: Training Deep Networks Robust To Label-Noise by Matching the Feature Distributions](https://openaccess.thecvf.com/content/CVPR2021/papers/Qu_DAT_Training_Deep_Networks_Robust_To_Label-Noise_by_Matching_the_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/Tyqnn0323/DAT)
* [End-to-End High Dynamic Range Camera Pipeline Optimization](https://openaccess.thecvf.com/content/CVPR2021/papers/Robidoux_End-to-End_High_Dynamic_Range_Camera_Pipeline_Optimization_CVPR_2021_paper.pdf)
* [Dual-GAN: Joint BVP and Noise Modeling for Remote Physiological Measurement](https://openaccess.thecvf.com/content/CVPR2021/papers/Lu_Dual-GAN_Joint_BVP_and_Noise_Modeling_for_Remote_Physiological_Measurement_CVPR_2021_paper.pdf)
* [User-Guided Line Art Flat Filling With Split Filling Mechanism](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_User-Guided_Line_Art_Flat_Filling_With_Split_Filling_Mechanism_CVPR_2021_paper.pdf)
* [KSM: Fast Multiple Task Adaption via Kernel-Wise Soft Mask Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_KSM_Fast_Multiple_Task_Adaption_via_Kernel-Wise_Soft_Mask_Learning_CVPR_2021_paper.pdf)
* [Coming Down to Earth: Satellite-to-Street View Synthesis for Geo-Localization](https://openaccess.thecvf.com/content/CVPR2021/papers/Toker_Coming_Down_to_Earth_Satellite-to-Street_View_Synthesis_for_Geo-Localization_CVPR_2021_paper.pdf)
* [Room-and-Object Aware Knowledge Reasoning for Remote Embodied Referring Expression](https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_Room-and-Object_Aware_Knowledge_Reasoning_for_Remote_Embodied_Referring_Expression_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/alloldman/CKR)
* [Group Whitening: Balancing Learning Efficiency and Representational Capacity](https://arxiv.org/abs/2009.13333)<br>:star:[code](https://github.com/huangleiBuaa/GroupWhitening)
* [Privacy-Preserving Collaborative Learning With Automatic Transformation Search](https://arxiv.org/abs/2011.12505)<br>:open_mouth:oral
* [Post-Hoc Uncertainty Calibration for Domain Drift Scenarios](https://arxiv.org/abs/2012.10988)<br>:star:[code](https://github.com/tochris/calibration-domain-drift)
* [Efficient Initial Pose-Graph Generation for Global SfM](https://arxiv.org/abs/2011.11986)<br>:star:[code](https://github.com/danini/pose-graph-initialization)
* [Spk2ImgNet: Learning To Reconstruct Dynamic Scene From Continuous Spike Stream](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_Spk2ImgNet_Learning_To_Reconstruct_Dynamic_Scene_From_Continuous_Spike_Stream_CVPR_2021_paper.pdf)
* [A Dual Iterative Refinement Method for Non-Rigid Shape Matching](https://arxiv.org/abs/2007.13049)<br>:star:[code](https://github.com/ruixiang440/Dual_Iterative_Refinement_Method)
* [Improving Accuracy of Binary Neural Networks Using Unbalanced Activation Distribution](https://arxiv.org/abs/2012.00938)
* [Rotation-Only Bundle Adjustment](https://arxiv.org/abs/2011.11724)<br>:star:[code](https://seonghun-lee.github.io/)
* [HoHoNet: 360 Indoor Holistic Understanding with Latent Horizontal Features](https://arxiv.org/abs/2011.11498)<br>:star:[code](https://github.com/sunset1995/HoHoNet)
* [Cross-Iteration Batch Normalization](https://arxiv.org/abs/2002.05712)<br>:star:[code](https://github.com/Howal/Cross-iterationBatchNorm)
* [Multimodal Contrastive Training for Visual Representation Learning](https://arxiv.org/abs/2104.12836)
* [Spatially-Varying Outdoor Lighting Estimation From Intrinsics](https://arxiv.org/abs/2104.04160)
* [Personalized Outfit Recommendation With Learnable Anchors](https://openaccess.thecvf.com/content/CVPR2021/papers/Lu_Personalized_Outfit_Recommendation_With_Learnable_Anchors_CVPR_2021_paper.pdf)
* [Architectural Adversarial Robustness: The Case for Deep Pursuit](https://arxiv.org/abs/2011.14427)
* [SetVAE: Learning Hierarchical Composition for Generative Modeling of Set-Structured Data](https://arxiv.org/abs/2103.15619)<br>:star:[code](https://github.com/jw9730/setvae)
* [Truly shift-invariant convolutional neural networks](https://arxiv.org/abs/2011.14214)<br>:star:[code](https://github.com/achaman2/truly_shift_invariant_cnns)
* [Scalable Differential Privacy With Sparse Network Finetuning](https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Scalable_Differential_Privacy_With_Sparse_Network_Finetuning_CVPR_2021_paper.pdf)
* [OpenMix: Reviving Known Knowledge for Discovering Novel Visual Categories in an Open World](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhong_OpenMix_Reviving_Known_Knowledge_for_Discovering_Novel_Visual_Categories_in_CVPR_2021_paper.pdf)
* [Event-Based Bispectral Photometry Using Temporally Modulated Illumination](https://openaccess.thecvf.com/content/CVPR2021/papers/Takatani_Event-Based_Bispectral_Photometry_Using_Temporally_Modulated_Illumination_CVPR_2021_paper.pdf)
* [Towards Extremely Compact RNNs for Video Recognition With Fully Decomposed Hierarchical Tucker Structure](https://arxiv.org/abs/2104.05758)
* [Enriching ImageNet With Human Similarity Judgments and Psychological Embeddings](https://arxiv.org/abs/2011.11015)
* [A Quasiconvex Formulation for Radial Cameras](https://openaccess.thecvf.com/content/CVPR2021/papers/Olsson_A_Quasiconvex_Formulation_for_Radial_Cameras_CVPR_2021_paper.pdf)
* [BRepNet: A Topological Message Passing System for Solid Models](https://arxiv.org/abs/2104.00706)<br>:open_mouth:oral
* [Exploiting & Refining Depth Distributions With Triangulation Light Curtains](https://openaccess.thecvf.com/content/CVPR2021/papers/Raaj_Exploiting__Refining_Depth_Distributions_With_Triangulation_Light_Curtains_CVPR_2021_paper.pdf)<br>:house:[project](https://soulslicer.github.io/rgb-lc-fusion/):tv:[video](https://youtu.be/kIjn3U8luV0)
* [Multispectral Photometric Stereo for Spatially-Varying Spectral Reflectances: A Well Posed Problem?](https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_Multispectral_Photometric_Stereo_for_Spatially-Varying_Spectral_Reflectances_A_Well_Posed_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/GH-HOME/MultispectralPS)
* [SOON: Scenario Oriented Object Navigation With Graph-Based Exploration](https://arxiv.org/abs/2103.17138)
* [Mesoscopic Photogrammetry With an Unstabilized Phone Camera](https://arxiv.org/abs/2012.06044)<br>:star:[code](https://github.com/kevinczhou/mesoscopic-photogrammetry)
* [Convolutional Hough Matching Networks](https://arxiv.org/abs/2103.16831)<br>:open_mouth:oral:star:[code](https://github.com/juhongm999/chm):house:[project](http://cvlab.postech.ac.kr/research/CHM/)
* [Learned Initializations for Optimizing Coordinate-Based Neural Representations](https://arxiv.org/abs/2012.02189)<br>:house:[project](https://www.matthewtancik.com/learnit):tv:[video](https://youtu.be/A-r9itCzcyo)
* [Patchwise Generative ConvNet: Training Energy-Based Models From a Single Natural Image for Internal Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Patchwise_Generative_ConvNet_Training_Energy-Based_Models_From_a_Single_Natural_CVPR_2021_paper.pdf)
* [LQF: Linear Quadratic Fine-Tuning](https://arxiv.org/abs/2012.11140)
* [Positive-Congruent Training: Towards Regression-Free Model Updates](https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_Positive-Congruent_Training_Towards_Regression-Free_Model_Updates_CVPR_2021_paper.pdf)
* [Shape from Sky: Polarimetric Normal Recovery Under The Sky](https://openaccess.thecvf.com/content/CVPR2021/papers/Ichikawa_Shape_From_Sky_Polarimetric_Normal_Recovery_Under_the_Sky_CVPR_2021_paper.pdf)
* [Orthogonal Over-Parameterized Training](https://arxiv.org/abs/2004.04690)<br>:open_mouth:oral
* [Optimal Gradient Checkpoint Search for Arbitrary Computation Graphs](https://arxiv.org/abs/1808.00079)<br>:open_mouth:oral:star:[code](https://github.com/lordfjw/OptimalGradCheckpointing)
* [T-vMF Similarity for Regularizing Intra-Class Feature Distribution](https://openaccess.thecvf.com/content/CVPR2021/papers/Kobayashi_T-vMF_Similarity_for_Regularizing_Intra-Class_Feature_Distribution_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/tk1980/tvMF)
* [Defending Multimodal Fusion Models Against Single-Source Adversaries](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Defending_Multimodal_Fusion_Models_Against_Single-Source_Adversaries_CVPR_2021_paper.pdf)
* [Rotation Coordinate Descent for Fast Globally Optimal Rotation Averaging](https://arxiv.org/abs/2103.08292)<br>:open_mouth:oral:star:[code](https://github.com/sfchng/Rotation_Coordinate_Descent)
* [Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations](https://openaccess.thecvf.com/content/CVPR2021/papers/Yifan_Iso-Points_Optimizing_Neural_Implicit_Surfaces_With_Hybrid_Representations_CVPR_2021_paper.pdf)
* [How Does Topology Influence Gradient Propagation and Model Performance of Deep Networks With DenseNet-Type Skip Connections?](https://arxiv.org/abs/1910.00780)<br>:star:[code](https://github.com/SLDGroup/NN_Mass)
* [Deep Stable Learning for Out-of-Distribution Generalization](https://arxiv.org/abs/2104.07876)
* [TrafficSim: Learning To Simulate Realistic Multi-Agent Behaviors](https://arxiv.org/abs/2101.06557)
* [Sign-Agnostic Implicit Learning of Surface Self-Similarities for Shape Modeling and Reconstruction From Raw Point Clouds](https://arxiv.org/abs/2012.07498)
* [Effective Sparsification of Neural Networks With Global Sparsity Constraint](https://arxiv.org/abs/2105.01571)
* [Hyperdimensional computing as a framework for systematic aggregation of image descriptors](https://arxiv.org/abs/2101.07720)<br>:house:[project](https://www.tu-chemnitz.de/etit/proaut/en/research/vsa.html)
* [Time Adaptive Recurrent Neural Network](https://openaccess.thecvf.com/content/CVPR2021/papers/Kag_Time_Adaptive_Recurrent_Neural_Network_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/anilkagak2/TARNN)
* [4D Hyperspectral Photoacoustic Data Restoration with Reliability Analysis](https://openaccess.thecvf.com/content/CVPR2021/papers/Liao_4D_Hyperspectral_Photoacoustic_Data_Restoration_With_Reliability_Analysis_CVPR_2021_paper.pdf)
* [Neighborhood Normalization for Robust Geometric Feature Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Neighborhood_Normalization_for_Robust_Geometric_Feature_Learning_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/lppllppl920/NeighborhoodNormalization-Pytorch)
* [Neural Surface Maps](https://arxiv.org/abs/2103.16942)<br>:star:[code](https://github.com/luca-morreale/neural_surface_maps):house:[project](http://geometry.cs.ucl.ac.uk/projects/2021/neuralmaps/):tv:[video](https://www.youtube.com/watch?v=DHkDCwapxc4)
* [Enhance Curvature Information by Structured Stochastic Quasi-Newton Methods](https://arxiv.org/abs/2006.09606)
* [NormalFusion: Real-Time Acquisition of Surface Normals for High-Resolution RGB-D Scanning](https://openaccess.thecvf.com/content/CVPR2021/papers/Ha_NormalFusion_Real-Time_Acquisition_of_Surface_Normals_for_High-Resolution_RGB-D_Scanning_CVPR_2021_paper.pdf)
* [Bilinear Parameterization for Non-Separable Singular Value Penalties](https://openaccess.thecvf.com/content/CVPR2021/papers/Ornhag_Bilinear_Parameterization_for_Non-Separable_Singular_Value_Penalties_CVPR_2021_paper.pdf)
* [On the Difficulty of Membership Inference Attacks](https://arxiv.org/abs/2005.13702)<br>:star:[code](https://github.com/shrezaei/MI-Attack)
* [ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks](https://arxiv.org/abs/2005.03788)<br>:star:[code](https://github.com/XinshaoAmosWang/ProSelfLC-CVPR2021)
* [Multi-Label Learning From Single Positive Labels](https://openaccess.thecvf.com/content/CVPR2021/papers/Cole_Multi-Label_Learning_From_Single_Positive_Labels_CVPR_2021_paper.pdf)
* [CompositeTasking: Understanding Images by Spatial Composition of Tasks](https://arxiv.org/abs/2012.09030)<br>:star:[code](https://github.com/nikola3794/composite-tasking)
* [Searching for Fast Model Families on Datacenter Accelerators](https://arxiv.org/abs/2102.05610)<br>:star:[code](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/tpu)
* [Understanding and Simplifying Perceptual Distances](https://openaccess.thecvf.com/content/CVPR2021/papers/Amir_Understanding_and_Simplifying_Perceptual_Distances_CVPR_2021_paper.pdf)
* [Bayesian Nested Neural Networks for Uncertainty Calibration and Adaptive Compression](https://arxiv.org/abs/2101.11353)<br>:star:[code](https://github.com/ralphc1212/variational_nested_dropout)
* [An Alternative Probabilistic Interpretation of the Huber Loss](https://arxiv.org/abs/1911.02088)
* [Scale-Localized Abstract Reasoning](https://arxiv.org/abs/2009.09405)<br>:star:[code](https://github.com/yanivbenny/MRNet):sunflower:[dataset](https://github.com/yanivbenny/RAVEN_FAIR)
* [Inferring CAD Modeling Sequences Using Zone Graphs](https://arxiv.org/abs/2104.03900)
* [Partially View-Aligned Representation Learning With Noise-Robust Contrastive Loss](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Partially_View-Aligned_Representation_Learning_With_Noise-Robust_Contrastive_Loss_CVPR_2021_paper.pdf)
* [Blocks-World Cameras](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Blocks-World_Cameras_CVPR_2021_paper.pdf)
* [The Affective Growth of Computer Vision](https://openaccess.thecvf.com/content/CVPR2021/papers/Su_The_Affective_Growth_of_Computer_Vision_CVPR_2021_paper.pdf)
* [Polarimetric Normal Stereo](https://openaccess.thecvf.com/content/CVPR2021/papers/Fukao_Polarimetric_Normal_Stereo_CVPR_2021_paper.pdf)
* [Uncalibrated Neural Inverse Rendering for Photometric Stereo of General Surfaces](https://arxiv.org/abs/2012.06777)
* [RSG: A Simple but Effective Module for Learning Imbalanced Datasets](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_RSG_A_Simple_but_Effective_Module_for_Learning_Imbalanced_Datasets_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/Jianf-Wang/RSG)
* [Fast Sinkhorn Filters: Using Matrix Scaling for Non-Rigid Shape Correspondence With Functional Maps](https://openaccess.thecvf.com/content/CVPR2021/papers/Pai_Fast_Sinkhorn_Filters_Using_Matrix_Scaling_for_Non-Rigid_Shape_Correspondence_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/paigautam/CVPR21_FastSinkhornFilters)
* [MetaSets: Meta-Learning on Point Sets for Generalizable Representations](https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_MetaSets_Meta-Learning_on_Point_Sets_for_Generalizable_Representations_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/SugarRuy/CVPR21_Transferred_Hash)
* [Isometric Multi-Shape Matching](https://arxiv.org/abs/2012.02689)
* [Efficient Deformable Shape Correspondence via Multiscale Spectral Manifold Wavelets Preservation](https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Efficient_Deformable_Shape_Correspondence_via_Multiscale_Spectral_Manifold_Wavelets_Preservation_CVPR_2021_paper.pdf)
* [TearingNet: Point Cloud Autoencoder To Learn Topology-Friendly Representations](https://arxiv.org/abs/2006.10187)
* [Boosting Ensemble Accuracy by Revisiting Ensemble Diversity Metrics](https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Boosting_Ensemble_Accuracy_by_Revisiting_Ensemble_Diversity_Metrics_CVPR_2021_paper.pdf)
* [Convolutional Dynamic Alignment Networks for Interpretable Classifications](https://arxiv.org/abs/2104.00032)<br>:open_mouth:oral:star:[code](https://github.com/moboehle/CoDA-Nets)
* [EDNet: Efficient Disparity Estimation With Cost Volume Combination and Attention-Based Spatial Residual](https://arxiv.org/abs/2010.13338)
* [How Robust are Randomized Smoothing based Defenses to Data Poisoning?](https://arxiv.org/abs/2012.01274)<br>:star:[code](https://github.com/akshaymehra24/poisoning_certified_defenses)
* [Generative Interventions for Causal Learning](https://arxiv.org/abs/2012.12265)
* [Learning to Identify Correct 2D-2D Line Correspondences on Sphere](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Learning_To_Identify_Correct_2D-2D_Line_Correspondences_on_Sphere_CVPR_2021_paper.pdf)
* [Domain-Independent Dominance of Adaptive Methods](https://arxiv.org/abs/1912.01823)<br>:star:[code](https://github.com/lolemacs/avagrad)
* [Combinatorial Learning of Graph Edit Distance via Dynamic Embedding](https://arxiv.org/abs/2011.15039)
* [IMODAL: Creating Learnable User-Defined Deformation Models](https://openaccess.thecvf.com/content/CVPR2021/papers/Lacroix_IMODAL_Creating_Learnable_User-Defined_Deformation_Models_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/imodal)
* [Robust Bayesian Neural Networks by Spectral Expectation Bound Regularization](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Robust_Bayesian_Neural_Networks_by_Spectral_Expectation_Bound_Regularization_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/AISIGSJTU/SEBR)
* [Neural Cellular Automata Manifold](https://openaccess.thecvf.com/content/CVPR2021/papers/Hernandez_Neural_Cellular_Automata_Manifold_CVPR_2021_paper.pdf)
* [MultiLink: Multi-Class Structure Recovery via Agglomerative Clustering and Model Selection](https://openaccess.thecvf.com/content/CVPR2021/papers/Magri_MultiLink_Multi-Class_Structure_Recovery_via_Agglomerative_Clustering_and_Model_Selection_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/magrilu/multilink)
* [A Sliced Wasserstein Loss for Neural Texture Synthesis](https://arxiv.org/abs/2006.07229)
* [A Second-Order Approach to Learning with Instance-Dependent Label Noise](https://arxiv.org/abs/2012.11854)<br>:open_mouth:oral:star:[code](https://github.com/UCSC-REAL/CAL)
* [Hilbert Sinkhorn Divergence for Optimal Transport](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Hilbert_Sinkhorn_Divergence_for_Optimal_Transport_CVPR_2021_paper.pdf)
* [The Multi-Temporal Urban Development SpaceNet Dataset](https://openaccess.thecvf.com/content/CVPR2021/papers/Van_Etten_The_Multi-Temporal_Urban_Development_SpaceNet_Dataset_CVPR_2021_paper.pdf)
* [Inverse Simulation: Reconstructing Dynamic Geometry of Clothed Humans via Optimal Control](https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_Inverse_Simulation_Reconstructing_Dynamic_Geometry_of_Clothed_Humans_via_Optimal_CVPR_2021_paper.pdf)
* [Learning Decision Trees Recurrently Through Communication](https://arxiv.org/abs/1902.01780)<br>:star:[code](https://github.com/ExplainableML/rdtc)
* [Learning the Predictability of the Future](https://arxiv.org/abs/2101.01600)<br>:star:[code](https://hyperfuture.cs.columbia.edu/):house:[project](https://hyperfuture.cs.columbia.edu/)
* [RaScaNet: Learning Tiny Models by Raster-Scanning Images](https://openaccess.thecvf.com/content/CVPR2021/papers/Yoo_RaScaNet_Learning_Tiny_Models_by_Raster-Scanning_Images_CVPR_2021_paper.pdf)
* [Joint Negative and Positive Learning for Noisy Labels](https://arxiv.org/abs/2104.06574)
* [The Heterogeneity Hypothesis: Finding Layer-Wise Differentiated Network Architectures](https://arxiv.org/abs/2006.16242)<br>:star:[code](https://github.com/ofsoundof/Heterogeneity_Hypothesis)
* [Understanding Failures of Deep Networks via Robust Feature Extraction](https://arxiv.org/abs/2012.01750)<br>:star:[code](https://github.com/singlasahil14/barlow)
* [Gradient-based Algorithms for Machine Teaching](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Gradient-Based_Algorithms_for_Machine_Teaching_CVPR_2021_paper.pdf)
* [Geo-FARM: Geodesic Factor Regression Model for Misaligned Pre-Shape Responses in Statistical Shape Analysis](https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Geo-FARM_Geodesic_Factor_Regression_Model_for_Misaligned_Pre-Shape_Responses_in_CVPR_2021_paper.pdf)
* [A Functional Approach to Rotation Equivariant Non-Linearities for Tensor Field Networks](https://openaccess.thecvf.com/content/CVPR2021/papers/Poulenard_A_Functional_Approach_to_Rotation_Equivariant_Non-Linearities_for_Tensor_Field_CVPR_2021_paper.pdf)
* [Real-Time Sphere Sweeping Stereo From Multiview Fisheye Images](https://openaccess.thecvf.com/content/CVPR2021/papers/Meuleman_Real-Time_Sphere_Sweeping_Stereo_From_Multiview_Fisheye_Images_CVPR_2021_paper.pdf)
* [Taskology: Utilizing Task Relations at Scale](https://arxiv.org/abs/2005.07289)
* [Soteria: Provable Defense against Privacy Leakage in Federated Learning from Representation Perspective](https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_Soteria_Provable_Defense_Against_Privacy_Leakage_in_Federated_Learning_From_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/jeremy313/Soteria)
* [Spatial Assembly Networks for Image Representation Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Spatial_Assembly_Networks_for_Image_Representation_Learning_CVPR_2021_paper.pdf)
* [SKFAC: Training Neural Networks With Faster Kronecker-Factored Approximate Curvature](https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_SKFAC_Training_Neural_Networks_With_Faster_Kronecker-Factored_Approximate_Curvature_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/fL0n9/SKFAC-MindSpore)
* [Student-Teacher Learning from Clean Inputs to Noisy Inputs](https://arxiv.org/abs/2103.07600)
* [Adversarial Invariant Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Ye_Adversarial_Invariant_Learning_CVPR_2021_paper.pdf)
* [S2-BNN: Bridging the Gap Between Self-Supervised Real and 1-Bit Neural Networks via Guided Distribution Calibration](https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_S2-BNN_Bridging_the_Gap_Between_Self-Supervised_Real_and_1-Bit_Neural_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/szq0214)
* [MaxUp: Lightweight Adversarial Training With Data Augmentation Improves Neural Network Training](https://openaccess.thecvf.com/content/CVPR2021/papers/Gong_MaxUp_Lightweight_Adversarial_Training_With_Data_Augmentation_Improves_Neural_Network_CVPR_2021_paper.pdf)
* [Explaining Classifiers using Adversarial Perturbations on the Perceptual Ball](https://arxiv.org/abs/1912.09405)
* Visual Grounding
  * [Look Before You Leap: Learning Landmark Features for One-Stage Visual Grounding](https://arxiv.org/abs/2104.04386)<br>:star:[code](https://github.com/svip-lab/LBYLNet)
  * [Refer-It-in-RGBD: A Bottom-Up Approach for 3D Visual Grounding in RGBD Images](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Refer-It-in-RGBD_A_Bottom-Up_Approach_for_3D_Visual_Grounding_in_RGBD_CVPR_2021_paper.pdf)
* è¯­ä¹‰åŒ¹é…
  * [Probabilistic Model Distillation for Semantic Correspondence](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Probabilistic_Model_Distillation_for_Semantic_Correspondence_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/fanyang587/PMD)
  * [PatchMatch-Based Neighborhood Consensus for Semantic Correspondence](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_PatchMatch-Based_Neighborhood_Consensus_for_Semantic_Correspondence_CVPR_2021_paper.pdf)<br>:star:[code](http://github.com/leejaeyong7/PMNC)
  * [Discovering Relationships Between Object Categories via Universal Canonical Maps](https://openaccess.thecvf.com/content/CVPR2021/papers/Neverova_Discovering_Relationships_Between_Object_Categories_via_Universal_Canonical_Maps_CVPR_2021_paper.pdf)
* æ¢¯åº¦å‹ç¼©
  * [Communication Efficient SGD via Gradient Sampling With Bayes Prior](https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Communication_Efficient_SGD_via_Gradient_Sampling_With_Bayes_Prior_CVPR_2021_paper.pdf)
* è‡ªåŠ¨ç”Ÿæˆæ¼«ç”»
  * [Generating Manga From Illustrations via Mimicking Manga Creation Workflow](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Generating_Manga_From_Illustrations_via_Mimicking_Manga_Creation_Workflow_CVPR_2021_paper.pdf)    
* è”åˆå­¦ä¹ 
  * [EffiScene: Efficient Per-Pixel Rigidity Inference for Unsupervised Joint Learning of Optical Flow, Depth, Camera Pose and Motion Segmentation](https://arxiv.org/abs/2011.08332)
* DL
  * [DeepLM: Large-Scale Nonlinear Least Squares on Deep Learning Frameworks Using Stochastic Domain Decomposition](https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_DeepLM_Large-Scale_Nonlinear_Least_Squares_on_Deep_Learning_Frameworks_Using_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/hjwdzh/DeepLM)
* å§¿åŠ¿ä¼°è®¡ï¼ˆéäººä½“ï¼‰
  * [Globally Optimal Relative Pose Estimation With Gravity Prior](https://arxiv.org/abs/2012.00458)
* å…¨å®¶ç¦
  * [Inception Convolution With Efficient Dilation Search](https://arxiv.org/abs/2012.13587)<br>å›¾åƒè¯†åˆ«ã€äººä½“å§¿æ€ä¼°è®¡ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²
* è§†è§‰æ¨ç†
  * [Transformation Driven Visual Reasoning](https://arxiv.org/abs/2011.13160)<br>:star:[code](https://github.com/hughplay/TVR/):house:[project](https://hongxin2019.github.io/TVR/)
* mesh saliency
  * [Mesh Saliency: An Independent Perceptual Measure or a Derivative of Image Saliency?](https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Mesh_Saliency_An_Independent_Perceptual_Measure_or_a_Derivative_of_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/rsong/MIMO-GAN)
* 3Dåœºæ™¯äº¤äº’
  * [Populating 3D Scenes by Learning Human-Scene Interaction](https://arxiv.org/abs/2012.11581)<br>:star:[code](https://github.com/mohamedhassanmus/POSA):house:[project](https://posa.is.tue.mpg.de/):tv:[video](https://youtu.be/aTDNupqcSfM)
* Stereo Matching(ç«‹ä½“åŒ¹é…)
  * [AdaStereo: A Simple and Efficient Approach for Adaptive Stereo Matching](https://arxiv.org/abs/2004.04627)
  * [HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching](https://arxiv.org/abs/2007.12140)<br>:star:[code](https://github.com/google-research/google-research/tree/master/hitnet)
  * [Bilateral Grid Learning for Stereo Matching Networks](https://arxiv.org/abs/2101.01601)<br>:star:[code](https://github.com/YuhuaXu/BGNet)
* å›¾åƒåˆ°è§†é¢‘åˆæˆ
  * [Understanding Object Dynamics for Interactive Image-to-Video Synthesis](https://openaccess.thecvf.com/content/CVPR2021/papers/Blattmann_Understanding_Object_Dynamics_for_Interactive_Image-to-Video_Synthesis_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/CompVis/interactive-image2video-synthesis):house:[project](https://compvis.github.io/interactive-image2video-synthesis/)
* Audio-Visual Navigation(è§†å¬å¯¼èˆª)
  * [Semantic Audio-Visual Navigation](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Semantic_Audio-Visual_Navigation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/sound-spaces/tree/master/ss_baselines/savi):house:[project](http://vision.cs.utexas.edu/projects/semantic-audio-visual-navigation/):tv:[video](https://youtu.be/EKCYc1dFOhw)
* å­—ä½“ç”Ÿæˆ
  * [DG-Font: Deformable Generative Networks for Unsupervised Font Generation](https://openaccess.thecvf.com/content/CVPR2021/papers/Xie_DG-Font_Deformable_Generative_Networks_for_Unsupervised_Font_Generation_CVPR_2021_paper.pdf)<br>:star:[code](https://github.com/ecnuycxie/DG-Font)
* å¤šä»»åŠ¡å­¦ä¹ 
  * [Deep Multi-Task Learning for Joint Localization, Perception, and Prediction](https://arxiv.org/abs/2101.06720)
* è§†è§‰å¯¼èˆª
  * [Visual Navigation With Spatial Attention](https://arxiv.org/abs/2104.09807)
* å›¾åƒåŒ¹é…
  * [Co-Attention for Conditioned Image Matching](https://arxiv.org/abs/2007.08480)<br>:star:[code](https://github.com/hyenal/coam):house:[project](https://www.robots.ox.ac.uk/~ow/coam.html) 
* texture recognition(çº¹ç†è¯†åˆ«)
  * [Deep Texture Recognition via Exploiting Cross-Layer Statistical Self-Similarity](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Deep_Texture_Recognition_via_Exploiting_Cross-Layer_Statistical_Self-Similarity_CVPR_2021_paper.pdf)
* Hyperspectral Image Reconstruction(é«˜å…‰è°±å›¾åƒé‡å»º)
  * [Learning Tensor Low-Rank Prior for Hyperspectral Image Reconstruction](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_Tensor_Low-Rank_Prior_for_Hyperspectral_Image_Reconstruction_CVPR_2021_paper.pdf)
* Visual Odometry(è§†è§‰é‡Œç¨‹è®¡)
  * [Spatiotemporal Registration for Event-Based Visual Odometry](https://arxiv.org/abs/2103.05955)<br>:sunflower:[dataset](https://github.com/liudaqikk/RobotEvt)
* image registration(å›¾åƒé…å‡†)
  * [Learning-Based Image Registration With Meta-Regularization](https://openaccess.thecvf.com/content/CVPR2021/papers/Safadi_Learning-Based_Image_Registration_With_Meta-Regularization_CVPR_2021_paper.pdf)
* semantic part completion(è¯­ä¹‰åœºæ™¯è¡¥å…¨)
  * [Towards Part-Based Understanding of RGB-D Scans](https://openaccess.thecvf.com/content/CVPR2021/papers/Bokhovkin_Towards_Part-Based_Understanding_of_RGB-D_Scans_CVPR_2021_paper.pdf)
* è¡Œäººå’Œè½¦è¾†ç›¸äº’ä½œç”¨
  * [Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers](https://openaccess.thecvf.com/content/CVPR2021/papers/Bhattacharyya_Euro-PVI_Pedestrian_Vehicle_Interactions_in_Dense_Urban_Centers_CVPR_2021_paper.pdf)  
* æƒ…æ„Ÿè®¡ç®—
  * [A Circular-Structured Representation for Visual Emotion Distribution Learning](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_A_Circular-Structured_Representation_for_Visual_Emotion_Distribution_Learning_CVPR_2021_paper.pdf)
* ä¼°è®¡å¯†é›†çš„å›¾åƒä¸å›¾åƒä¹‹é—´çš„å¯¹åº”å…³ç³»å’Œç›¸å…³çš„ä¿¡åº¦ä¼°è®¡
  * [Learning Accurate Dense Correspondences and When To Trust Them](https://arxiv.org/abs/2101.01710)<br>:open_mouth:oral:star:[code](https://github.com/PruneTruong/DenseMatching):house:[project](https://prunetruong.com/research/pdcnet):tv:[video](https://www.youtube.com/watch?v=bX0rEaSf88o)
* ç”¨Deep-Red Flashçœ‹é»‘æš—ä¸­çš„ç‰©ä½“
  * [Seeing in Extra Darkness Using a Deep-Red Flash](https://openaccess.thecvf.com/content/CVPR2021/papers/Xiong_Seeing_in_Extra_Darkness_Using_a_Deep-Red_Flash_CVPR_2021_paper.pdf) 
 

## æ‰«ç CVå›å¾®ä¿¡ï¼ˆæ³¨æ˜ï¼šCVPRï¼‰å…¥å¾®ä¿¡äº¤æµç¾¤ï¼š

![image](https://user-images.githubusercontent.com/62801906/109789529-655e4380-7c4b-11eb-9f1a-58c5cb097428.png)
