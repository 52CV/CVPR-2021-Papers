# CVPR2021æœ€æ–°ä¿¡æ¯åŠå·²æ¥æ”¶è®ºæ–‡/ä»£ç (æŒç»­æ›´æ–°)


æœ¬è´´æ˜¯å¯¹ CVPR2021 å·²æ¥å—è®ºæ–‡çš„ç²—ç•¥æ±‡æ€»ï¼ŒåæœŸä¼šæœ‰æ›´è¯¦ç»†çš„æ€»ç»“ã€‚æœŸå¾…ing......

å®˜ç½‘é“¾æ¥ï¼šhttp://cvpr2021.thecvf.com<br>
å¼€ä¼šæ—¶é—´ï¼š2021å¹´6æœˆ19æ—¥-6æœˆ25æ—¥<br>
è®ºæ–‡æ¥æ”¶å…¬å¸ƒæ—¶é—´ï¼š2021å¹´2æœˆ28æ—¥<br>

æ¥æ”¶è®ºæ–‡IDsï¼š<br>

* [CVPR 2021 æ¥æ”¶è®ºæ–‡åˆ—è¡¨ï¼27%æ¥å—ç‡ï¼](https://zhuanlan.zhihu.com/p/353686917)

## ğŸ“—ğŸ“—ğŸ“—ä¸‹è½½å·²å…¬å¼€è®ºæ–‡åœ¨ã€æˆ‘çˆ±è®¡ç®—æœºè§†è§‰ã€‘åå°å›å¤â€œCVPR2021â€ï¼Œå³å¯æ”¶åˆ°ã€‚ç›®å‰å·²å…¬å¼€ 739+6 ç¯‡ã€‚

# CVPR2021æœ€æ–°ä¿¡æ¯åŠå·²æ¥æ”¶è®ºæ–‡/ä»£ç (æŒç»­æ›´æ–°)
### :fireworks::fireworks::fireworks:æ›´æ–°æç¤ºï¼š5æœˆ26æ—¥æ–°å¢3ç¯‡
* NAS
  * [TransNAS-Bench-101: Improving Transferability and Generalizability of Cross-Task Neural Architecture Search](https://arxiv.org/abs/2105.11871)<br>:sunflower:[dataset](https://download.mindspore.cn/dataset/TransNAS-Bench-101/)
* ä¸‰ç»´
  * [Multi-view 3D Reconstruction of a Texture-less Smooth Surface of Unknown Generic Reflectance](https://arxiv.org/abs/2105.11599)<br>:star:[code](https://github.com/za-cheng/PM-PMVS/)
* è·Ÿè¸ª
  * [SiamMOT: Siamese Multi-Object Tracking](https://arxiv.org/abs/2105.11595)<br>:star:[code](https://github.com/amazon-research/siam-mot)

### :fireworks::fireworks::fireworks:æ›´æ–°æç¤ºï¼š5æœˆ25æ—¥æ–°å¢6ç¯‡

* è§†è§‰å®šä½
  * [VS-Net: Voting with Segmentation for Visual Localization](https://arxiv.org/abs/2105.10886)<br>:star:[code](https://github.com/zju3dv/VS-Net):house:[project](https://drinkingcoder.github.io/publication/vs-net/):tv:[video](https://youtu.be/5WLEyyLdxAs)
* æ¨¡å‹å‹ç¼©
  * [Towards Compact CNNs via Collaborative Compression](https://arxiv.org/abs/2105.11228)
  * [BCNet: Searching for Network Width with Bilaterally Coupled Network](https://arxiv.org/abs/2105.10533)
* åŸŸæ³›åŒ–
  * [A Fourier-based Framework for Domain Generalization](https://arxiv.org/abs/2105.11120)<br>:open_mouth:oral
* äººè„¸
  * [Dynamic Class Queue for Large Scale Face Recognition In the Wild](https://arxiv.org/abs/2105.11113)<br>:star:[code](https://github.com/bilylee/DCQ)
* å»å™ª
  * [FBI-Denoiser: Fast Blind Image Denoiser for Poisson-Gaussian Noise](https://arxiv.org/abs/2105.10967)<br>:star:[code](https://github.com/csm9493/FBI-Denoiser)

### :fireworks::fireworks::fireworks:æ›´æ–°æç¤ºï¼š5æœˆ24æ—¥æ–°å¢3ç¯‡
* åˆ†å‰²
  * [Omni-supervised Point Cloud Segmentation via Gradual Receptive Field Component Reasoning](https://arxiv.org/abs/2105.10203)
* NAS
  * [ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search](https://arxiv.org/abs/2105.10154)
* åˆ†ç±»
  * [Correlated Input-Dependent Label Noise in Large-Scale Image Classification](https://arxiv.org/abs/2105.10305)<br>:open_mouth:oral:star:[code](https://github.com/google/uncertainty-baselines/tree/master/baselines/imagenet)

### :fireworks::fireworks::fireworks:æ›´æ–°æç¤ºï¼š5æœˆ21æ—¥æ–°å¢4ç¯‡
* è§†é¢‘å‹ç¼©
  * [FVC: A New Framework towards Deep Video Compression in Feature Space](https://arxiv.org/abs/2105.09600)<br>:open_mouth:oral
* ç›®æ ‡æ£€æµ‹
  * [Generalized Few-Shot Object Detection without Forgetting](https://arxiv.org/abs/2105.09491)
* ä¸‰ç»´
  * [Birds of a Feather: Capturing Avian Shape Models from Images](https://arxiv.org/abs/2105.09396)<br>:house:[project](https://yufu-wang.github.io/aves/):tv:[video](https://youtu.be/TDR2LC7mFpw)
* Gaze Estimation
  * [Weakly-Supervised Physically Unconstrained Gaze Estimation](https://arxiv.org/abs/2105.09803)<br>:open_mouth:oral:star:[code](https://github.com/NVlabs/weakly-supervised-gaze)

### :fireworks::fireworks::fireworks:æ›´æ–°æç¤ºï¼š5æœˆ20æ—¥æ–°å¢4ç¯‡
* å›¾åƒåˆ°å›¾åƒç¿»è¯‘
  * [High-Resolution Photorealistic Image Translation in Real-Time: A Laplacian Pyramid Translation Network](https://arxiv.org/abs/2105.09188)<br>:star:[code](https://github.com/csjliang/LPTN)
* æ•°æ®é›†
  * [PPR10K: A Large-Scale Portrait Photo Retouching Dataset with Human-Region Mask and Group-Level Consistency](https://arxiv.org/abs/2105.09180)<br>:star:[code](https://github.com/csjliang/PPR10K)
* Reid
  * [Generalizable Person Re-identification with Relevance-aware Mixture of Experts](https://arxiv.org/abs/2105.09156)
* åˆ†å‰²
  * [Railroad is not a Train: Saliency as Pseudo-pixel Supervision for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2105.08965)<br>:star:[code](https://github.com/halbielee/EPS)

### :fireworks::fireworks::fireworks:æ›´æ–°æç¤ºï¼š5æœˆ19æ—¥æ–°å¢5ç¯‡
* åˆ†å‰²
  * [Exemplar-Based Open-Set Panoptic Segmentation Network](https://arxiv.org/abs/2105.08336)<br>:star:[code](https://github.com/jd730/EOPSN):house:[project](https://cv.snu.ac.kr/research/EOPSN/)
* åŸŸé€‚åº”
  * [PixMatch: Unsupervised Domain Adaptation via Pixelwise Consistency Training](https://arxiv.org/abs/2105.08128)
* æ•°æ®é›†
  * [SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction from Video Data](https://arxiv.org/abs/2105.08612)<br>:open_mouth:oral:sunflower:[dataset](http://sailvos.web.illinois.edu/_site/index.html)
* åœºæ™¯æµä¼°è®¡
  * [Self-Point-Flow: Self-Supervised Scene Flow Estimation from Point Clouds with Optimal Transport and Random Walk](https://arxiv.org/abs/2105.08248)<br>:open_mouth:oral
* VQA
  * [NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions](https://arxiv.org/abs/2105.08276)<br>:star:[code](https://github.com/doc-doc/NExT-QA)

### :fireworks::fireworks::fireworks:æ›´æ–°æç¤ºï¼š5æœˆ18æ—¥æ–°å¢6ç¯‡

* æ£€ç´¢
  * [Prototype-supervised Adversarial Network for Targeted Attack of Deep Hashing](https://arxiv.org/abs/2105.07553)<br>:star:[code](https://github.com/xunguangwang/ProS-GAN)
* åŸŸé€‚åº”
  * [Learning to Relate Depth and Semantics for Unsupervised Domain Adaptation](https://arxiv.org/abs/2105.07830)<br>:star:[code](https://github.com/susaha/ctrl-uda)
* æ— ç›‘ç£
  * [SMURF: Self-Teaching Multi-Frame Unsupervised RAFT with Full-Image Warping](https://arxiv.org/abs/2105.07014)<br>:star:[code](https://github.com/google-research/google-research/tree/master/smurf)
* è¿ç»­å­¦ä¹ 
  * [Layerwise Optimization by Gradient Decomposition for Continual Learning](https://arxiv.org/abs/2105.07561)
* åœºæ™¯æµä¼°è®¡
  * [HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs and Position-aware Flow Embedding](https://arxiv.org/abs/2105.07751)
* SLAM
  * [Differentiable SLAM-net: Learning Particle SLAM for Visual Navigation](https://arxiv.org/abs/2105.07593)<br>:house:[project](https://sites.google.com/view/slamnet):tv:[video](https://youtu.be/dk1fdtf3fNI)

### :fireworks::fireworks::fireworks:æ›´æ–°æç¤ºï¼š5æœˆ17æ—¥æ–°å¢3ç¯‡

* åˆ†å‰²
  * [Omnimatte: Associating Objects and Their Effects in Video](https://arxiv.org/abs/2105.06993)<br>:open_mouth:oral:house:[project](https://omnimatte.github.io/)
* åŠ¨ä½œæ£€æµ‹
  * [Collaborative Spatial-Temporal Modeling for Language-Queried Video Actor Segmentation](https://arxiv.org/abs/2105.06818)
* ä¸‰ç»´
  * [Sketch2Model: View-Aware 3D Modeling from Single Free-Hand Sketches](https://arxiv.org/abs/2105.06663)


# ç›®å½•

|:dog:|:mouse:|:hamster:|:tiger:|
|------|------|------|------|
|[Workshopå¾ç¨¿](#*)|
|[72.Gaze Estimation(è§†çº¿ä¼°è®¡)](#72)|[71.Image-to-Image Translation(å›¾åƒåˆ°å›¾åƒç¿»è¯‘)](#71)|[70.Trajectory Forecasting(è½¨è¿¹é¢„æµ‹)](#70)|[69.Transfer learning(è¿ç§»å­¦ä¹ )](#69)|
|[68.Crowd Counting(è®¡æ•°)](#68)|[67.Defect Detection(ç¼ºé™·æ£€æµ‹)](#67)|[66.Optical Flow Estimation(å…‰æµä¼°è®¡)](#66)|[65.Style Transfer(é£æ ¼è¿ç§»)](#65)
|[64.Speech processing(è¯­éŸ³å¤„ç†)](#64)|[63.Image Processing(å›¾åƒå¤„ç†)](#63)|[62.Free-Hand Sketches(æ‰‹ç»˜è‰å›¾è¯†åˆ«)](#62)|[61.ç®—æ³•](#61)|
|[60. SLAM/AR/æœºå™¨äºº](#60)|[59.æ·±åº¦å­¦ä¹ æ¨¡å‹](#59)|[58.Metric Learning(åº¦é‡å­¦ä¹ /ç›¸ä¼¼åº¦å­¦ä¹ )](#58)|[57.Sign Language Recognition(æ‰‹è¯­è¯†åˆ«)](#57)|
|[56.Computational Photography(å…‰å­¦ã€å‡ ä½•ã€å…‰åœºæˆåƒã€è®¡ç®—æ‘„å½±)](#56)|[55.Graph Matching(å›¾åŒ¹é…)](#55)|[54.Emotion Perception(æƒ…ç»ªæ„ŸçŸ¥/æƒ…æ„Ÿé¢„æµ‹)](#54)|[53.Dataset(æ•°æ®é›†)](#53)|
|[52. Image Generation/Synthesis(å›¾åƒç”Ÿæˆ)](#52)|[51.Contrastive Learning(å¯¹æ¯”å­¦ä¹ )](#51)|[50.OCR](#50)|[49.Adversarial Learning(å¯¹æŠ—å­¦ä¹ )](#49)|
|[48.Image Representation(å›¾åƒè¡¨ç¤º)](#48)|[47.Vision-Language(è§†è§‰è¯­è¨€)](#47)|[46.Human-Object Interaction(äººç‰©äº¤äº’)](#46)|[45.Camera Localization(ç›¸æœºå®šä½)](#45)|
|[44. Image/video Captioning(å›¾åƒ/è§†é¢‘å­—å¹•)](#44)|[43.Active Learning(ä¸»åŠ¨å­¦ä¹ )](#43)|[42.Scene Flow Estimation(åœºæ™¯æµä¼°è®¡)](#42)|[41. Representation Learning(è¡¨ç¤ºå­¦ä¹ ï¼ˆå›¾åƒ+å­—å¹•ï¼‰)](#41)|
|[40.Superpixel (è¶…åƒç´ )](#40)|[39.Debiasing(å»åè§)](#39)|[38.Class-Incremental learning(ç±»å¢é‡å­¦ä¹ )](#38)|[37.Continual Learning(æŒç»­å­¦ä¹ )](#37)|
|[36.Action Detection and Recognition(åŠ¨ä½œæ£€æµ‹ä¸è¯†åˆ«)](#36)|[35.Image Clustering(å›¾åƒèšç±») ](#35)|[34.Image/Fine-Grained Classification(å›¾åƒåˆ†ç±»/ç»†ç²’åº¦åˆ†ç±»)](#34)|[33.6D Pose Estimation(6Dä½å§¿ä¼°è®¡)](#33)|
|[32.View Synthesis(è§†å›¾åˆæˆ)](#32)|[31.Open-Set Recognition(å¼€æ”¾é›†è¯†åˆ«)](#31)|[30.Neural rendering(ç¥ç»æ¸²æŸ“)](#30)|[29.Human Pose Estimation(äººä½“å§¿æ€ä¼°è®¡)](#29)|
|[28.Dense prediction(å¯†é›†é¢„æµ‹)](#28)|[27.Semantic Line Detection(è¯­ä¹‰çº¿æ£€æµ‹)](#27)|[26.Video Processing(è§†é¢‘ç›¸å…³æŠ€æœ¯)](#26)|[25.3D(ä¸‰ç»´è§†è§‰)](#25)|
|[24.Reinforcement Learning(å¼ºåŒ–å­¦ä¹ )](#24)|[23.Autonomous Driving(è‡ªåŠ¨é©¾é©¶)](#23)|[22.Medical Imaging(åŒ»å­¦å½±åƒ)](#22)|[21.Transformer/Self-attention](#21)|
|[20.Person Re-Identification(äººå‘˜é‡è¯†åˆ«)](#20)|[19.Quantization/Pruning/Knowledge Distillation/Model Compression(é‡åŒ–ã€å‰ªæã€è’¸é¦ã€æ¨¡å‹å‹ç¼©/æ‰©å±•ä¸ä¼˜åŒ–)](#19)|[18.Aeria/Drones/Satellite/RS Image(èˆªç©ºå½±åƒ/æ— äººæœº)](#18)|[17.Super-Resolution(è¶…åˆ†è¾¨ç‡)](#17)|
|[16.Visual Question Answering(è§†è§‰é—®ç­”)](#16)|[15.GAN](#15)|[14.Few-Shot/Zero-Shot Learning,Domain Generalization/Adaptation(å°/é›¶æ ·æœ¬å­¦ä¹ ï¼ŒåŸŸé€‚åº”ï¼ŒåŸŸæ³›åŒ–)](#14)|[13.Image/Video Retrieval(å›¾åƒ/è§†é¢‘æ£€ç´¢)](#13)|
|[12.Image Quality Assessment(å›¾åƒè´¨é‡è¯„ä¼°)](#12)|[11. Face(äººè„¸æŠ€æœ¯)](#11)|[10.Neural Architecture Search(ç¥ç»æ¶æ„æœç´¢)](#10)|[9.Object Tracking(ç›®æ ‡è·Ÿè¸ª)](#9)
|[8.Image Segmentation(å›¾åƒåˆ†å‰²)](#8)|[7.Object Detection(ç›®æ ‡æ£€æµ‹)](#7)|[6.Data Augmentation(æ•°æ®å¢å¹¿)](#6)|[5.Anomaly Detection(å¼‚å¸¸æ£€æµ‹)](#5)|
|[4.Weakly Supervised/Semi-Supervised/Self-supervised/Unsupervised Learning(è‡ª/åŠ/å¼±ç›‘ç£å­¦ä¹ )](#4)|[3.Point Cloud(ç‚¹äº‘)](#3)|[2.Graph Neural Networks(å›¾å·ç§¯ç½‘ç»œGNN)](#2)|[1.Unkown(æœªåˆ†ç±»)](#1)|

<a name="72"/>

## 72.Gaze Estimation(è§†çº¿ä¼°è®¡)
* [Weakly-Supervised Physically Unconstrained Gaze Estimation](https://arxiv.org/abs/2105.09803)<br>:open_mouth:oral:star:[code](https://github.com/NVlabs/weakly-supervised-gaze)

<a name="71"/>

## 71.Image-to-Image Translation(å›¾åƒåˆ°å›¾åƒç¿»è¯‘)
* [High-Resolution Photorealistic Image Translation in Real-Time: A Laplacian Pyramid Translation Network](https://arxiv.org/abs/2105.09188)<br>:star:[code](https://github.com/csjliang/LPTN)

<a name="70"/>

## 70.Trajectory Forecasting(è½¨è¿¹é¢„æµ‹)
* äººä½“è½¨è¿¹é¢„æµ‹
  * [Interpretable Social Anchors for Human Trajectory Forecasting in Crowds](https://arxiv.org/abs/2105.03136)

<a name="69"/>

## 69.Transfer learning(è¿ç§»å­¦ä¹ )
* åŸŸè¿ç§»
  * [Visualizing Adapted Knowledge in Domain Transfer](https://arxiv.org/abs/2104.10602)<br>:star:[code](https://github.com/hou-yz/DA_visualization) 

<a name="68"/>

## 68.Crowd Counting(è®¡æ•°)
  * [Learning To Count Everything](https://arxiv.org/abs/2104.08391)<br>:star:[code](https://github.com/cvlab-stonybrook)

<a name="67"/>

## 67.Defect Detection(ç¼ºé™·æ£€æµ‹)
  * [CutPaste: Self-Supervised Learning for Anomaly Detection and Localization](https://arxiv.org/abs/2104.04015)

<a name="66"/>

## 66.Optical Flow Estimation(å…‰æµä¼°è®¡)
* [UPFlow:Upsampling Pyramid for Unsupervised Optical Flow Learning](https://arxiv.org/abs/2012.00212)<br>ç²—è§£ï¼š[8](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* [Learning Optical Flow from a Few Matches](https://arxiv.org/abs/2104.02166)<br>:star:[code](https://github.com/zacjiang/scv)
* [Learning optical flow from still images](https://arxiv.org/abs/2104.03965)<br>:star:[code](https://github.com/mattpoggi/depthstillation):house:[project](https://mattpoggi.github.io/projects/cvpr2021aleotti/)
* [AutoFlow: Learning a Better Training Set for Optical Flow](https://arxiv.org/abs/2104.14544)<br>:open_mouth::house:[project](https://autoflow-google.github.io/)<br>AutoFlow ï¼šCVPR 2021 Oral ,ä½œè€…å‘æ˜äº†ä¸€ç§ä¸“ä¸ºå…‰æµç®—æ³•è®­ç»ƒè€Œè®¾è®¡çš„æ•°æ®æ¸²æŸ“æ–¹æ³•ï¼Œæ‰€è®­ç»ƒå¾—åˆ°çš„PWC-Net ä¸ RAFTå…‰æµç®—æ³•è¾¾åˆ°äº†SOTA,ä»£ç å’Œæ•°æ®å°†å¼€æºã€‚

<a name="65"/>

## 65.Style Transfer(é£æ ¼è¿ç§»)
* [Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes](https://arxiv.org/abs/2103.17185)<br>:star:[code](https://github.com/CompVis/brushstroke-parameterized-style-transfer)
* [ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows](https://arxiv.org/abs/2103.16877)<br>:star:[code](https://github.com/pkuanjie/ArtFlow) 
* [Lipstick ain't enough: Beyond Color Matching for In-the-Wild Makeup Transfer](https://arxiv.org/abs/2104.01867)
* [Rethinking and Improving the Robustness of Image Style Transfer](https://arxiv.org/abs/2104.05623)<br>:open_mouth:oral 
* [Drafting and Revision: Laplacian Pyramid Network for Fast High-Quality Artistic Style Transfer](https://arxiv.org/abs/2104.05376)<br>:star:[code](https://github.com/PaddlePaddle/PaddleGAN/)
* [Style-Aware Normalized Loss for Improving Arbitrary Style Transfer](https://arxiv.org/abs/2104.10064)<br>:open_mouth:oral

<a name="64"/>

## 64.Speech processing(è¯­éŸ³å¤„ç†)
  
  * [Can audio-visual integration strengthen robustness under multimodal attacks?](https://arxiv.org/abs/2104.02000)<br>:star:[code](https://github.com/YapengTian/AV-Robustness-CVPR21)
* ç«‹ä½“éŸ³é¢‘ç”Ÿæˆ
  * [Visually Informed Binaural Audio Generation without Binaural Audios](https://arxiv.org/abs/2104.06162)<br>:star:[code](https://github.com/SheldonTsui/PseudoBinaural_CVPR2021):house:[project](https://sheldontsui.github.io/projects/PseudoBinaural):tv:[video](https://youtu.be/r-uC2MyAWQc)
* è§†å¬åˆ†ç¦»
  * [Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation](https://arxiv.org/abs/2104.02775)<br>:house:[project](https://caffnet.github.io/):tv:[video](https://youtu.be/9R2qQ7dGTp8)
  * [Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation](https://arxiv.org/abs/2104.02026)<br>:star:[code](https://github.com/YapengTian/CCOL-CVPR21)

<a name="63"/>

## 63.Image Processing(å›¾åƒå¤„ç†)
* å›¾åƒä¿¡å·å¤„ç†
  * [Invertible Image Signal Processing](https://arxiv.org/abs/2103.15061)<br>:star:[code](https://github.com/yzxing87/Invertible-ISP):house:[project](https://yzxing87.github.io/InvISP/index.html)
* å…‰è°±é‡å»º
  * [Tuning IR-cut Filter for Illumination-aware Spectral Reconstruction from RGB](https://arxiv.org/abs/2103.14708)<br>:open_mouth:oral

<a name="62"/>

## 62.Free-Hand Sketches(æ‰‹ç»˜è‰å›¾è¯†åˆ«)
  * [Cloud2Curve: Generation and Vectorization of Parametric Sketches](https://arxiv.org/abs/2103.15536)

<a name="61"/>

## 61.ç®—æ³•
* å› æœæ¨ç†ç®—æ³•
  * [ACRE: Abstract Causal REasoning Beyond Covariation](https://arxiv.org/abs/2103.14232)<br>:star:[code](https://github.com/WellyZhang/ACRE):house:[project](http://wellyzhang.github.io/project/acre.html)
* æŠ½è±¡æ—¶ç©ºæ¨ç†ç®—æ³•
  * [Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution](https://arxiv.org/abs/2103.14230)<br>:star:[code](https://github.com/WellyZhang/PrAE):house:[project](http://wellyzhang.github.io/project/prae.html)

<a name="60"/>

## 60. SLAM/AR/æœºå™¨äºº
* [Tangent Space Backpropagation for 3D Transformation Groups](https://arxiv.org/abs/2103.12032)<br>:star:[code](https://github.com/princeton-vl/lietorch)
* è§†è§‰é‡Œç¨‹è®¡
  * [Generalizing to the Open World: Deep Visual Odometry with Online Adaptation](https://arxiv.org/abs/2103.15279)
* æœºå™¨äºº
  * [Visual Room Rearrangement](https://arxiv.org/abs/2103.16544)<br>:open_mouth:oral:house:[project](https://ai2thor.allenai.org/rearrangement/):tv:[video](https://www.youtube.com/watch?v=1APxaOC9U-A)
  * [GATSBI: Generative Agent-centric Spatio-temporal Object Interaction](https://arxiv.org/abs/2104.04275)<br>:open_mouth:oral:star:[code](https://github.com/mch5048/gatsbi):tv:[video](https://www.youtube.com/watch?v=3urXFiU9kao)
  * [DexYCB: A Benchmark for Capturing Hand Grasping of Objects](https://arxiv.org/abs/2104.04631)<br>:star:[code](https://github.com/NVlabs/dex-ycb-toolkit):house:[project](https://dex-ycb.github.io/):tv:[video](https://youtu.be/Q4wyBaZeBw0)
  * [ContactOpt: Optimizing Contact to Improve Grasps](https://arxiv.org/abs/2104.07267)<br>:star:[code](https://github.com/facebookresearch/contactopt)<br>æœºå™¨äººæ‰‹æŠ“å–
  * [ManipulaTHOR: A Framework for Visual Object Manipulation](https://arxiv.org/abs/2104.11213)<br>:open_mouth:oral:star:[code](https://github.com/allenai/manipulathor/):house:[project](https://ai2thor.allenai.org/manipulathor/):tv:[video](https://www.youtube.com/watch?v=nINZ52nlzX0)
  * è§†è§‰å¯¼èˆª
    * [Pushing it out of the Way: Interactive Visual Navigation](https://arxiv.org/abs/2104.14040)<br>:house:[project](https://prior.allenai.org/projects/interactive-visual-navigation):tv:[video](https://www.youtube.com/watch?v=GvTI5XCMvPw)
    * [Differentiable SLAM-net: Learning Particle SLAM for Visual Navigation](https://arxiv.org/abs/2105.07593)<br>:house:[project](https://sites.google.com/view/slamnet):tv:[video](https://youtu.be/dk1fdtf3fNI)
* AR
  * Stay Positive: Non-Negative Image Synthesis for Augmented Reality<br>:open_mouth:oral
  * è™šæ‹Ÿè¯•ç©¿
    * [VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization](https://arxiv.org/abs/2103.16874)
    * [Self-Supervised Collision Handling via Generative 3D Garment Models for Virtual Try-On](https://arxiv.org/abs/2105.06462)<br>:house:[project](http://mslab.es/projects/SelfSupervisedGarmentCollisions/):tv:[video](https://youtu.be/9AnBNco6i2U)

<a name="59"/>

## 59.(æ·±åº¦å­¦ä¹ æ¨¡å‹)
  * [Dynamic Slimmable Network](https://arxiv.org/abs/2103.13258)<br>:open_mouth:oral:star:[code](https://github.com/changlin31/DS-Net)
  * [Towards Evaluating and Training Verifiably Robust Neural Networks](https://arxiv.org/abs/2104.00447)<br>:open_mouth:oral:star:[code](https://github.com/ZhaoyangLyu/VerifiablyRobustNN)
  * [Activate or Not: Learning Customized Activation](https://arxiv.org/abs/2009.04759)<br>ç²—è§£ï¼š[4](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)<br>è§£è¯»ï¼š[CVPR 2021 | è‡ªé€‚åº”æ¿€æ´»å‡½æ•°ACON: ç»Ÿä¸€ReLUå’ŒSwishçš„æ–°èŒƒå¼](https://mp.weixin.qq.com/s/pbeA2w54MZ_-wXsmGoo3hg)

<a name="58"/>

## 58.Metric Learning(åº¦é‡å­¦ä¹ /ç›¸ä¼¼åº¦å­¦ä¹ )
  * [Dynamic Metric Learning: Towards a Scalable Metric Space to Accommodate Multiple Semantic Scales](https://arxiv.org/abs/2103.11781)<br>:star:[code](https://github.com/SupetZYK/DynamicMetricLearning)
  * [Embedding Transfer with Label Relaxation for Improved Metric Learning](https://arxiv.org/abs/2103.14908)
  * [Noise-resistant Deep Metric Learning with Ranking-based Instance Selection](https://arxiv.org/abs/2103.16047)<br>:star:[code](https://github.com/alibaba-edu/Ranking-based-Instance-Selection)
  
<a name="57"/>

## 57.Sign Language Recognition(æ‰‹è¯­è¯†åˆ«)
  * [Skeleton Based Sign Language Recognition Using Whole-body Keypoints](https://arxiv.org/abs/2103.08833)<br>:star:[code](https://github.com/jackyjsy/CVPR21Chal-SLR)
  * [Read and Attend: Temporal Localisation in Sign Language Videos](https://arxiv.org/abs/2103.16481)<br>:house:[project](https://www.robots.ox.ac.uk/~vgg/research/bslattend/)
  * [Fingerspelling Detection in American Sign Language](https://arxiv.org/abs/2104.01291)

<a name="56"/>

## 56.Computational Photography(å…‰å­¦ã€å‡ ä½•ã€å…‰åœºæˆåƒã€è®¡ç®—æ‘„å½±)
  * [Deep Gaussian Scale Mixture Prior for Spectral Compressive Imaging](https://arxiv.org/abs/2103.07152)<br>:star:[code](https://github.com/TaoHuang95/DGSMP):house:[project](https://see.xidian.edu.cn/faculty/wsdong/Projects/DGSM-SCI.htm)
  * [Mask-ToF: Learning Microlens Masks for Flying Pixel Correction in Time-of-Flight Imaging](https://arxiv.org/abs/2103.16693)<br>:house:[project](https://light.princeton.edu/publication/mask-tof/)
  * [Passive Inter-Photon Imaging](https://arxiv.org/abs/2104.00059)<br>:open_mouth:oral
  * [Shape and Material Capture at Home](https://arxiv.org/abs/2104.06397)<br>:star:[code](https://github.com/dlichy/ShapeAndMaterial):house:[project](https://dlichy.github.io/ShapeAndMaterialAtHome/)
  * [Event-based Synthetic Aperture Imaging with a Hybrid Network](https://arxiv.org/abs/2103.02376)<br>åˆ†äº«ä¼š
* ç›¸æœºå§¿åŠ¿
  * [Fusing the Old with the New: Learning Relative Camera Pose with Geometry-Guided Uncertainty](https://arxiv.org/abs/2104.08278)<br>:open_mouth:oral

<a name="55"/>

## 55.Graph Matching(å›¾åŒ¹é…)
  * [Deep Graph Matching under Quadratic Constraint](https://arxiv.org/abs/2103.06643)

<a name="54"/>

## 54.Emotion Perception(æƒ…ç»ªæ„ŸçŸ¥/æƒ…æ„Ÿé¢„æµ‹)
  * [Affect2MM: Affective Analysis of Multimedia Content Using Emotion Causality](https://arxiv.org/abs/2103.06541)<br>:house:[project](https://gamma.umd.edu/researchdirections/affectivecomputing/affect2mm/)

<a name="53"/>

## 53.Dataset(æ•°æ®é›†)
  * [Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](https://arxiv.org/abs/2102.08981)<br>:sunflower:[dataset](https://github.com/google-research-datasets/conceptual-12m)
  * [Sewer-ML: A Multi-Label Sewer Defect Classification Dataset and Benchmark](https://arxiv.org/abs/2103.10895)<br>:house:[project](https://vap.aau.dk/sewer-ml/)
  * [Benchmarking Representation Learning for Natural World Image Collections](https://arxiv.org/abs/2103.16483)<br>:sunflower:[dataset](https://github.com/visipedia/newt)
  * [SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction from Video Data](https://arxiv.org/abs/2105.08612)<br>:open_mouth:oral:sunflower:[dataset](http://sailvos.web.illinois.edu/_site/index.html)
 * äººè„¸å›¾åƒä¿®é¥°æ•°æ®é›†
  * [PPR10K: A Large-Scale Portrait Photo Retouching Dataset with Human-Region Mask and Group-Level Consistency](https://arxiv.org/abs/2105.09180)<br>:star:[code](https://github.com/csjliang/PPR10K)

<a name="52"/>

## 52. Image Generation/Synthesis(å›¾åƒç”Ÿæˆ)

- [Spatially-Adaptive Pixelwise Networks for Fast Image Translation](https://arxiv.org/abs/2012.02992)<br>:house:[project](https://tamarott.github.io/ASAPNet_web/)<br>é‡‡ç”¨è¶…ç½‘ç»œå’Œéšå¼å‡½æ•°ï¼Œæå¿«çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘é€Ÿåº¦ï¼ˆæ¯”åŸºçº¿å¿«18å€ï¼‰
- [Image Generators with Conditionally-Independent Pixel Synthesis](https://arxiv.org/abs/2011.13775)<br>:open_mouth:oral:star:[code](https://github.com/saic-mdal/CIPS)
* [Im2Vec: Synthesizing Vector Graphics without Vector Supervision](https://arxiv.org/abs/2102.02798)<br>:open_mouth:oral:star:[code](https://github.com/preddy5/Im2Vec):house:[project](http://geometry.cs.ucl.ac.uk/projects/2021/im2vec/)
* [Context-Aware Layout to Image Generation with Enhanced Object Appearance](https://arxiv.org/abs/2103.11897)<br>:star:[code](https://github.com/wtliao/layout2img) 
* [Adversarial Generation of Continuous Images](https://arxiv.org/pdf/2011.12026.pdf)<br>:star:[code](https://github.com/universome/inr-gan)
* [StEP: Style-based Encoder Pre-training for Multi-modal Image Synthesis](https://arxiv.org/abs/2104.07098)
* [IMAGINE: Image Synthesis by Image-Guided Model Inversion](https://arxiv.org/abs/2104.05895)
* Leveraging Line-point Consistence to Preserve Structures forWide Parallax Image Stitching<br>:star:[code](https://github.com/dut-media-lab/Image-Stitching)<br>åˆ†äº«ä¼š

<a name="51"/>

## 51.Contrastive Learning(å¯¹æ¯”å­¦ä¹ )
* [AdCo: Adversarial Contrast for Efficient Learning of Unsupervised Representations from Self-Trained Negative Adversaries](https://arxiv.org/abs/2011.08435)<br>:star:[code](https://arxiv.org/abs/2011.08435)<br>è§£è¯»:[CVPR 2021æ¥æ”¶è®ºæ–‡ï¼šAdCoåŸºäºå¯¹æŠ—çš„å¯¹æ¯”å­¦ä¹ ](https://mp.weixin.qq.com/s/u7Lhzh8uYEEHfWiM32-4yQ)
* [LAFEAT: Piercing Through Adversarial Defenses with Latent Features](https://arxiv.org/abs/2104.09284)<br>:open_mouth:oral
* [Distilling Audio-Visual Knowledge by Compositional Contrastive Learning](https://arxiv.org/abs/2104.10955)<br>:star:[code](https://github.com/yanbeic/CCL)
 
<a name="50"/>

## 50.OCR

* [Fourier Contour Embedding for Arbitrary-Shaped Text Detection](https://arxiv.org/abs/2104.10442)
* åœºæ™¯æ–‡æœ¬æ£€æµ‹
  * [What If We Only Use Real Datasets for Scene Text Recognition? Toward Scene Text Recognition With Fewer Labels](https://arxiv.org/abs/2103.04400)
  * [Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition](https://arxiv.org/abs/2103.06495)<br>:open_mouth:oral:star:[code](https://github.com/FangShancheng/ABINet)
  * [MOST: A Multi-Oriented Scene Text Detector with Localization Refinement](https://arxiv.org/abs/2104.01070)
  * [Scene Text Retrieval via Joint Text Detection and Similarity Learning](https://arxiv.org/abs/2104.01552)<br>:star:[code](https://github.com/lanfeng4659/STR-TDSL)
  * [TextOCR: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text](https://arxiv.org/abs/2105.05486)<br>:house:[project](https://textvqa.org/textocr)
* æ‰‹å†™æ–‡æœ¬è¯†åˆ«
  * [MetaHTR: Towards Writer-Adaptive Handwritten Text Recognition](https://arxiv.org/abs/2104.01876)

<a name="49"/>

## 49.Adversarial Learning(å¯¹æŠ—å­¦ä¹ )

- [Simulating Unknown Target Models for Query-Efficient Black-box Attacks](https://arxiv.org/abs/2009.00960)<br>:star:[code](https://github.com/machanic/MetaSimulator)<br>é»‘ç›’å¯¹æŠ—æ”»å‡»
- [Delving into Data: Effectively Substitute Training for Black-box Attack](https://arxiv.org/abs/2104.12378)<br>åŸºäºé«˜æ•ˆè®­ç»ƒæ›¿ä»£æ¨¡å‹çš„é»‘ç›’æ”»å‡»æ–¹æ³•<br>è§£è¯»ï¼š[8](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
- [LiBRe: A Practical Bayesian Approach to Adversarial Detection](https://arxiv.org/abs/2103.14835)<br>:star:[code](https://github.com/thudzj/ScalableBDL)

<a name="48"/>

## 48.Image Representation(å›¾åƒè¡¨ç¤º)

- [Learning Continuous Image Representation with Local Implicit Image Function](https://arxiv.org/abs/2012.09161)<br>:open_mouth:oral:star:[code](https://github.com/yinboc/liif):house:[project](https://yinboc.github.io/liif/):tv:[video](https://youtu.be/6f2roieSY_8)

<a name="47"/>

## 47.Vision-Language(è§†è§‰è¯­è¨€)

- [Structured Scene Memory for Vision-Language Navigation](https://arxiv.org/abs/2103.03454)<br>:star:[code](https://github.com/HanqingWangAI/SSM-VLN)
* [Kaleido-BERT: Vision-Language Pre-training on Fashion Domain](https://arxiv.org/abs/2103.16110)<br>
* [Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning](https://arxiv.org/abs/2104.03135)

<a name="46"/>

## 46.Human-Object Interaction(äººç‰©äº¤äº’)

- [Learning Asynchronous and Sparse Human-Object Interaction in Videos](https://arxiv.org/abs/2103.02758)
- [QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information](https://arxiv.org/abs/2103.05399)<br>:star:[code](https://github.com/hitachi-rd-cv/qpic)
- [Reformulating HOI Detection as Adaptive Set Prediction](https://arxiv.org/abs/2103.05983)<br>:star:[code](https://github.com/yoyomimi/AS-Net)
* [Detecting Human-Object Interaction via Fabricated Compositional Learning](https://arxiv.org/abs/2103.08214)<br>:star:[code](https://github.com/zhihou7/FCL)
* [Affordance Transfer Learning for Human-Object Interaction Detection](https://arxiv.org/abs/2104.02867)<br>:star:[code](https://github.com/zhihou7/HOI-CL)
* [Glance and Gaze: Inferring Action-aware Points for One-Stage Human-Object Interaction Detection](https://arxiv.org/abs/2104.05269)<br>:star:[code](https://github.com/SherlockHolmes221/GGNet)

<a name="45"/>

## 45.Camera Localization(ç›¸æœºå®šä½)

- [Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments](https://arxiv.org/abs/2012.04746)<br>:open_mouth:oral
- [Back to the Feature: Learning Robust Camera Localization from Pixels to Pose](https://arxiv.org/abs/2103.09213)<br>:star:[code](https://github.com/cvg/pixloc)
- [Learning Camera Localization via Dense Scene Matching](https://arxiv.org/abs/2103.16792)<br>:star:[code](https://github.com/Tangshitao/Dense-Scene-Matching)
* è§†è§‰å®šä½
  * [VS-Net: Voting with Segmentation for Visual Localization](https://arxiv.org/abs/2105.10886)<br>:star:[code](https://github.com/zju3dv/VS-Net):house:[project](https://drinkingcoder.github.io/publication/vs-net/):tv:[video](https://youtu.be/5WLEyyLdxAs)

<a name="44"/>

## 44. Image/video Captioning(å›¾åƒ/è§†é¢‘å­—å¹•)

- [Scan2Cap: Context-aware Dense Captioning in RGB-D Scans](https://arxiv.org/abs/2012.02206)<br>:star:[code](https://github.com/daveredrum/Scan2Cap):house:[project](https://daveredrum.github.io/Scan2Cap/):tv:[video](https://youtu.be/AgmIpDbwTCY)
- [VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs](https://arxiv.org/abs/2101.12059)<br>è§†é¢‘å­—å¹•ã€è§†é¢‘é—®ç­”å’Œè§†é¢‘å¯¹è¯ä»»åŠ¡çš„å¤šæ¨¡å¼æ¡†æ¶
- [Open-book Video Captioning with Retrieve-Copy-Generate Network](https://arxiv.org/abs/2103.05284)
* å›¾åƒå­—å¹•
  * [Human-like Controllable Image Captioning with Verb-specific Semantic Roles](https://arxiv.org/abs/2103.12204)<br>:star:[code](https://github.com/mad-red/VSR-guided-CIC)
  * [Towards Accurate Text-based Image Captioning with Content Diversity Exploration](https://arxiv.org/abs/2105.03236)<br>:star:[code](https://github.com/guanghuixu/AnchorCaptioner)

<a name="43"/>

## 43.Active Learning(ä¸»åŠ¨å­¦ä¹ )

- [Vab-AL: Incorporating Class Imbalance and Difficulty with Variational Bayes for Active Learning](https://arxiv.org/abs/2003.11249)

<a name="42"/>

## 42.Scene Flow Estimation(åœºæ™¯æµä¼°è®¡)
* åœºæ™¯æµä¼°è®¡
  * [Self-Supervised Multi-Frame Monocular Scene Flow](https://arxiv.org/abs/2105.02216)<br>:star:[code](https://github.com/visinf/multi-mono-sf)
  * [HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs and Position-aware Flow Embedding](https://arxiv.org/abs/2105.07751)
  * [Self-Point-Flow: Self-Supervised Scene Flow Estimation from Point Clouds with Optimal Transport and Random Walk](https://arxiv.org/abs/2105.08248)<br>:open_mouth:oral

<a name="41"/>

## 41. Representation Learning(è¡¨ç¤ºå­¦ä¹ ï¼ˆå›¾åƒ+å­—å¹•ï¼‰)

- [VirTex: Learning Visual Representations from Textual Annotations](https://arxiv.org/abs/2006.06666)<br>:star:[code](https://github.com/kdexd/virtex)
- [Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566)<br>:open_mouth:oral
- [Representation Learning via Global Temporal Alignment and Cycle-Consistency](https://arxiv.org/abs/2105.05217)<br>:star:[code](https://github.com/hadjisma/VideoAlignment)

<a name="40"/>

## 40.Superpixel (è¶…åƒç´ )

- [Learning the Superpixel in a Non-iterative and Lifelong Manner](https://arxiv.org/abs/2103.10681)<br>:star:[code](https://github.com/zh460045050/LNSNet)

<a name="39"/>

## 39.Debiasing(å»åè§)

- [Fair Attribute Classification through Latent Space De-biasing](https://arxiv.org/abs/2012.01469)<br>:star:[code](https://github.com/princetonvisualai/gan-debiasing):house:[project](https://princetonvisualai.github.io/gan-debiasing/)<br>

<a name="38"/>

## 38.Class-Incremental learning(ç±»å¢é‡å­¦ä¹ )

- [IIRC: Incremental Implicitly-Refined Classification](https://arxiv.org/abs/2012.12477)<br>:house:[project](https://chandar-lab.github.io/IIRC/)<br>
- [Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2103.04059)
- [DER: Dynamically Expandable Representation for Class Incremental Learning](https://arxiv.org/abs/2103.16788)<br>:star:[code](https://github.com/Rhyssiyan/DER-ClassIL.pytorch)
- [Few-Shot Incremental Learning with Continually Evolved Classifiers](https://arxiv.org/abs/2104.03047)
* [On Learning the Geodesic Path for Incremental Learning](https://arxiv.org/abs/2104.08572)<br>:star:[code](https://github.com/chrysts/geodesic_continual_learning)

<a name="37"/>

## 37. Continual Learning(æŒç»­å­¦ä¹ )

- Rainbow Memory: Continual Learning with a Memory of Diverse Samples<br>
- [Training Networks in Null Space for Continual Learning]()<br>:open_mouth:oral:star:[code](https://github.com/ShipengWang/Adam-NSCL)
* [Efficient Feature Transformations for Discriminative and Generative Continual Learning](https://arxiv.org/abs/2103.13558)
* [Rainbow Memory: Continual Learning with a Memory of Diverse Samples](https://arxiv.org/abs/2103.17230) 
* [Rectification-based Knowledge Retention for Continual Learning](https://arxiv.org/abs/2103.16597) 
* [Layerwise Optimization by Gradient Decomposition for Continual Learning](https://arxiv.org/abs/2105.07561)

<a name="36"/>

## 36.Action Detection and Recognition(åŠ¨ä½œæ£€æµ‹ä¸è¯†åˆ«)

- [Coarse-Fine Networks for Temporal Activity Detection in Videos](https://arxiv.org/abs/2103.01302)<br>
- [3D CNNs with Adaptive Temporal Feature Resolutions](https://arxiv.org/abs/2011.08652)<br> 
- [Understanding the Robustness of Skeleton-based Action Recognition under Adversarial Attack](https://arxiv.org/abs/2103.05347)
- [BASAR:Black-box Attack on Skeletal Action Recognition](https://arxiv.org/abs/2103.05266)<br>:tv:[video](https://www.youtube.com/watch?v=PjWgwnAkV8g)
- [TDN: Temporal Difference Networks for Efficient Action Recognition]( https://arxiv.org/abs/2012.10071)<br>:star:[code](https://github.com/MCG-NJU/TDN)
- [ACTION-Net: Multipath Excitation for Action Recognition](https://arxiv.org/abs/2103.07372)<br>:star:[code](https://github.com/V-Sense/ACTION-Net)<br>è§£è¯»ï¼š[CVPR 2021 | ç”¨äºåŠ¨ä½œè¯†åˆ«ï¼Œå³æ’å³ç”¨ã€æ··åˆæ³¨æ„åŠ›æœºåˆ¶çš„ ACTION æ¨¡å—](https://mp.weixin.qq.com/s/L2_lkhKbVhW8fjAaDdsyWQ)<br>è§£è¯»ï¼š[CVPR 2021 ï½œé’ˆå¯¹å¼ºæ—¶åºä¾èµ–ï¼Œå³æ’å³ç”¨ã€æ··åˆæ³¨æ„åŠ›æœºåˆ¶çš„ ACTION æ¨¡å—](https://mp.weixin.qq.com/s/tonyk649KzU1Y_c6p8isuQ)
- [No frame left behind: Full Video Action Recognition](https://arxiv.org/abs/2103.15395)
* [Recognizing Actions in Videos from Unseen Viewpoints](https://arxiv.org/abs/2103.16516)
* [Beyond Short Clips: End-to-End Video-Level Learning with Collaborative Memories](https://arxiv.org/abs/2104.01198)
* [Motion Representations for Articulated Animation](https://arxiv.org/abs/2104.11280)<br>:star:[code](https://github.com/snap-research/articulated-animation):house:[project](https://snap-research.github.io/articulated-animation/):tv:[video](https://www.youtube.com/watch?v=gpBYN8t8_yY)
* [Home Action Genome: Cooperative Compositional Action Understanding](https://arxiv.org/abs/2105.05226)
* æ—¶åºåŠ¨ä½œå®šä½
  * [Modeling Multi-Label Action Dependencies for Temporal Action Localization](https://arxiv.org/abs/2103.03027)<br>:open_mouth:oral<br>æå‡ºåŸºäºæ³¨æ„åŠ›çš„ç½‘ç»œæ¶æ„æ¥å­¦ä¹ è§†é¢‘ä¸­çš„åŠ¨ä½œä¾èµ–æ€§ï¼Œç”¨äºè§£å†³å¤šæ ‡ç­¾æ—¶é—´åŠ¨ä½œå®šä½ä»»åŠ¡ã€‚
  * Learning Salient Boundary Feature for Anchor-free Temporal Action Localization<br>åŸºäºæ˜¾è‘—è¾¹ç•Œç‰¹å¾å­¦ä¹ çš„æ— é”šæ¡†æ—¶åºåŠ¨ä½œå®šä½<br>è§£è¯»ï¼š[10](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [The Blessings of Unlabeled Background in Untrimmed Videos](https://arxiv.org/abs/2103.13183)
  * [Temporal Context Aggregation Network for Temporal Action Proposal Refinement](https://arxiv.org/abs/2103.13141)
  * [Learning Salient Boundary Feature for Anchor-free Temporal Action Localization](https://arxiv.org/abs/2103.13137)
  * [CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning](https://arxiv.org/abs/2103.16392)
  * [Action Unit Memory Network for Weakly Supervised Temporal Action Localization](https://arxiv.org/abs/2104.14135)
* Video Actor Segmentation
  * [Collaborative Spatial-Temporal Modeling for Language-Queried Video Actor Segmentation](https://arxiv.org/abs/2105.06818)

<a name="35"/>

## 35.Image Clustering(å›¾åƒèšç±») 

- [Improving Unsupervised Image Clustering With Robust Learning](https://arxiv.org/abs/2012.11150)<br>:star:[code](https://github.com/deu30303/RUC)<br>åˆ©ç”¨é²æ£’å­¦ä¹ æ”¹è¿›æ— ç›‘ç£å›¾åƒèšç±»æŠ€æœ¯<br>
- [Jigsaw Clustering for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2104.00323)<br>:open_mouth:oral:star:[code](https://github.com/Jia-Research-Lab/JigsawClustering)

<a name="34"/>

## 34.Image Classification(å›¾åƒåˆ†ç±»)

- [Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels](https://arxiv.org/abs/2101.05022)<br>:star:[code](https://github.com/naver-ai/relabel_imagenet)<br>
- [Differentiable Patch Selection for Image Recognition](https://arxiv.org/abs/2104.03059)<br>:star:[code](https://github.com/google-research/google-research/tree/master/ptopk_patch_selection/)
* ç»†ç²’åº¦åˆ†ç±»
  * [Fine-grained Angular Contrastive Learning with Coarse Labels](https://arxiv.org/abs/2012.03515)<br>:open_mouth:oral<br>ä½¿ç”¨è‡ªç›‘ç£è¿›è¡Œ Coarse Labelsï¼ˆç²—æ ‡ç­¾ï¼‰çš„ç»†ç²’åº¦åˆ†ç±»æ–¹é¢çš„å·¥ä½œã€‚ç²—æ ‡ç­¾ä¸ç»†ç²’åº¦æ ‡ç­¾ç›¸æ¯”ï¼Œæ›´å®¹æ˜“å’Œæ›´ä¾¿å®œï¼Œå› ä¸ºç»†ç²’åº¦æ ‡ç­¾é€šå¸¸éœ€è¦åŸŸä¸“å®¶ã€‚
  * Graph-based High-Order Relation Discovery for Fine-grained Recognition<br>åŸºäºç‰¹å¾é—´é«˜é˜¶å…³ç³»æŒ–æ˜çš„ç»†ç²’åº¦è¯†åˆ«æ–¹æ³•<br>è§£è¯»ï¼š[20](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * Fine-Grained Few-Shot Classification with Feature Map Reconstruction Networks
  * [A Realistic Evaluation of Semi-Supervised Learning for Fine-Grained Classification](https://arxiv.org/abs/2104.00679)<br>:open_mouth:oral
* å›¾åƒåˆ†ç±»
  * [MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition](https://arxiv.org/abs/2103.12579)
  * [PML: Progressive Margin Loss for Long-tailed Age Classification](https://arxiv.org/abs/2103.02140)<br>
  * [Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification](https://arxiv.org/abs/2103.14267)<br>:house:[project](https://www.kaihan.org/HybridLT/)
  * [Capsule Network is Not More Robust than Convolutional Network](https://arxiv.org/abs/2103.15459)
  * [Model-Contrastive Federated Learning](https://arxiv.org/abs/2103.16257)
  * [Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets](https://arxiv.org/abs/2104.12690)<br>:open_mouth:oral:star:[code](https://github.com/fidler-lab/efficient-annotation-cookbook):house:[project](https://fidler-lab.github.io/efficient-annotation-cookbook/)
  * [Correlated Input-Dependent Label Noise in Large-Scale Image Classification](https://arxiv.org/abs/2105.10305)<br>:open_mouth:oral:star:[code](https://github.com/google/uncertainty-baselines/tree/master/baselines/imagenet)
* åŠç›‘ç£å›¾åƒåˆ†ç±»
  * [SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification](https://arxiv.org/abs/2103.16725)<br>:star:[code](https://github.com/zijian-hu/SimPLE)
* é•¿å°¾è§†è§‰è¯†åˆ«
  * [Distribution Alignment: A Unified Framework for Long-tail Visual Recognition](https://arxiv.org/abs/2103.16370)<br>:star:[code](https://github.com/Megvii-BaseDetection/DisAlign)
  * [Improving Calibration for Long-Tailed Recognition](https://arxiv.org/abs/2104.00466)<br>:star:[code](https://github.com/Jia-Research-Lab/MiSLAS)
  * [Adversarial Robustness under Long-Tailed Distribution](https://arxiv.org/abs/2104.02703)<br>:open_mouth:oral:star:[code](https://github.com/wutong16/Adversarial_Long-Tail)

<a name="33"/>

## 33.6D Pose Estimation(6Dä½å§¿ä¼°è®¡)

- [FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation](https://arxiv.org/abs/2103.02242)<br>:open_mouth:oral:star:[code](https://github.com/ethnhe/FFB6D)<br>
- [GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation](http://arxiv.org/abs/2102.12145)<br>:star:[code](https://github.com/THU-DA-6D-Pose-Group/GDR-Net)
- [FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism](https://arxiv.org/abs/2103.07054)<br>:open_mouth:oral:star:[code](https://github.com/DC1991/FS-Net)
- [Wide-Depth-Range 6D Object Pose Estimation in Space](https://arxiv.org/abs/2104.00337)<br>:star:[code](https://github.com/cvlab-epfl/wide-depth-range-pose)
- [DSC-PoseNet: Learning 6DoF Object Pose Estimation via Dual-scale Consistency](https://arxiv.org/abs/2104.03658)
* [Single-view robot pose and joint angle estimation via render & compare](https://arxiv.org/abs/2104.09359)<br>:open_mouth:oral:star:[code](https://github.com/ylabbe/robopose):house:[project](https://www.di.ens.fr/willow/research/robopose/):tv:[video](https://www.youtube.com/watch?v=3yzwS99sgLI)

<a name="32"/>

## 32.View Synthesis(è§†å›¾åˆæˆ)

- [ID-Unet: Iterative Soft and Hard Deformation for View Synthesis](https://arxiv.org/abs/2103.02264)<br>:open_mouth:oral
- [NeX: Real-time View Synthesis with Neural Basis Expansion](https://arxiv.org/abs/2103.05606)<br>:open_mouth:oral:house:[project](https://nex-mpi.github.io/):tv:[video](https://www.youtube.com/watch?v=HyfkF7Z-ddA)<br>åˆ©ç”¨ç¥ç»åŸºç¡€æ‰©å±•çš„å®æ—¶è§†å›¾åˆæˆæŠ€æœ¯
- [Layout-Guided Novel View Synthesis from a Single Indoor Panorama](https://arxiv.org/abs/2103.17022)<br>:star:[code](https://github.com/bluestyle97/PNVS)
- [Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes](https://arxiv.org/abs/2104.06935)

<a name="31"/>

## 31.Open-Set Recognition(å¼€æ”¾é›†è¯†åˆ«)

- [Counterfactual Zero-Shot and Open-Set Visual Recognition](https://arxiv.org/abs/2103.00887)<br>:star:[code](https://github.com/yue-zhongqi/gcm-cf)<br>
- [Few-shot Open-set Recognition by Transformation Consistency](https://arxiv.org/abs/2103.01537)<br>
- [Learning Placeholders for Open-Set Recognition](https://arxiv.org/abs/2103.15086)<br>:open_mouth:oral

<a name="30"/>

## 30.Neural rendering(ç¥ç»æ¸²æŸ“)

- [DeRF: Decomposed Radiance Fields](https://arxiv.org/abs/2011.12490)<br>:house:[project](https://ubc-vision.github.io/derf/)<br>
- [D-NeRF: Neural Radiance Fields for Dynamic Scenes](https://arxiv.org/abs/2011.13961)<br>:house:[project](https://www.albertpumarola.com/research/D-NeRF/index.html)<br>
* [Neural Lumigraph Rendering](https://arxiv.org/abs/2103.11571)<br>:sunflower:[dataset](https://drive.google.com/file/d/1BBpIfrqwZNYmG1TiFljlCnwsmL2OUxNT/view):house:[project](http://www.computationalimaging.org/publications/nlr/):tv:[video](https://www.youtube.com/watch?v=maVF-7x9644)<br>æ–¯å¦ç¦å¤§å­¦
* [AutoInt: Automatic Integration for Fast Neural Volume Rendering](https://arxiv.org/abs/2012.01714)<br>:open_mouth:oral:house:[project](http://www.computationalimaging.org/publications/automatic-integration/):tv:[video](https://youtu.be/GYxFYbih0PU)<br>æ–¯å¦ç¦å¤§å­¦
* [pixelNeRF: Neural Radiance Fields from One or Few Images](https://arxiv.org/abs/2012.02190)<br>:star:[code](https://github.com/sxyu/pixel-nerf):house:[project](https://alexyu.net/pixelnerf/):tv:[video](https://youtu.be/voebZx7f32g)
* [IBRNet: Learning Multi-View Image-Based Rendering](https://arxiv.org/abs/2102.13090)<br>:house:[project](https://ibrnet.github.io/)<br>å¤‡æ³¨ï¼šæœ‰å­¦è€…è¯„è®ºpixelNeRFå’ŒIBRNetçš„å·¥ä½œæ€æƒ³ç›¸è¿‘ï¼Œä½†IBRNetä¼¼ä¹æ›´åŠ æˆç†Ÿã€‚
* [Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans](https://arxiv.org/abs/2012.15838)<br>:star:[code](https://github.com/zju3dv/neuralbody):house:[project](https://zju3dv.github.io/neuralbody/):tv:[video](https://youtu.be/BPCAMeBCE-8)<br>æµ™å¤§ç­‰å­¦è€…å‘æ˜çš„Neural Bodyç®—æ³•ï¼Œè¾“å…¥å¤šè§’åº¦è§†é¢‘å¯è¾“å‡º3Däººä½“å’Œæ–°è§’åº¦è§†å›¾ã€‚
* [NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis](https://arxiv.org/abs/2012.03927)<br>:house:[project](https://pratulsrinivasan.github.io/nerv/):tv:[video](https://youtu.be/4XyDdvhhjVo)<br>åœ¨ä»»æ„ç…§æ˜æ¡ä»¶ä¸‹ï¼Œæ ¹æ®ä¸€ç»„è¾“å…¥å›¾åƒç”Ÿæˆå®Œæ•´çš„3Dåœºæ™¯
* [Self-Supervised Visibility Learning for Novel View Synthesis](https://arxiv.org/abs/2103.15407)<br>:star:[code](https://github.com/shiyujiao/SVNVS)

<a name="29"/>

## 29.Human Pose Estimation(äººä½“å§¿æ€ä¼°è®¡)

- [Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration](https://arxiv.org/abs/2103.02845)<br>:star:[code](https://github.com/SeanChenxy/HandMesh)<br>
- [Monocular Real-time Full Body Capture with Inter-part Correlations](https://arxiv.org/abs/2012.06087)<br>:tv:[video](https://www.youtube.com/watch?v=pAcywTUTv-E)<br>åœ¨ç”µå½±åŠ¨ä½œç‰¹æ•ˆä¸­ï¼Œäººä½“è¿åŠ¨æ•æ‰æ˜¯å…³é”®æŠ€æœ¯ï¼Œé«˜è´¨é‡çš„æ•æ‰å¾€å¾€éœ€è¦ç‰¹æ®Šè®¾å¤‡ï¼Œè€Œå¦‚æœèƒ½ä½¿ç”¨æ™®é€šRGBç›¸æœºè¿›è¡Œè¿åŠ¨æ•æ‰ï¼Œå°†ä¼šä½¿äººäººéƒ½æ˜¯ç‰¹æ•ˆå¸ˆã€‚è¯¥è§†é¢‘æ¥è‡ªæ¸…åã€é©¬æ™®æ‰€ç­‰å•ä½çš„å­¦è€…å‘è¡¨äºCVPR2021çš„è®ºæ–‡ç»“æœï¼Œä½¿ç”¨å•ç›®RGBç›¸æœºçš„åŠ¨ä½œæ•æ‰ã€‚
- [Behavior-Driven Synthesis of Human Dynamics](https://arxiv.org/abs/2103.04677)<br>:star:[code](https://github.com/CompVis/behavior-driven-video-synthesis):house:[project](https://compvis.github.io/behavior-driven-video-synthesis/)
- [Learning Compositional Representation for 4D Captures with Neural ODE](https://arxiv.org/abs/2103.08271)
- [Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation](https://arxiv.org/abs/2012.15175)<br>:star:[code](https://github.com/greatlog/SWAHR-HumanPose)<br>ç²—è§£ï¼š[2](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
- [Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression](https://arxiv.org/abs/2104.02300)<br>:star:[code](https://github.com/HRNet/DEKR)
- [SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks](https://arxiv.org/abs/2104.03313)<br>:open_mouth:oral:house:[project](https://scanimate.is.tue.mpg.de/)
- [On Self-Contact and Human Pose](https://arxiv.org/abs/2104.03176)<br>:house:[project](https://tuch.is.tue.mpg.de/)
- [Lite-HRNet: A Lightweight High-Resolution Network](https://arxiv.org/abs/2104.06403)<br>:star:[code](https://github.com/HRNet/)<br>è§£è¯»ï¼š[Lite-HRNetï¼šè½»é‡çº§HRNetï¼ŒFLOPså¤§å¹…ä¸‹é™](https://mp.weixin.qq.com/s/4V6EOYVSybMR9oxpcsWv9w)
- [Deep Dual Consecutive Network for Human Pose Estimation](https://arxiv.org/abs/2103.07254)
- [3D Human Action Representation Learning via Cross-View Consistency Pursuit](https://arxiv.org/abs/2104.14466)<br>:star:[code](https://github.com/LinguoLi/CrosSCLR)
- [Body Meshes as Points](https://arxiv.org/abs/2105.02467)<br>:star:[code](https://github.com/jfzhang95/BMP)
- [Unsupervised Human Pose Estimation through Transforming Shape Templates](https://arxiv.org/abs/2105.04154)<br>:house:[project](https://infantmotion.github.io/)
- [When Human Pose Estimation Meets Robustness: Adversarial Algorithms and Benchmarks](https://arxiv.org/abs/2105.06152)
* 3Dæ‰‹éƒ¨é‡å»º
  * [Model-based 3D Hand Reconstruction via Self-Supervised Learning](https://arxiv.org/abs/2103.11703)
* äººä½“è¿åŠ¨è¿ç§»
  * [Few-Shot Human Motion Transfer by Personalized Geometry and Texture Modeling](https://arxiv.org/abs/2103.14338)<br>:star:[code](https://github.com/HuangZhiChao95/FewShotMotionTransfer):tv:[video](https://www.youtube.com/watch?v=ZJ15X-sdKSU)
* Human Volumetric Capture
  * [POSEFusion: Pose-guided Selective Fusion for Single-view Human Volumetric Capture](https://arxiv.org/abs/2103.15331)<br>:open_mouth:oral:house:[project](http://www.liuyebin.com/posefusion/posefusion.html)
* 3Däººä½“å§¿æ€ä¼°è®¡
  * [CanonPose: Self-supervised Monocular 3D Human Pose Estimation in the Wild](https://arxiv.org/abs/2011.14679)
  * Context Modeling in 3D Human Pose Estimation: A Unified Perspective
  * [PCLs: Geometry-aware Neural Reconstruction of 3D Pose with Perspective Crop Layers](https://arxiv.org/abs/2011.13607)<br>:tv:[video](https://twitter.com/i/status/1334395954644930560)<br>é€šè¿‡æ¶ˆé™¤ location-dependent é€è§†æ•ˆæœæ¥æ”¹è¿›3Däººä½“å§¿åŠ¿ä¼°è®¡æŠ€æœ¯å·¥ä½œã€‚<br>
  * [Graph Stacked Hourglass Networks for 3D Human Pose Estimation](https://arxiv.org/abs/2103.16385)
  * [Human POSEitioning System (HPS): 3D Human Pose Estimation and Self-localization in Large Scenes from Body-Mounted Sensors](https://arxiv.org/abs/2103.17265)<br>:open_mouth:oral:house:[project](http://virtualhumans.mpi-inf.mpg.de/hps/)
  * [SimPoE: Simulated Character Control for 3D Human Pose Estimation](https://arxiv.org/abs/2104.00683)<br>:open_mouth:oral:house:[project](https://www.ye-yuan.com/simpoe/)
  * [Reconstructing 3D Human Pose by Watching Humans in the Mirror](https://arxiv.org/abs/2104.00340)<br>:open_mouth:oral:star:[code](https://github.com/zju3dv/Mirrored-Human):house:[project](https://zju3dv.github.io/Mirrored-Human/)
  * [Multi-View Multi-Person 3D Pose Estimation with Plane Sweep Stereo](https://arxiv.org/abs/2104.02273)<br>:star:[code](https://github.com/jiahaoLjh/PlaneSweepPose)
  * [PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation](https://arxiv.org/abs/2105.02465)<br>:open_mouth:oral:star:[code](https://github.com/jfzhang95/PoseAug)
  * [AGORA: Avatars in Geography Optimized for Regression Analysis](https://arxiv.org/abs/2104.14643)<br>:house:[project](https://agora.is.tue.mpg.de/)
* åŠ¨ç‰©å§¿æ€ä¼°è®¡
  * [From Synthetic to Real: Unsupervised Domain Adaptation for Animal Pose Estimation](https://arxiv.org/abs/2103.14843)<br>:open_mouth:oral:star:[code](https://github.com/chaneyddtt/UDA-Animal-Pose):tv:[video](https://www.youtube.com/watch?v=uF8BE9J7wNw)
* 3Däººä½“ç½‘æ ¼é…å‡†
  * [Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration](https://arxiv.org/abs/2104.08160)<br>:star:[code](https://github.com/taconite/PTF):house:[project](https://taconite.github.io/PTF/website/PTF.html):tv:[video](https://youtu.be/XNk4o2Z0S2c)
* å¤šäººäººä½“é‡å»º
  * [Multi-person Implicit Reconstruction from a Single Image](https://arxiv.org/abs/2104.09283)
* 3Däººä½“è¿åŠ¨
  * [We are More than Our Joints: Predicting how 3D Bodies Move](https://arxiv.org/pdf/2012.00619.pdf)<br>:house:[project](https://yz-cnsdqz.github.io/MOJO/MOJO.html):tv:[video](https://youtu.be/5DqLWAb37X0)<br>åˆ†äº«ä¼š
* äººä½“è¿åŠ¨æ•æ‰
  * [Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer RGBD Sensors](http://www.liuyebin.com/Function4D/assets/Function4D.pdf)<br>:open_mouth:oral:house:[project](http://www.liuyebin.com/Function4D/Function4D.html):tv:[video](https://www.youtube.com/watch?v=-rWUn4fEQNU)

<a name="28"/>

## 28.Dense prediction(å¯†é›†é¢„æµ‹)

- [Densely connected multidilated convolutional networks for dense prediction tasks](https://arxiv.org/abs/2011.11844)<br>æå‡ºçš„D3Netåœ¨è¯­ä¹‰åˆ†å‰²&éŸ³ä¹æºåˆ†ç¦»ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºSOTAç½‘ç»œ<br>
- [Dense Contrastive Learning for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2011.09157)<br>:open_mouth:oral:star:[code](https://github.com/WXinlong/DenseCL)
* [Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2011.10043)<br>:star:[code](https://github.com/zdaxie/PixPro)

<a name="27"/>

## 27.Semantic Line Detection(è¯­ä¹‰çº¿æ£€æµ‹)
* [Harmonious Semantic Line Detection via Maximal Weight Clique Selection](https://arxiv.org/abs/2104.06903)<br>:star:[code](https://github.com/dongkwonjin)
 
<a name="26"/>

## 26.Video Processing(è§†é¢‘ç›¸å…³æŠ€æœ¯)
* [Skip-Convolutions for Efficient Video Processing](https://arxiv.org/abs/2104.11487)
* [VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples](https://arxiv.org/abs/2103.05905)<br>:star:[code](https://github.com/tinapan-pt/VideoMoCo)
* [Learning by Aligning Videos in Time](https://arxiv.org/abs/2103.17260)
* [Hierarchical Motion Understanding via Motion Programs](https://arxiv.org/abs/2104.11216)<br>:house:[project](https://sumith1896.github.io/motion2prog/):tv:[video](https://youtu.be/OpyY-s0LKAs)
* [Stochastic Image-to-Video Synthesis using cINNs](https://arxiv.org/abs/2105.04551)<br>:star:[code](https://github.com/CompVis/image2video-synthesis-using-cINNs):house:[project](https://compvis.github.io/image2video-synthesis-using-cINNs/)
* [Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions](https://arxiv.org/abs/2105.04489)<br>:house:[project](http://moments.csail.mit.edu/spoken.html)
* è§†é¢‘æ‘˜è¦
  * [Learning Discriminative Prototypes with Dynamic Time Warping](https://arxiv.org/abs/2103.09458)<br>:star:[code](https://github.com/BorealisAI/TSC-Disc-Proto)
  * [Learning Triadic Belief Dynamics in Nonverbal Communication from Videos](https://arxiv.org/abs/2104.02841)<br>:open_mouth:oral:star:[code](https://github.com/LifengFan/Triadic-Belief-Dynamics)
* è§†é¢‘ç¼–è§£ç 
  * [MetaSCI: Scalable and Adaptive Reconstruction for Video Compressive Sensing](https://arxiv.org/abs/2103.01786)<br>:star:[code](https://github.com/xyvirtualgroup/MetaSCI-CVPR2021)
  * [FVC: A New Framework towards Deep Video Compression in Feature Space](https://arxiv.org/abs/2105.09600)<br>:open_mouth:oral
* è§†é¢‘æ’å¸§
  * [FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation](https://arxiv.org/pdf/2012.08512.pdf)<br>:star:[code](https://tarun005.github.io/FLAVR/Code):house:[project](https://tarun005.github.io/FLAVR/)<br>
  * [Deep Animation Video Interpolation in the Wild](https://arxiv.org/abs/2104.02495)<br>:star:[code](https://github.com/lisiyao21/AnimeInterp/)
* è§†é¢‘è¯­è¨€å­¦ä¹ ï¼ˆvideo-and-language learningï¼‰
  * [Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling](https://arxiv.org/pdf/2102.06183.pdf)<br>:open_mouth:oral:star:[code](https://github.com/jayleicn/ClipBERT)<br>
* è§†é¢‘é¢„æµ‹
  * [Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction](https://arxiv.org/abs/2103.04174)<br>:house:[project](https://sites.google.com/view/ghvae):tv:[video](https://youtu.be/C8_-z8SEGOU)
  * [Learning Semantic-Aware Dynamics for Video Prediction](https://arxiv.org/abs/2104.09762)
* è§†é¢‘ç†è§£
  * [Context-aware Biaffine Localizing Network for Temporal Sentence Grounding](https://arxiv.org/abs/2103.11555)<br>:star:[code](https://github.com/liudaizong/CBLN)
  * [Co-Grounding Networks with Semantic Attention for Referring Expression Comprehension in Videos](https://arxiv.org/abs/2103.12346)<br>:house:[project](https://sijiesong.github.io/co-grounding/)
  * [Visual Semantic Role Labeling for Video Understanding](https://arxiv.org/abs/2104.00990)<br>:house:[project](https://vidsitu.org/)
  * [Temporal Query Networks for Fine-grained Video Understanding](https://arxiv.org/abs/2104.09496)<br>:open_mouth:oral:house:[project](https://www.robots.ox.ac.uk/~vgg/research/tqn/) 
  * [Shot Contrastive Self-Supervised Learning for Scene Boundary Detection](https://arxiv.org/abs/2104.13537)
  * [FrameExit: Conditional Early Exiting for Efficient Video Recognition](https://arxiv.org/abs/2104.13400)<br>:open_mouth:oral
* è§†é¢‘ç¼©æ”¾
  * [Video Rescaling Networks with Joint Optimization Strategies for Downscaling and Upscaling](https://arxiv.org/abs/2103.14858)<br>:star:[code](https://github.com/ding3820/MIMO-VRN):house:[project](https://ding3820.github.io/MIMO-VRN/)
* è§†é¢‘å¼‚å¸¸æ£€æµ‹
  * [MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection](https://arxiv.org/abs/2104.01633)
* è§†é¢‘å£°æºå®šä½
  * [Localizing Visual Sounds the Hard Way](https://arxiv.org/abs/2104.02691)<br>:star:[code](https://github.com/hche11/Localizing-Visual-Sounds-the-Hard-Way):house:[project](https://www.robots.ox.ac.uk/~vgg/research/lvs/)
* è§†é¢‘åˆ†æ
  * [Self-Supervised Learning for Semi-Supervised Temporal Action Proposal](https://arxiv.org/abs/2104.03214)<br>:star:[code](https://github.com/wangxiang1230/SSTAP)
* è§†é¢‘ç”Ÿæˆ
  * [Playable Video Generation](https://arxiv.org/abs/2101.12195)<br>:open_mouth:oral:star:[code](https://github.com/willi-menapace/PlayableVideoGeneration):house:[project](https://willi-menapace.github.io/playable-video-generation-website/):tv:[video](https://www.youtube.com/watch?v=QtDjSyZERpg)
  * [One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing](https://arxiv.org/abs/2011.15126)<br>:open_mouth:oral:star:[code](https://github.com/NVlabs/imaginaire):house:[project](https://nvlabs.github.io/face-vid2vid/):tv:[video](https://youtu.be/nLYg9Waw72U)<br>è§£è¯»ï¼š[é¢ è¦†è§†é¢‘å‹ç¼©çš„ä¸ä¸€å®šæ˜¯æ–°å‹ç¼©ç®—æ³•ï¼Œè€Œå¯èƒ½æ˜¯GANï¼è‹±ä¼Ÿè¾¾æ–°ç®—æ³•æœ€é«˜å‹ç¼©90%æµé‡](https://mp.weixin.qq.com/s/UpfgxiIaSU4iIjbrkS--zA)<br>Nvidiaçš„æ–°ç ”ç©¶ï¼Œä½¿ç”¨äººè„¸å…³é”®ç‚¹+GANé‡å»ºè§†é¢‘é€šè¯ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„H.264èŠ‚çœ90%æµé‡ã€‚ä»£ç æœªå¼€æºï¼Œä½†è‹±ä¼Ÿè¾¾çš„GANæ¡†æ¶å¼€æºäº†ã€‚
* è§†é¢‘è§†è§’åˆ‡æ¢
  * [Ego-Exo: Transferring Visual Representations from Third-person to First-person Videos](https://arxiv.org/abs/2104.07905)
* Action Selection Learning
  * [Weakly Supervised Action Selection Learning in Video](https://arxiv.org/abs/2105.02439)<br>:star:[code](https://github.com/layer6ai-labs/ASL)

<a name="25"/>

## 25.3D(ä¸‰ç»´è§†è§‰)

- [A Deep Emulator for Secondary Motion of 3D Characters](https://arxiv.org/abs/2103.01261)<br>:open_mouth:oral
- [Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction](https://arxiv.org/abs/2012.01451)<br>:open_mouth:oral:house:[project](https://aljazbozic.github.io/neural_deformation_graphs/):tv:[video](https://www.youtube.com/watch?v=vyq36eFkdWo)<br>
- [Deep Implicit Templates for 3D Shape Representation](https://arxiv.org/abs/2011.14565)<br>:open_mouth:oral:star:[code](https://github.com/ZhengZerong/DeepImplicitTemplates):house:[project](http://www.liuyebin.com/dit/dit.html):tv:[video](http://www.liuyebin.com/dit/assets/supp_vid.mp4)<br>[CVPR 2021 Oralï¼Œæ¸…åå­¦è€…æå‡ºDeep Implicit Templatesï¼Œæå¤§æ‰©å±•DIFèƒ½åŠ›](https://zhuanlan.zhihu.com/p/354737798)<br>
- [SMPLicit: Topology-aware Generative Model for Clothed People](https://arxiv.org/abs/2103.06871)<br>:house:[project](http://www.iri.upc.edu/people/ecorona/smplicit/)
- [Picasso: A CUDA-based Library for Deep Learning over 3D Meshes](https://arxiv.org/abs/2103.15076)<br>:star:[code](https://github.com/hlei-ziyan/Picasso)
- [Semi-supervised Synthesis of High-Resolution Editable Textures for 3D Humans](https://arxiv.org/abs/2103.17266)
* [RGB-D Local Implicit Function for Depth Completion of Transparent Objects](https://arxiv.org/abs/2104.00622)<br>:house:[project](https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit)
* [Deep Two-View Structure-from-Motion Revisited](https://arxiv.org/abs/2104.00556)
* [Deformed Implicit Field: Modeling 3D Shapes with Learned Dense Correspondence](https://arxiv.org/abs/2011.13650)
* [S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling](https://arxiv.org/abs/2101.06571)
* [Deep Polarization Imaging for 3D Shape and SVBRDF Acquisition](https://arxiv.org/abs/2105.02875)<br>:open_mouth:oral
* [Learning Feature Aggregation for Deep 3D Morphable Models](https://arxiv.org/abs/2105.02173)<br>:star:[code](https://github.com/zxchen110/Deep3DMM/)
* æ·±åº¦ä¼°è®¡
  * [PLADE-Net: Towards Pixel-Level Accuracy for Self-Supervised Single-View Depth Estimation with Neural Positional Encoding and Distilled Matting Loss](https://arxiv.org/abs/2103.07362)
  * [Beyond Image to Depth: Improving Depth Prediction using Echoes](https://arxiv.org/abs/2103.08468)<br>:star:[code](https://github.com/krantiparida/beyond-image-to-depth):house:[project](https://krantiparida.github.io/projects/bimgdepth.html)
  * [Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos](https://arxiv.org/abs/2103.03319)<br>:open_mouth:oral:star:[code](https://github.com/yasaminjafarian/HDNet_TikTok):house:[project](https://www.yasamin.page/hdnet_tiktok):tv:[video](https://youtu.be/EFJ8WXdKghs)
  * [3D Packing for Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/1905.02693)<br>:open_mouth:oral:star:[code](https://github.com/TRI-ML/packnet-sfm)
  * [LED2-Net: Monocular 360 Layout Estimation via Differentiable Depth Rendering](https://arxiv.org/abs/2104.00568)<br>:open_mouth:oral:star:[code](https://github.com/fuenwang/LED2-Net):house:[project](https://fuenwang.ml/project/led2net/)
  * [S2R-DepthNet: Learning a Generalizable Depth-specific Structural Representation](https://arxiv.org/abs/2104.00877)<br>:open_mouth:oral
  * [Depth Completion with Twin Surface Extrapolation at Occlusion Boundaries](https://arxiv.org/abs/2104.02253)<br>:star:[code](https://github.com/imransai/TWISE)
  * [Self-supervised Learning of Depth Inference for Multi-view Stereo](https://arxiv.org/abs/2104.02972)<br>:star:[code](https://github.com/JiayuYANG/Self-supervised-CVP-MVSNet)
  * [SMD-Nets: Stereo Mixture Density Networks](https://arxiv.org/abs/2104.03866)<br>:star:[code](https://github.com/fabiotosi92/SMD-Nets)
  * [The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth](https://arxiv.org/abs/2104.14540)
* ä¸‰ç»´é‡å»º
  * [Deep Implicit Moving Least-Squares Functions for 3D Reconstruction](https://arxiv.org/abs/2103.12266)<br>:star:[code](https://github.com/Andy97/DeepMLS)
  * [Bilevel Online Adaptation for Out-of-Domain Human Mesh Reconstruction](https://arxiv.org/abs/2103.16449)<br>:house:[project](https://sites.google.com/view/humanmeshboa)
  * [Learning Parallel Dense Correspondence from Spatio-Temporal Descriptors for Efficient and Robust 4D Reconstruction](https://arxiv.org/abs/2103.16341)<br>:star:[code](https://github.com/tangjiapeng/LPDC-Net)
  * [Fostering Generalization in Single-view 3D Reconstruction by Learning a Hierarchy of Local and Global Shape Priors](https://arxiv.org/abs/2104.00476)
  * [NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video](https://arxiv.org/abs/2104.00681)<br>:open_mouth:oral:star:[code](https://github.com/zju3dv/NeuralRecon):house:[project](https://zju3dv.github.io/neuralrecon/)
  * [Fully Understanding Generic Objects: Modeling, Segmentation, and Reconstruction](https://arxiv.org/abs/2104.00858)
  * [CodedStereo: Learned Phase Masks for Large Depth-of-field Stereo](https://arxiv.org/abs/2104.04641)<br>:open_mouth:oral
  * [SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements](https://arxiv.org/abs/2104.07660)<br>:house:[project](https://qianlim.github.io/SCALE):tv:[video](https://www.youtube.com/watch?v=-EvWqFCUb7U)
  * [LASR: Learning Articulated Shape Reconstruction from a Monocular Video](https://arxiv.org/abs/2105.02976)<br>:house:[project](https://lasr-google.github.io/)
  * [Sketch2Model: View-Aware 3D Modeling from Single Free-Hand Sketches](https://arxiv.org/abs/2105.06663)
  * [Birds of a Feather: Capturing Avian Shape Models from Images](https://arxiv.org/abs/2105.09396)<br>:house:[project](https://yufu-wang.github.io/aves/):tv:[video](https://youtu.be/TDR2LC7mFpw)
  * [Multi-view 3D Reconstruction of a Texture-less Smooth Surface of Unknown Generic Reflectance](https://arxiv.org/abs/2105.11599)<br>:star:[code](https://github.com/za-cheng/PM-PMVS/)
* è¯­ä¹‰åœºæ™¯è¡¥å…¨
  * [Semantic Scene Completion via Integrating Instances and Scene in-the-Loop](https://arxiv.org/abs/2104.03640)<br>:star:[code](https://github.com/yjcaimeow/SISNet)
* ä¸‰ç»´å…³é”®ç‚¹
  * [KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control](https://arxiv.org/abs/2104.11224)<br>:open_mouth:oral:star:[code](https://github.com/tomasjakab/keypoint_deformer/):house:[project](https://tomasjakab.github.io/KeypointDeformer/):tv:[video](https://youtu.be/GdDX1ZFh1k0)
* ä¸‰ç»´å½¢çŠ¶è¡¥å…¨
  * [Unsupervised 3D Shape Completion through GAN Inversion](https://arxiv.org/abs/2104.13366)<br>:star:[code](https://github.com/junzhezhang/shape-inversion):house:[project](https://junzhezhang.github.io/projects/ShapeInversion/)
* ä¸‰ç»´å½¢çŠ¶é€‚é…
  * [Cuboids Revisited: Learning Robust 3D Shape Fitting to Single RGB Images](https://arxiv.org/abs/2105.02047)<br>:star:[code](https://github.com/fkluger/cuboids_revisited)
* ä¸‰ç»´å‹ç¼©
  * [Neural 3D Scene Compression via Model Compression](https://arxiv.org/abs/2105.03120)
* Stereo Matching-ç«‹ä½“åŒ¹é…
   * [A Decomposition Model for Stereo Matching](https://arxiv.org/abs/2104.07516)
* Depth Completion-æ·±åº¦è¡¥å…¨
   * [Depth Completion using Plane-Residual Representation](https://arxiv.org/abs/2104.07350)

<a name="24"/> 

## 24.Reinforcement Learning(å¼ºåŒ–å­¦ä¹ )
- [Hierarchical and Partially Observable Goal-driven Policy Learning with Goals Relational Graph](https://arxiv.org/abs/2103.01350)<br>:star:[code](https://github.com/Xin-Ye-1/HRL-GRG):house:[project](https://xin-ye-1.github.io/HRL-GRG/)
- [Unsupervised Learning for Robust Fitting:A Reinforcement Learning Approach](https://arxiv.org/abs/2103.03501)
- [Unsupervised Visual Attention and Invariance for Reinforcement Learning](https://arxiv.org/abs/2104.02921)

<a name="23"/> 

## 23.Autonomous Driving(è‡ªåŠ¨é©¾é©¶)

- [Patch-NetVLAD: Multi-Scale Fusion of Locally-Global Descriptors for Place Recognition](https://arxiv.org/abs/2103.01486)<br>:star:[code](https://github.com/QVPR/Patch-NetVLAD)<br>ECCV 2020 Facebook Mapillary Visual Place Recognition Challenge å† å†›æ–¹æ¡ˆ
- [AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles](https://arxiv.org/abs/2101.06549)
- [Self-Supervised Pillar Motion Learning for Autonomous Driving](https://arxiv.org/abs/2104.08683)<br>:star:[code](https://github.com/qcraftai/pillar-motion)
* è½¦é“çº¿é¢„æµ‹
  * [LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of Dynamic Agents](https://arxiv.org/abs/2104.00249)
  * [Divide-and-Conquer for Lane-Aware Diverse Trajectory Prediction](https://arxiv.org/abs/2104.08277)<br>:open_mouth:oral
* è½¨è¿¹é¢„æµ‹
  * [SGCN:Sparse Graph Convolution Network for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2104.01528)<br>:star:[code](https://github.com/shuaishiliu/SGCN)

<a name="22"/> 

## 22.Medical Imaging(åŒ»å­¦å½±åƒ)

- [3D Graph Anatomy Geometry-Integrated Network for Pancreatic Mass Segmentation, Diagnosis, and Quantitative Patient Management](https://arxiv.org/abs/2012.04701)<br>ç”¨çº¯å¤šæ¨¡æ€ CT å½±åƒå¯æ›¿ä»£ç›®å‰ JHMI çš„éœ€è¦åšè‚¿ç˜¤åŒ–å­¦æ£€æµ‹å’Œ DNA æµ‹åº+åŒ»å­¦å½±åƒçš„ç»¼åˆå¤šæ¨¡æ€è¯Šæ–­æµç¨‹ï¼Œä»è¯Šæ–­å‡†ç¡®åº¦ä¸Šæœ‰å¯æ¯”è¾ƒæ€§ï¼Œå®šé‡è¯Šæ–­ç²¾åº¦æ›´ä¼˜<br>
- [Deep Lesion Tracker: Monitoring Lesions in 4D Longitudinal Imaging Studies](https://arxiv.org/abs/2012.04872)<br>è‚¿ç˜¤å½±åƒé‡Œé¢æ™ºèƒ½ PACS è¾…åŠ©åŒ»ç”Ÿè¯»ç‰‡çš„é‡è¦åŠŸèƒ½<br>
- [Automatic Vertebra Localization and Identification in CT by Spine Rectification and Anatomically-constrained Optimization](https://arxiv.org/abs/2012.07947)<br>åŸºäºCT å½±åƒçš„éª¨æŠ˜/éª¨è´¨ç–æ¾ç³»ç»Ÿ<br>
- [Multi-institutional Collaborations for Improving Deep Learning-based Magnetic Resonance Image Reconstruction Using Federated Learning](https://arxiv.org/abs/2103.02148)<br>:star:[code](https://github.com/guopengf/FL-MRCM)<br>å¤šæœºæ„åˆä½œï¼Œåˆ©ç”¨è”åˆå­¦ä¹ æ”¹è¿›åŸºäºæ·±åº¦å­¦ä¹ çš„ç£å…±æŒ¯å›¾åƒé‡å»ºæŠ€æœ¯<br>
- [DeepTag: An Unsupervised Deep Learning Method for Motion Tracking on Cardiac Tagging Magnetic Resonance Images](https://arxiv.org/abs/2103.02772)<br>:open_mouth:oral:star:[code](https://github.com/DeepTag/cardiac_tagging_motion_estimation)<br>DeepTag: ä¸€ç§æ— ç›‘ç£çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¿ƒè„æ ‡è®°ç£å…±æŒ¯å›¾åƒçš„è¿åŠ¨è·Ÿè¸ª<br>
- [Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles](https://arxiv.org/abs/2103.05121)
* [XProtoNet: Diagnosis in Chest Radiography with Global and Local Explanations](https://arxiv.org/abs/2103.10663)
* åŒ»å­¦å›¾åƒåˆ†å‰²
  * [FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space](https://arxiv.org/abs/2103.06030)<br>:star:[code](https://github.com/liuquande/FedDG-ELCFS)
  * [DoDNet: Learning to segment multi-organ and tumors from multiple partially labeled datasets](https://arxiv.org/abs/2011.10217)<br>:star:[code](https://github.com/jianpengz/DoDNet):sunflower:[dataset](https://github.com/aim-uofa/partially-labelled)
  * [DiNTS: Differentiable Neural Network Topology Search for 3D Medical Image Segmentation]<br>:open_mouth:oral(https://arxiv.org/abs/2103.15954)
  * [DARCNN: Domain Adaptive Region-based Convolutional Neural Network for Unsupervised Instance Segmentation in Biomedical Images](https://arxiv.org/abs/2104.01325)<br>
  * [Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation](https://arxiv.org/abs/2104.13243)
* åŒ»å­¦å›¾åƒåˆæˆ
  * [Brain Image Synthesis with Unsupervised Multivariate Canonical CSCâ„“4Net](https://arxiv.org/abs/2103.11587)<br>:open_mouth:oral

<a name="21"/> 

## 21.Transformer 

- [Transformer Interpretability Beyond Attention Visualization](https://arxiv.org/pdf/2012.09838.pdf)<br>:star:[code](https://github.com/hila-chefer/Transformer-Explainability)<br> 
- [MIST: Multiple Instance Spatial Transformer Network](https://arxiv.org/abs/1811.10725)<br>è¯•å›¾ä»çƒ­å›¾ä¸­è¿›è¡Œå¯å¾®çš„top-Ké€‰æ‹©(MIST)ï¼ˆç›®å‰åœ¨è‡ªç„¶å›¾åƒä¸Šä¹Ÿæœ‰äº†ä¸€äº›ç»“æœï¼›) ç”¨å®ƒå¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•å®šä½ç›‘ç£çš„æƒ…å†µä¸‹è¿›è¡Œæ£€æµ‹å’Œåˆ†ç±»ï¼ˆå¹¶ä¸æ˜¯å®ƒå”¯ä¸€èƒ½åšçš„äº‹æƒ…!ï¼‰
- [Variational Transformer Networks for Layout Generation](https://arxiv.org/abs/2104.02416)
* åŠ¨ä½œè¯†åˆ«æ£€æµ‹
  * 3D Vision Transformers for Action Recognition<br>ç”¨äºåŠ¨ä½œè¯†åˆ«çš„3Dè§†è§‰Transformer
* ç›®æ ‡æ£€æµ‹
  * [UP-DETR: Unsupervised Pre-training for Object Detection with Transformers](https://arxiv.org/pdf/2011.09094.pdf)<br>:open_mouth:oral:star:[code](https://github.com/dddzg/up-detr)
* å›¾åƒå¤„ç†
  * [Pre-Trained Image Processing Transformer](https://arxiv.org/pdf/2012.00364.pdf)<br>
* äººæœºäº¤äº’
  * [End-to-End Human Object Interaction Detection with HOI Transformer](https://arxiv.org/abs/2103.04503)<br>:star:[code](https://github.com/bbepoch/HoiTransformer)
  * [HOTR: End-to-End Human-Object Interaction Detection with Transformers](https://arxiv.org/abs/2104.13682)<br>:open_mouth:oral
* å›¾åƒåˆ†å‰²
  * [Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers](https://arxiv.org/abs/2012.15840)<br>:star:[code](https://github.com/fudan-zvg/SETR):house:[project](https://fudan-zvg.github.io/SETR/)<br>åŸºäºTransformersä»åºåˆ—åˆ°åºåˆ—çš„è§’åº¦é‡æ–°æ€è€ƒè¯­ä¹‰åˆ†å‰²<br>è§£è¯»ï¼š[16](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)<br>è§£è¯»ï¼š[Transformer åœ¨è¯­ä¹‰åˆ†å‰²ä¸­çš„åº”ç”¨ï¼Œæ›¾ä½ADE20K æ¦œé¦–ï¼ˆ44.42% mIoUï¼‰](https://zhuanlan.zhihu.com/p/341768446)
  * [VisTR: End-to-End Video Instance Segmentation with Transformers](https://arxiv.org/abs/2011.14503)<br>:open_mouth:oral:star:[code](https://github.com/Epiphqny/VisTR)
* è·Ÿè¸ª
  * [Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking](https://arxiv.org/abs/2103.11681)<br>:open_mouth:oral:star:[code](https://github.com/594422814/TransformerTrack)<br>more:[Transformerå†è“„åŠ›ï¼Œè·Ÿè¸ªä»»åŠ¡ä¸­åˆ›æ–°é«˜ï¼Œæ¡¥æ¥ç‹¬ç«‹å¸§ï¼Œè·¨å¸§ä¼ é€’æ—¶åŸŸä¿¡æ¯ï¼ŒCVPR 2021 Oral](https://zhuanlan.zhihu.com/p/359237554)
  * [Transformer Tracking](https://arxiv.org/abs/2103.15436)<br>:star:[code](https://github.com/chenxin-dlut/TransT)
* åŠ¨ä½œé¢„æµ‹
  * [Multimodal Motion Prediction with Stacked Transformers](https://arxiv.org/abs/2103.11624)<br>:star:[code](https://github.com/Mrmoore98/mmTransformer-Multimodal-Motion-Prediction-with-Stacked-Transformers):house:[project](https://decisionforce.github.io/mmTransformer/):tv:[video](https://youtu.be/ytqS8dgVcx0)  
* Self-attentionè‡ªæ³¨æ„åŠ›æœºåˆ¶
  * [Scaling Local Self-Attention For Parameter Efficient Visual Backbones](https://arxiv.org/abs/2103.12731)<br>:open_mouth:oral
* æ£€ç´¢
  * [Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning](https://arxiv.org/abs/2103.13061)<br>:star:[code](https://github.com/amzn/image-to-recipe-transformers)
* ç‰¹å¾åŒ¹é…
  * [LoFTR: Detector-Free Local Feature Matching with Transformers](https://arxiv.org/abs/2104.00680)<br>:star:[code](https://github.com/zju3dv/LoFTR):house:[project](https://zju3dv.github.io/loftr/)
* å§¿åŠ¿è¯†åˆ«
  * [Pose Recognition with Cascade Transformers](https://arxiv.org/abs/2104.06976)<br>:star:[code](https://github.com/mlpc-ucsd/PRTR)
* è‡ªåŠ¨é©¾é©¶
  * [Multi-Modal Fusion Transformer for End-to-End Autonomous Driving](https://arxiv.org/abs/2104.09224)<br>:star:[code](https://github.com/autonomousvision/transfuser)

<a name="20"/> 

## 20.Person Re-Identification(äººå‘˜é‡è¯†åˆ«)

- [Meta Batch-Instance Normalization for Generalizable Person Re-Identification](https://arxiv.org/abs/2011.14670)<br>
- [Watching You: Global-guided Reciprocal Learning for Video-based Person Re-identification](https://arxiv.org/abs/2103.04337)
- [Joint Noise-Tolerant Learning and Meta Camera Shift Adaptation for Unsupervised Person Re-Identification](https://arxiv.org/abs/2103.04618)<br>:star:[code](https://github.com/FlyingRoastDuck/MetaCam_DSCE)
- [Self-supervised 3D Reconstruction and Re-Projection for Texture Insensitive Person Re-identification]<br>åŸºäºè‡ªç›‘ç£ä¸‰ç»´é‡å»ºå’Œé‡æŠ•å½±çš„çº¹ç†ä¸æ•æ„Ÿè¡Œäººé‡è¯†åˆ«<br>è§£è¯»ï¼š[12](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
- [Intra-Inter Camera Similarity for Unsupervised Person Re-Identification](https://arxiv.org/abs/2103.11658)<br>:star:[code](https://github.com/SY-Xuan/IICS)<br>è®ºæ–‡å…¬å¼€
- [Anchor-Free Person Search](https://arxiv.org/abs/2103.11617)<br>:star:[code](https://github.com/daodaofr/AlignPS)
* [Lifelong Person Re-Identification via Adaptive Knowledge Accumulation](https://arxiv.org/abs/2103.12462)<br>:star:[code](https://github.com/TPCD/LifelongReID)
* [Group-aware Label Transfer for Domain Adaptive Person Re-identification](https://arxiv.org/abs/2103.12366)<br>:star:[code](https://github.com/zkcys001/UDAStrongBaseline)|[code](https://github.com/JDAI-CV/fast-reid)
* [Neural Feature Search for RGB-Infrared Person Re-Identification](https://arxiv.org/abs/2104.02366)
* [Combined Depth Space based Architecture Search For Person Re-identification](https://arxiv.org/abs/2104.04163)
* [Unsupervised Multi-Source Domain Adaptation for Person Re-Identification](https://arxiv.org/abs/2104.12961)<br>:open_mouth:oral
* [Spatial-Temporal Correlation and Topology Learning for Person Re-Identification in Videos](https://arxiv.org/abs/2104.08241)<br>:open_mouth:oral
* [BiCnet-TKS: Learning Efficient Spatial-Temporal Representation for Video Person Re-Identification](https://arxiv.org/abs/2104.14783)<br>:star:[code](https://github.com/blue-blue272/BiCnet-TKS)
* [Generalizable Person Re-identification with Relevance-aware Mixture of Experts](https://arxiv.org/abs/2105.09156)
* æ‹¥æŒ¤äººç¾¤è®¡æ•°
  * [Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT Benchmark for Crowd Counting](https://arxiv.org/abs/2012.04529)
 
<a name="19"/> 

## 19.Quantization/Pruning/Knowledge Distillation/Model Compression(é‡åŒ–ã€å‰ªæã€è’¸é¦ã€æ¨¡å‹å‹ç¼©/æ‰©å±•ä¸ä¼˜åŒ–)

- Learning Student Networks in the Wild<br>
- [ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network](https://arxiv.org/abs/2007.00992)<br>:star:[code](https://github.com/clovaai/rexnet)<br>
- [RepVGG: Making VGG-style ConvNets Great Again](https://arxiv.org/abs/2101.03697)<br>:star:[code](https://github.com/megvii-model/RepVGG)<br>
- [Coordinate Attention for Efficient Mobile Network Design](https://arxiv.org/abs/2103.02907)<br>:star:[code](https://github.com/Andrew-Qibin/CoordAttention)
* å‰ªæ
  * [Manifold Regularized Dynamic Network Pruning](https://arxiv.org/abs/2103.05861)
  * [Neural Response Interpretation through the Lens of Critical Pathways](https://arxiv.org/abs/2103.16886)<br>:star:[code](https://github.com/CAMP-eXplain-AI/PathwayGrad)|[code](https://github.com/CAMP-eXplain-AI/RoarTorch)
  * [Riggable 3D Face Reconstruction via In-Network Optimization](https://arxiv.org/abs/2104.03438)
  * [Towards Compact CNNs via Collaborative Compression](https://arxiv.org/abs/2105.11228)
  * [BCNet: Searching for Network Width with Bilaterally Coupled Network](https://arxiv.org/abs/2105.10533)
* æ¨¡å‹æ‰©å±•
  * [Fast and Accurate Model Scaling](https://arxiv.org/abs/2103.06877)<br>:star:[code](https://github.com/facebookresearch/pycls)
* é‡åŒ–  
  * [Learnable Companding Quantization for Accurate Low-bit Neural Networks](https://arxiv.org/abs/2103.07156)
  * [Diversifying Sample Generation for Accurate Data-Free Quantization](https://arxiv.org/abs/2103.01049)
  * [Zero-shot Adversarial Quantization](https://arxiv.org/abs/2103.15263)<br>:open_mouth:oral:star:[code](https://github.com/FLHonker/ZAQ-code)
  * [Network Quantization with Element-wise Gradient Scaling](https://arxiv.org/abs/2104.00903)<br>:house:[project](https://cvlab.yonsei.ac.kr/projects/EWGS/) 
* çŸ¥è¯†è’¸é¦
  * [Refine Myself by Teaching Myself: Feature Refinement via Self-Knowledge Distillation](https://arxiv.org/abs/2103.08273)<br>:star:[code](https://github.com/MingiJi/FRSKD)
  * [Complementary Relation Contrastive Distillation](https://arxiv.org/abs/2103.16367)
  * [Distilling Knowledge via Knowledge Review](https://arxiv.org/abs/2104.09044)<br>:star:[code](https://github.com/Jia-Research-Lab/ReviewKD)
* å¯é€†ç¥ç»ç½‘ç»œ
  * [Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks](https://arxiv.org/abs/2103.10429)<br>:house:[project](https://paschalidoud.github.io/neural_parts)
* æ¨¡å‹å‹ç¼©
  * [CDFI: Compression-Driven Network Design for Frame Interpolation](https://arxiv.org/abs/2103.10559)<br>:star:[code](https://github.com/tding1/CDFI)

<a name="18"/> 

## 18.Aerial/Drones/Satellite/RS Image(èˆªç©ºå½±åƒ/æ— äººæœº)

- Dogfight: Detecting Drones from Drone Videos<br>
- [UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles](https://arxiv.org/abs/2104.00946)
- [Detection, Tracking, and Counting Meets Drones in Crowds: A Benchmark](https://arxiv.org/abs/2105.02440)<br>:star:[code](https://github.com/VisDrone/DroneCrowd)
- [SIPSA-Net: Shift-Invariant Pan Sharpening with Moving Object Alignment for Satellite Imagery](https://arxiv.org/abs/2105.02400)<br>:star:[code](https://github.com/brachiohyup/SIPSA)
* èˆªç©ºå½±åƒåˆ†å‰²
  * [PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation](https://arxiv.org/abs/2103.06564)<br>:star:[code](https://github.com/lxtGH/PFSegNets)
* èˆªç©ºå½±åƒæ£€æµ‹
  * [ReDet: A Rotation-equivariant Detector for Aerial Object Detection](https://arxiv.org/abs/2103.07733)<br>:star:[code](https://github.com/csuhan/ReDet)
* æ— äººæœºæ£€æµ‹
  * [Dogfight: Detecting Drones from Drones Videos](https://arxiv.org/abs/2103.17242)
* å¤šè§†è§’å«æ˜Ÿæ‘„å½±æµ‹é‡
  * [Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry](https://arxiv.org/abs/2104.09877)

<a name="17"/> 

## 17.Super-Resolution(è¶…åˆ†è¾¨ç‡)

- Data-Free Knowledge Distillation For Image Super-Resolution<br>
- [AdderSR: Towards Energy Efficient Image Super-Resolution](https://arxiv.org/pdf/2009.08891.pdf)<br>:star:[code](https://github.com/huawei-noah/AdderNet)<br>
- [Cross-MPI: Cross-scale Stereo for Image Super-Resolution using Multiplane Images](https://arxiv.org/abs/2011.14631)<br>:house:[project](http://www.liuyebin.com/crossMPI/crossMPI.html):tv:[video](http://www.liuyebin.com/crossMPI/assets/supp_vid.mp4)<br>[CVPR 2021ï¼ŒCross-MPIä»¥åº•å±‚åœºæ™¯ç»“æ„ä¸ºçº¿ç´¢çš„ç«¯åˆ°ç«¯ç½‘ç»œï¼Œåœ¨å¤§åˆ†è¾¨ç‡ï¼ˆx8ï¼‰å·®è·ä¸‹ä¹Ÿå¯å®Œæˆé«˜ä¿çœŸçš„è¶…åˆ†è¾¨ç‡](https://zhuanlan.zhihu.com/p/354752197)
- [ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic](https://arxiv.org/abs/2103.04039)<br>:star:[code](https://github.com/Xiangtaokong/ClassSR)
* Robust Reference-based Super-Resolution via CÂ²-Matching
* [GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution](https://arxiv.org/abs/2012.00739)<br>:open_mouth:oral:house:[project](https://ckkelvinchan.github.io/projects/GLEAN/)<br>è§£è¯»ï¼š[CVPR 2021 Oral | GLEAN: åŸºäºéšå¼ç”Ÿæˆåº“çš„é«˜å€ç‡å›¾åƒè¶…åˆ†è¾¨ç‡](https://mp.weixin.qq.com/s/ZdfoS_VkFfQVsRhA4g6Yog)
* [BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond](https://arxiv.org/abs/2012.02181)<br>:star:[code](https://github.com/ckkelvinchan/BasicVSR-IconVSR):house:[project](https://ckkelvinchan.github.io/projects/BasicVSR/)
* [Temporal Modulation Network for Controllable Space-Time Video Super-Resolution]<br>[ä½œè€…ä¸»é¡µ](https://csjunxu.github.io/)<br>åŸºäºæ—¶ç©ºç‰¹å¾å¯æ§æ’å€¼çš„è§†é¢‘è¶…åˆ†è¾¨ç‡ç½‘ç»œ<br>è§£è¯»ï¼š[18](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* [Flow-based Kernel Prior with Application to Blind Super-Resolution](https://arxiv.org/abs/2103.15977)
* [Unsupervised Degradation Representation Learning for Blind Super-Resolution](https://arxiv.org/abs/2104.00416)<br>:star:[code](https://github.com/LongguangWang/DASR)
* [SRWarp: Generalized Image Super-Resolution under Arbitrary Transformation](https://arxiv.org/abs/2104.10325)<br>:star:[code](https://github.com/sanghyun-son/srwarp)

<a name="16"/> 

## 16.Visual Question Answering(è§†è§‰é—®ç­”)

- Weakly-supervised Grounded Visual Question Answering using Capsules<br>
* [Counterfactual VQA: A Cause-Effect Look at Language Bias](https://arxiv.org/abs/2006.04315)<br>:star:[code](https://github.com/yuleiniu/cfvqa)
* [AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning](https://arxiv.org/abs/2103.16002)
* [Domain-robust VQA with diverse datasets and methods but no target labels](https://arxiv.org/abs/2103.15974)
* [Found a Reason for me? Weakly-supervised Grounded Visual Question Answering using Capsules](https://arxiv.org/abs/2105.04836)<br>:star:[code](https://github.com/aurooj/WeakGroundedVQA_Capsules)
* è§†é¢‘é—®ç­”
  * [TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events](https://arxiv.org/abs/2103.15538)<br>:star:[code](https://github.com/SUTDCV/SUTD-TrafficQA)
  * [Bridge to Answer: Structure-aware Graph Interaction Network for Video Question Answering](https://arxiv.org/abs/2104.14085)
  * [NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions](https://arxiv.org/abs/2105.08276)<br>:star:[code](https://github.com/doc-doc/NExT-QA)

<a name="15"/> 

## 15.GAN
- Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing<br>
- [Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs](https://arxiv.org/pdf/2011.14107.pdf)<br>
- [Image-to-image Translation via Hierarchical Style Disentanglement](https://arxiv.org/abs/2103.01456)<br>:star:[code](https://github.com/imlixinyang/HiSD)
- [Efficient Conditional GAN Transfer with Knowledge Propagation across Classes](https://arxiv.org/abs/2102.06696)<br>:star:[code](https://github.com/mshahbazi72/cGANTransfer)
- [Anycost GANs for Interactive Image Synthesis and Editing](https://arxiv.org/abs/2103.03243)<br>:star:[code](https://github.com/mit-han-lab/anycost-gan):house:[project](https://hanlab.mit.edu/projects/anycost-gan/):tv:[video](https://www.youtube.com/watch?v=_yEziPl9AkM&t=90s)<br>Anycost GANï¼Œå¯é€‚åº”å¹¿æ³›çš„ç¡¬ä»¶å’Œå»¶è¿Ÿè¦æ±‚ï¼Œä»¥åŠå®ç°äº¤äº’å¼å›¾åƒç¼–è¾‘
- [TediGAN: Text-Guided Diverse Image Generation and Manipulation](https://arxiv.org/abs/2012.03308)<br>:star:[code](https://github.com/weihaox/TediGAN):house:[project](https://xiaweihao.com/projects/tedigan/):tv:[video](https://www.youtube.com/watch?v=L8Na2f5viAM)
- [Generative Hierarchical Features from Synthesizing Images](https://arxiv.org/abs/2007.10379)<br>:open_mouth:oral:star:[code](https://github.com/genforce/ghfeat):house:[project](https://genforce.github.io/ghfeat/)<br>ä½œè€…ç§°é¢„è®­ç»ƒ GAN ç”Ÿæˆå™¨å¯ä»¥å½“ä½œæ˜¯ä¸€ç§å­¦ä¹ çš„å¤šå°ºåº¦æŸå¤±ã€‚ç”¨å®ƒè¿›è¡Œè®­ç»ƒå¯ä»¥å¸¦æ¥é«˜åº¦ç«äº‰çš„å±‚æ¬¡åŒ–å’Œåˆ†ç¦»çš„è§†è§‰ç‰¹å¾ï¼Œç§°ä¹‹ä¸ºç”Ÿæˆå±‚æ¬¡åŒ–ç‰¹å¾ï¼ˆGH-Featï¼‰ã€‚å¹¶è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒGH-Featä¸ä»…æœ‰åˆ©äºç”Ÿæˆæ€§ä»»åŠ¡ï¼Œæ›´é‡è¦çš„æ˜¯æœ‰åˆ©äºåˆ†è¾¨æ€§ä»»åŠ¡ï¼ŒåŒ…æ‹¬äººè„¸éªŒè¯ã€å…³é”®ç‚¹æ£€æµ‹ã€layout predictionã€è¿ç§»å­¦ä¹ ã€style mixingã€å›¾åƒç¼–è¾‘ç­‰ã€‚
- [Teachers Do More Than Teach: Compressing Image-to-Image Models](https://arxiv.org/abs/2103.03467)
- [PISE: Person Image Synthesis and Editing with Decoupled GAN](https://arxiv.org/abs/2103.04023)<br>:star:[code](https://github.com/Zhangjinso/PISE)
- [LOHO: Latent Optimization of Hairstyles via Orthogonalization](https://arxiv.org/abs/2103.03891)
- [HumanGAN: A Generative Model of Humans Images](https://arxiv.org/abs/2103.06902)
- [HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms](https://arxiv.org/abs/2011.11731)<br>:star:[code](https://github.com/mahmoudnafifi/HistoGAN)
- [DivCo: Diverse Conditional Image Synthesis via Contrastive Generative Adversarial Network](https://arxiv.org/abs/2103.07893)<br>:star:[code](https://github.com/ruiliu-ai/DivCo)
* [pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis](https://arxiv.org/abs/2012.00926)<br>:open_mouth:oral:house:[project](https://marcoamonteiro.github.io/pi-GAN-website/):tv:[video](https://youtu.be/0HCdof9BGtw)<br>æ›´å¤šï¼š[æ–¯å¦ç¦å­¦è€…æå‡ºå‘¨æœŸæ€§éšå¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆÏ€-GANæˆ–pi-GANï¼‰ï¼Œç”¨äºé«˜è´¨é‡çš„3Dæ„ŸçŸ¥å›¾åƒåˆæˆ](https://zhuanlan.zhihu.com/p/336155077)<br>æ–¯å¦ç¦å¤§å­¦
* [ReMix: Towards Image-to-Image Translation with Limited Data](https://arxiv.org/abs/2103.16835)
* [Unsupervised Disentanglement of Linear-Encoded Facial Semantics](https://arxiv.org/abs/2103.16605)
* [Content-Aware GAN Compression](https://arxiv.org/abs/2104.02244)
* [Regularizing Generative Adversarial Networks under Limited Data](https://arxiv.org/abs/2104.03310)<br>:star:[code](https://github.com/google/lecam-gan):house:[project](https://hytseng0509.github.io/lecam-gan/)
* [Where and What? Examining Interpretable Disentangled Representations](https://arxiv.org/abs/2104.05622)<br>:star:[code](https://github.com/zhuxinqimac/PS-SC)
* [Few-shot Image Generation via Cross-domain Correspondence](https://arxiv.org/abs/2104.06820)<br>:house:[project](https://utkarshojha.github.io/)
* [DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort](https://arxiv.org/abs/2104.06490)<br>:open_mouth:oral
* [Surrogate Gradient Field for Latent Space Manipulation](https://arxiv.org/abs/2104.09065)
* [StylePeople: A Generative Model of Fullbody Human Avatars](https://arxiv.org/abs/2104.08363)
* [Ensembling with Deep Generative Views](https://arxiv.org/abs/2104.14551)<br>:star:[code](https://github.com/chail/gan-ensembling):house:[project](https://chail.github.io/gan-ensembling/)
* [Continuous Face Aging via Self-estimated Residual Age Embedding](https://arxiv.org/abs/2105.00020)
* å›¾åƒåˆ°å›¾åƒç¿»è¯‘
  * [Memory-guided Unsupervised Image-to-image Translation](https://arxiv.org/abs/2104.05170)
  * [Image-to-image Translation via Hierarchical Style Disentanglement](https://arxiv.org/abs/2103.01456)<br>:open_mouth:oral:star:[code](https://github.com/imlixinyang/HiSD)<br>åœ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸Šå®ç°å±‚æ¬¡é£æ ¼è§£è€¦
  * [CoMoGAN: continuous model-guided image-to-image translation](https://arxiv.org/abs/2103.06879)<br>:open_mouth:oral:star:[code](https://github.com/cv-rits/CoMoGAN)
  * [Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation](https://arxiv.org/abs/2008.00951)<br>:star:[code](https://github.com/eladrich/pixel2style2pixel):house:[project](https://eladrich.github.io/pixel2style2pixel/)<br>
* å›¾åƒç¼–è¾‘
  * [StyleMapGAN: Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing](https://arxiv.org/abs/2104.14754)<br>:star:[code](https://github.com/naver-ai/StyleMapGAN):tv:[video](https://www.youtube.com/watch?v=qCapNyRA_Ng)

<a name="14"/> 

## 14.Few-Shot/Zero-Shot Learning,Domain Generalization/Adaptation(å°/é›¶æ ·æœ¬å­¦ä¹ ï¼ŒåŸŸé€‚åº”ï¼ŒåŸŸæ³›åŒ–)

* å°æ ·æœ¬å­¦ä¹ 
  * [Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning](https://arxiv.org/abs/2103.01315)<br>
  * [Learning Dynamic Alignment via Meta-filter for Few-shot Learning](https://arxiv.org/abs/2103.13582)<br>[ä½œè€…ä¸»é¡µ](https://yanweifu.github.io/page3.html)<br>é€šè¿‡å…ƒå·ç§¯æ ¸å®ç°åŸºäºåŠ¨æ€å¯¹é½çš„å°æ ·æœ¬å­¦ä¹ <br>è§£è¯»ï¼š[17](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* åŸŸæ³›åŒ–
  * [FSDR: Frequency Space Domain Randomization for Domain Generalization](https://arxiv.org/abs/2103.02370)<br>å— JPEG å°†ç©ºé—´å›¾åƒè½¬æ¢ä¸ºå¤šä¸ªé¢‘ç‡åˆ†é‡(FCs)çš„å¯å‘ï¼Œæå‡ºé¢‘ç‡ç©ºé—´åŸŸéšæœºåŒ–(FSDR)ï¼Œé€šè¿‡ä¿ç•™åŸŸå˜é‡FCs(DIFs)å’ŒåªéšæœºåŒ–åŸŸå˜é‡FCs(DVFs)æ¥éšæœºåŒ–é¢‘ç‡ç©ºé—´çš„å›¾åƒã€‚
  * [Domain Generalization via Inference-time Label-Preserving Target Projections](https://arxiv.org/abs/2103.01134)
  * [Adaptive Methods for Real-World Domain Generalization](https://arxiv.org/abs/2103.15796)<br>:open_mouth: Oral
  * [Progressive Domain Expansion Network for Single Domain Generalization](https://arxiv.org/abs/2103.16050)<br>:star:[code](https://github.com/lileicv/PDEN)
  * [A Fourier-based Framework for Domain Generalization](https://arxiv.org/abs/2105.11120)<br>:open_mouth:oral
* é›¶æ ·æœ¬å­¦ä¹ 
  * [Goal-Oriented Gaze Estimation for Zero-Shot Learning](https://arxiv.org/abs/2103.03433):star:[code](https://github.com/osierboy/GEM-ZSL)
  * [Contrastive Embedding for Generalized Zero-Shot Learning](https://arxiv.org/abs/2103.16173)<br>:star:[code](https://github.com/Hanzy1996/CE-GZSL)
* åŸŸé€‚åº”
  * [Dynamic Transfer for Multi-Source Domain Adaptation](https://arxiv.org/abs/2103.10583)<br>:star:[code](https://github.com/liyunsheng13/DRT)  
  * [Transferable Semantic Augmentation for Domain Adaptation](https://arxiv.org/abs/2103.12562)
  * [MetaAlign: Coordinating Domain Alignment and Classification for Unsupervised Domain Adaptation](https://arxiv.org/abs/2103.13575)
  * [DRANet: Disentangling Representation and Adaptation Networks for Unsupervised Cross-Domain Adaptation](https://arxiv.org/abs/2103.13447)
  * [Dynamic Domain Adaptation for Efficient Inference](https://arxiv.org/abs/2103.16403)<br>:star:[code](https://github.com/BIT-DA/DDA)
  * [Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation](https://arxiv.org/abs/2103.16765)<br>:house:[project](http://xyue.io/pcs-fuda/)
  * [Domain Consensus Clustering for Universal Domain Adaptation](http://reler.net/papers/guangrui_cvpr2021.pdf)<br>:star:[code](https://github.com/Solacex/Domain-Consensus-Clustering)
  * [Divergence Optimization for Noisy Universal Domain Adaptation](https://arxiv.org/abs/2104.00246)
  * [Curriculum Graph Co-Teaching for Multi-Target Domain Adaptation](https://arxiv.org/abs/2104.00808)<br>:star:[code](https://github.com/Evgeneus/Graph-Domain-Adaptaion):house:[project](https://roysubhankar.github.io/graph-coteaching-adaptation/)
  * [Instance Level Affinity-Based Transfer for Unsupervised Domain Adaptation](https://arxiv.org/abs/2104.01286)<br>:star:[code](https://github.com/astuti/ILA-DA)
  * [Unsupervised Multi-source Domain Adaptation Without Access to Source Data](https://arxiv.org/abs/2104.01845)
  * [Domain Adaptation with Auxiliary Target Domain-Oriented Classifier](https://arxiv.org/pdf/2007.04171.pdf)<br>:star:[code](https://github.com/tim-learn/ATDOC)
  * [Cross-Domain Adaptive Clustering for Semi-Supervised Domain Adaptation](https://arxiv.org/abs/2104.09415)
  * [Learning to Relate Depth and Semantics for Unsupervised Domain Adaptation](https://arxiv.org/abs/2105.07830)<br>:star:[code](https://github.com/susaha/ctrl-uda)
  * [PixMatch: Unsupervised Domain Adaptation via Pixelwise Consistency Training](https://arxiv.org/abs/2105.08128)

<a name="13"/> 

## 13.Image/Video Retrieval(å›¾åƒ/è§†é¢‘æ£€ç´¢)
* [Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers](https://arxiv.org/abs/2103.16553)
* [Convolutional Hough Matching](https://arxiv.org/abs/2103.16831)<br>:open_mouth:oral:house:[project](http://cvlab.postech.ac.kr/research/CHM/)
* [T2VLAD: Global-Local Sequence Alignment for Text-Video Retrieval](https://arxiv.org/abs/2104.10054)
* å›¾åƒæ£€ç´¢
  * [Probabilistic Embeddings for Cross-Modal Retrieval](https://arxiv.org/abs/2101.05068)<br>
  * [QAIR: Practical Query-efficient Black-Box Attacks for Image Retrieval](https://arxiv.org/abs/2103.02927)
  * [More Photos are All You Need: Semi-Supervised Learning for Fine-Grained Sketch Based Image Retrieval](https://arxiv.org/abs/2103.13990)<br>:star:[code](https://github.com/AyanKumarBhunia/semisupervised-FGSBIR)
  * [StyleMeUp: Towards Style-Agnostic Sketch-Based Image Retrieval](https://arxiv.org/abs/2103.15706)
  * [Prototype-supervised Adversarial Network for Targeted Attack of Deep Hashing](https://arxiv.org/abs/2105.07553)<br>:star:[code](https://github.com/xunguangwang/ProS-GAN)
* è§†é¢‘æ£€ç´¢
  * [On Semantic Similarity in Video Retrieval](https://arxiv.org/abs/2103.10095)<br>:star:[code](https://github.com/mwray/Semantic-Video-Retrieval):house:[project](https://mwray.github.io/SSVR/):tv:[video](https://youtu.be/pS9qa_B771I)
* è§†è§‰æœç´¢
  * [Compatibility-aware Heterogeneous Visual Search](https://arxiv.org/abs/2105.06047)

<a name="12"/> 

## 12.Image Quality Assessment(å›¾åƒè´¨é‡è¯„ä¼°)

* å›¾åƒæ¢å¤Image Restoration
  * [Multi-Stage Progressive Image Restoration](https://arxiv.org/abs/2102.02808)<br>:star:[code](https://github.com/swz30/MPRNet)<br>
  * [See through Gradients: Image Batch Recovery via GradInversion](https://arxiv.org/abs/2104.07586)
* å»é˜´å½±Shadow Removal
  * [Auto-Exposure Fusion for Single-Image Shadow Removal](https://arxiv.org/abs/2103.01255)<br>:star:[code](https://github.com/tsingqguo/exposure-fusion-shadow-removal)<br>
  * [From Shadow Generation to Shadow Removal](https://arxiv.org/abs/2103.12997)<br>:star:[code](https://github.com/hhqweasd/G2R-ShadowNet)
* å»æ¨¡ç³ŠDeblurring
  * [DeFMO: Deblurring and Shape Recovery of Fast Moving Objects](https://arxiv.org/abs/2012.00595)<br>:star:[code](https://github.com/rozumden/DeFMO):tv:[video](https://www.youtube.com/watch?v=pmAynZvaaQ4)<br>
  * [ARVo: Learning All-Range Volumetric Correspondence for Video Deblurring](https://arxiv.org/abs/2103.04260)
  * [Explore Image Deblurring via Blur Kernel Space](https://arxiv.org/abs/2104.00317)
  * [Towards Rolling Shutter Correction and Deblurring in Dynamic Scenes](https://arxiv.org/abs/2104.01601)<br>:star:[code](https://github.com/zzh-tech/RSCD)
* å»åå°„Reflection Removal
  * [Robust Reflection Removal with Reflection-free Flash-only Cues](https://arxiv.org/abs/2103.04273)<br>:star:[ccode](https://github.com/ChenyangLEI/flash-reflection-removal)
* å»é›¾
  * Learning to Restore Hazy Video: A New Real-World Dataset and A New Method<br>å­¦ä¹ å¤åŸæœ‰é›¾è§†é¢‘ï¼šä¸€ç§æ–°çš„çœŸå®æ•°æ®é›†åŠç®—æ³•<br>è§£è¯»ï¼š[9](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [Contrastive Learning for Compact Single Image Dehazing](https://arxiv.org/abs/2104.09367)<br>:star:[code](https://github.com/GlassyWu/AECR-Net)<br>åŸºäºå¯¹æ¯”å­¦ä¹ çš„ç´§å‡‘å›¾åƒå»é›¾æ–¹æ³•<br>è§£è¯»ï¼š[5](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* å»å™ªDenoising
  * [Neighbor2Neighbor: Self-Supervised Denoising from Single Noisy Images](https://arxiv.org/abs/2101.02824)<br>è§£è¯»ï¼š[CVPR 2021 | Neighbor2Neighborï¼šä»…éœ€å™ªå£°å›¾åƒå³å¯è®­ç»ƒä»»æ„é™å™ªç½‘ç»œçš„æ–¹æ³•](https://mp.weixin.qq.com/s/Eg7vbjTILSd1Si3HSyz3CA)
  * [NBNet: Noise Basis Learning for Image Denoising with Subspace Projection](https://arxiv.org/abs/2012.15028)<br>ç²—è§£ï¼š[9](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
  * [Invertible Denoising Network: A Light Solution for Real Noise Removal](https://arxiv.org/abs/2104.10546)<br>:star:[code](https://github.com/Yang-Liu1082/InvDN)
  * [FBI-Denoiser: Fast Blind Image Denoiser for Poisson-Gaussian Noise](https://arxiv.org/abs/2105.10967)<br>:star:[code](https://github.com/csm9493/FBI-Denoiser)
* å»é›¨Deraining
  * [Semi-Supervised Video Deraining with Dynamic Rain Generator](https://arxiv.org/abs/2103.07939)
  * [Closing the Loop: Joint Rain Generation and Removal via Disentangled Image Translation](https://arxiv.org/abs/2103.13660)
* æ›å…‰æ ¡æ­£
  * [Learning Multi-Scale Photo Exposure Correction](https://arxiv.org/abs/2003.11596)<br>:star:[code](https://github.com/mahmoudnafifi/Exposure_Correction)
* å›¾åƒä¿®å¤Image Inpainting
  * [Generating Diverse Structure for Image Inpainting With Hierarchical VQ-VAE](https://arxiv.org/abs/2103.10022)<br>:star:[code](https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting)
  * [TransFill: Reference-guided Image Inpainting by Merging Multiple Color and Spatial Transformations](https://arxiv.org/abs/2103.15982)<br>:house:[project](https://yzhouas.github.io/projects/TransFill/index.html)
  * [Image Inpainting with External-internal Learning and Monochromic Bottleneck](https://arxiv.org/abs/2104.09068)<br>:star:[code](https://github.com/Tengfei-Wang/external-internal-inpainting)
  * [PD-GAN: Probabilistic Diverse GAN for Image Inpainting](https://arxiv.org/abs/2105.02201)<br>:star:[code](https://github.com/KumapowerLIU/PD-GAN)
* å›¾åƒç¼–è¾‘
  * [DeFLOCNet: Deep Image Editing via Flexible Low-level Controls](https://arxiv.org/abs/2103.12723)<br>:star:[code](https://github.com/KumapowerLIU/DeFLOCNet)
* å›¾åƒå‹ç¼©
  * [Attention-guided Image Compression by Deep Reconstruction of Compressive Sensed Saliency Skeleton](https://arxiv.org/abs/2103.15368)
  * [Slimmable Compressive Autoencoders for Practical Neural Image Compression](https://arxiv.org/abs/2103.15726)<br>:star:[code](https://github.com/FireFYF/SlimCAE)
  * [Checkerboard Context Model for Efficient Learned Image Compression](https://arxiv.org/abs/2103.15306)
  * [Learning Scalable â„“âˆ-constrained Near-lossless Image Compression via Joint Lossy Image and Residual Compression](https://arxiv.org/abs/2103.17015)<br>:star:[code](https://github.com/BYchao100/Scalable-Near-lossless-Image-Compression)
  * [Deep Homography for Efficient Stereo Image Compression](http://buaamc2.net/pdf/cvpr21hesic.pdf)<br>:star:[code](https://github.com/ywz978020607/HESIC)<br>åˆ†äº«ä¼š
* de-rendering
  * [De-rendering the World's Revolutionary Artefacts](https://arxiv.org/abs/2104.03954)<br>:house:[project](https://sorderender.github.io/):tv:[video](https://www.youtube.com/watch?v=pxkYyyw02H0)
* è§†é¢‘ä¿®å¤
  * [Progressive Temporal Feature Alignment Network for Video Inpainting](https://arxiv.org/abs/2104.03507)<br>:star:[code](https://github.com/MaureenZOU/TSAM)<br>ä½œè€…æå‡º Progressive Temporal Feature Alignment Networkï¼Œåˆ©ç”¨å…‰æµä»ç›¸é‚»å¸§ä¸­æå–çš„ç‰¹å¾é€æ­¥ä¸°å¯Œå½“å‰å¸§çš„ç‰¹å¾ã€‚çº æ­£äº†æ—¶ç©ºç‰¹å¾ä¼ æ’­é˜¶æ®µçš„ spatial misalignmentï¼Œæå¤§åœ°æé«˜äº† inpainted videos çš„è§†è§‰è´¨é‡å’Œæ—¶ç©ºä¸€è‡´æ€§ã€‚åœ¨ DAVIS å’Œ FVI æ•°æ®é›†ä¸Šå®ç°äº†ä¸ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•ç›¸æ¯”çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚
* æ¶ˆé™¤å›¾åƒä¼ªå½±
  * [Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network](https://arxiv.org/abs/2104.09556)<br>:star:[code](https://github.com/jnjaby/DISCNet):house:[project](https://jnjaby.github.io/projects/UDC/) 
* å›¾åƒå¯¹é½
  * [Deep Lucas-Kanade Homography for Multimodal Image Alignment](https://arxiv.org/abs/2104.11693)<br>:star:[code](https://github.com/placeforyiming/CVPR21-Deep-Lucas-Kanade-Homography)

<a name="11"/> 

## 11. Face(äººè„¸æŠ€æœ¯)

* [Towards High Fidelity Face Relighting with Realistic Shadows](https://arxiv.org/abs/2104.00825)<br>:star:[code](https://github.com/andrewhou1/Shadow-Mask-Face-Relighting)
* [IronMask: Modular Architecture for Protecting Deep Face Template](https://arxiv.org/abs/2104.02239)
* [Everything's Talkin': Pareidolia Face Reenactment](https://arxiv.org/abs/2104.03061)
* äººè„¸è¯†åˆ«
  * [A 3D GAN for Improved Large-pose Facial Recognition](https://arxiv.org/pdf/2012.10545.pdf)<br>
  * [When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework](https://arxiv.org/abs/2103.01520)<br>:open_mouth:oral:star:[code](https://github.com/Hzzone/MTLFace)<br>
  * [MagFace: A Universal Representation for Face Recognition and Quality Assessment](https://arxiv.org/abs/2103.06627)<br>:open_mouth:oral:star:[code](https://github.com/IrvingMeng/MagFace)<br>äººè„¸è¯†åˆ«+è´¨é‡ï¼Œä»Šå¹´çš„Oral presentationã€‚ ä»£ç å¾…æ•´ç†
  * [WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition](https://arxiv.org/abs/2103.04098)<br>:house:[project](https://www.face-benchmark.org/)
  * [ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis](https://arxiv.org/abs/2103.05630)<br>:open_mouth:oral:house:[project](https://yinanhe.github.io/projects/forgerynet.html):tv:[video](https://youtu.be/e8XIL3Di2Y8) 
  * Spherical Confidence Learning for Face Recognition<br>:open_mouth:oral<br>åŸºäºè¶…çƒæµå½¢ç½®ä¿¡åº¦å­¦ä¹ çš„äººè„¸è¯†åˆ«
  * Consistent Instance False Positive Improves Fairness in Face Recognition<br>åŸºäºå®ä¾‹è¯¯æŠ¥ä¸€è‡´æ€§çš„äººè„¸è¯†åˆ«å…¬å¹³æ€§æå‡æ–¹æ³•<br>è§£è¯»ï¼š[7](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [CRFace: Confidence Ranker for Model-Agnostic Face Detection Refinement](https://arxiv.org/abs/2103.07017)
  * [Cross-Domain Similarity Learning for Face Recognition in Unseen Domains](https://arxiv.org/abs/2103.07503)
  * [HLA-Face: Joint High-Low Adaptation for Low Light Face Detection](https://arxiv.org/abs/2104.01984)<br>:house:[project](https://daooshee.github.io/HLA-Face-Website/)
  * [FACESEC: A Fine-grained Robustness Evaluation Framework for Face Recognition Systems](https://arxiv.org/abs/2104.04107)
  * [Dynamic Class Queue for Large Scale Face Recognition In the Wild](https://arxiv.org/abs/2105.11113)<br>:star:[code](https://github.com/bilylee/DCQ)
* åˆæˆäººè„¸ï¼ˆDeepfake/Face Forgeryï¼‰æ£€æµ‹
  * [Multi-attentional Deepfake Detection](https://arxiv.org/abs/2103.02406)<br>
  * [Frequency-aware Discriminative Feature Learning Supervised by Single-Center Loss for Face Forgery Detection](https://arxiv.org/abs/2103.09096)
  * [MagDR: Mask-guided Detection and Reconstruction for Defending Deepfakes](https://arxiv.org/abs/2103.14211)
  * [Face Forensics in the Wild](https://arxiv.org/abs/2103.16076)<br>:open_mouth:oral:star:[code](https://github.com/tfzhou/FFIW)
  * [Improving the Efficiency and Robustness of Deepfakes Detection through Precise Geometric Features](https://arxiv.org/abs/2104.04480)<br>:star:[code](https://github.com/frederickszk/LRNet)
* äººè„¸è´¨é‡è¯„ä¼°
  * [SDD-FIQA: Unsupervised Face Image Quality Assessment with Similarity Distribution Distance](https://arxiv.org/abs/2103.05977)<br>åŸºäºç›¸ä¼¼åº¦åˆ†å¸ƒè·ç¦»çš„æ— ç›‘ç£äººè„¸è´¨é‡è¯„ä¼°<br>è§£è¯»ï¼š[6](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* 3Däººè„¸é‡å»º
  * Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection<br>:open_mouth:oral<br>åœ¨å¼€æ”¾çš„äººåƒé›†åˆä¸­å­¦ä¹ 3Däººè„¸çš„èšåˆä¸ç‰¹å¼‚åŒ–é‡å»º
  * [3DCaricShop: A Dataset and A Baseline Method for Single-view 3D Caricature Face Reconstruction](https://arxiv.org/abs/2103.08204)<br>:star:[code](https://github.com/qiuyuda/3DCaricShop):house:[project](https://qiuyuda.github.io/3DCaricShop/)
  * [Riggable 3D Face Reconstruction via In-Network Optimization](https://arxiv.org/abs/2104.03493)<br>:star:[code](https://github.com/zqbai-jeremy/INORig)<br>æœ¬æ–‡é€šè¿‡ä¸€ä¸ªåµŒå…¥äº†ç½‘ç»œå†…ä¼˜åŒ–çš„ç«¯åˆ°ç«¯å¯è®­ç»ƒç½‘ç»œï¼Œè§£å†³äº†ä»å•ç›® RGB å›¾åƒä¸­ riggable 3D äººè„¸é‡å»ºã€‚å¹¶ä¸”è¾¾åˆ°äº†æœ€å…ˆè¿›çš„é‡å»ºç²¾åº¦ï¼Œåˆç†çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åº”ç”¨äºæ ‡å‡†çš„ face rig åº”ç”¨ï¼Œå¦‚é‡å®šä½ã€‚
  * [Pixel Codec Avatars](https://arxiv.org/abs/2104.04638)<br>:open_mouth:oral
  * [Inverting Generative Adversarial Renderer for Face Reconstruction](https://arxiv.org/pdf/2105.02431.pdf)<br>:star:[code](https://github.com/WestlyPark/StyleRenderer)
* äººè„¸è¡¨æƒ…è¯†åˆ«
  * [Affective Processes: stochastic modelling of temporal context for emotion and facial expression recognition](https://arxiv.org/abs/2103.13372)<br>
  * [Dive into Ambiguity: Latent Distribution Mining and Pairwise Uncertainty Estimation for Facial Expression Recognition](https://arxiv.org/abs/2104.00232)
  * [Feature Decomposition and Reconstruction Learning for Effective Facial Expression Recognition](https://arxiv.org/abs/2104.05160)
* äººè„¸èšç±» 
  * [Structure-Aware Face Clustering on a Large-Scale Graph with 10^7 Nodes](https://arxiv.org/abs/2103.13225)<br>:star:[code](https://github.com/sstzal/STAR-FC):house:[project](https://sstzal.github.io/STAR-FC/)
* äººè„¸ç¼–è¾‘
  * [High-Fidelity and Arbitrary Face Editing](https://arxiv.org/abs/2103.15814)
* äººè„¸è·Ÿè¸ª
  * [High-fidelity Face Tracking for AR/VR via Deep Lighting Adaptation](https://arxiv.org/abs/2103.15876)<br>:house:[project](https://www.cs.rochester.edu/u/lchen63/):tv:[video](https://www.youtube.com/watch?v=dtz1LgZR8cc)
* å¹¿è§’äººè„¸çŸ«æ­£
  * [Practical Wide-Angle Portraits Correction with Deep Structured Models](https://arxiv.org/abs/2104.12464)<br>ç²—è§£ï¼š[7](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* äººè„¸æ´»ä½“æ£€æµ‹
  * [Cross Modal Focal Loss for RGBD Face Anti-Spoofing](https://arxiv.org/abs/2103.00948)<br>
* éŸ³é¢‘é©±åŠ¨åˆæˆèµ‹æœ‰æƒ…æ„Ÿçš„äººè„¸
  * [Audio-Driven Emotional Video Portraits](https://arxiv.org/abs/2104.07452)<br>:star:[code](https://github.com/jixinya/EVP/):house:[project](https://jixinya.github.io/projects/evp/)
* æ¢è„¸
  * Information Bottleneck Disentanglement for Identity Swapping<br>åˆ†äº«ä¼š
  * [One Shot Face Swapping on Megapixels](https://arxiv.org/abs/2105.04932)<br>:sunflower:[dataset](https://github.com/zyainfal/One-Shot-Face-Swapping-on-Megapixels)
* äººè„¸ä¿®å¤
  * FaceInpainter: High Fidelity Face Adaptation to Heterogeneous Domains<br>åˆ†äº«ä¼š
* äººè„¸åŠ¨ç”»
  * [Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation](https://arxiv.org/abs/2104.11116)<br>:star:[code](https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS):house:[project](https://hangz-nju-cuhk.github.io/projects/PC-AVS):tv:[video](https://www.youtube.com/watch?v=lNQQHIggnUg)

<a name="10"/> 

## 10.Neural Architecture Search(ç¥ç»æ¶æ„æœç´¢)

- [AttentiveNAS: Improving Neural Architecture Search via Attentive](https://arxiv.org/pdf/2011.09011.pdf)<br>
- [HourNAS: Extremely Fast Neural Architecture Search Through an Hourglass Lens](https://arxiv.org/pdf/2005.14446.pdf)<br>
- [ReNAS: Relativistic Evaluation of Neural Architecture Search](https://arxiv.org/pdf/1910.01523.pdf)<br>
- [OPANAS: One-Shot Path Aggregation Network Architecture Search for Object](https://arxiv.org/abs/2103.04507)
- Towards Improving the Consistency, Efficiency, and Flexibility of Differentiable Neural Architecture Search<br>åŒ—äº¬å¤§å­¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æœºå™¨å­¦ä¹ ç ”ç©¶ä¸­å¿ƒ
- [Contrastive Neural Architecture Search with Neural Architecture Comparators](https://arxiv.org/abs/2103.05471)<br>:star:[code](https://arxiv.org/abs/2103.05471)
- [Searching by Generating: Flexible and Efficient One-Shot NAS with Architecture Generator](https://arxiv.org/abs/2103.07289)<br>:star:[code](https://github.com/eric8607242/SGNAS)
- [Prioritized Architecture Sampling with Monto-Carlo Tree Search](https://arxiv.org/abs/2103.11922)<br>:star:[code](https://github.com/xiusu/NAS-Bench-Macro)
* [One-Shot Neural Ensemble Architecture Search by Diversity-Guided Search Space Shrinking](https://arxiv.org/abs/2104.00597)<br>:star:[code](https://github.com/researchmm/NEAS)
* [NetAdaptV2: Efficient Neural Architecture Search with Fast Super-Network Training and Architecture Optimization](https://arxiv.org/abs/2104.00031)<br>:house:[project](http://web.mit.edu/netadapt/)
* [Neural Architecture Search with Random Labels](https://arxiv.org/abs/2101.11834)<br>ç²—è§£ï¼š[1](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* [Landmark Regularization: Ranking Guided Super-Net Training in Neural Architecture Search](https://arxiv.org/abs/2104.05309)<br>:star:[code](https://github.com/kcyu2014/nas-landmarkreg)
* [ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search](https://arxiv.org/abs/2105.10154)
* [TransNAS-Bench-101: Improving Transferability and Generalizability of Cross-Task Neural Architecture Search](https://arxiv.org/abs/2105.11871)<br>:sunflower:[dataset](https://download.mindspore.cn/dataset/TransNAS-Bench-101/)

<a name="9"/> 

## 9.Object Tracking(ç›®æ ‡è·Ÿè¸ª)

- [Rotation Equivariant Siamese Networks for Tracking](https://arxiv.org/abs/2012.13078)<br>
- [Multiple Object Tracking with Correlation Learning](https://arxiv.org/abs/2104.03541)<br>æå‡º CorrTrackerï¼Œä¸€ä¸ªç»Ÿä¸€çš„å…³è”è·Ÿè¸ªå™¨ï¼Œå¯ä»¥å¯†é›†å»ºæ¨¡ç›®æ ‡ä¹‹é—´çš„å…³è”ï¼Œå¹¶é€šè¿‡å…³è”ä¼ é€’ä¿¡æ¯ã€‚åœ¨ MOT17 ä¸Šè·å¾—æœ€å…ˆè¿›çš„ MOTA 76.5% å’Œ IDF1 73.6%ã€‚
- [LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search](https://arxiv.org/abs/2104.14545)<br>:star:[code](https://github.com/researchmm/LightTrack)<br>LightTrackï¼šç”¨ç¥ç»æ¶æ„æœç´¢å¾—åˆ°çš„è½»é‡çº§è·Ÿè¸ªç½‘ç»œï¼Œç²¾åº¦è¶…è¿‡SiamRPN++ å’Œ Oceanï¼Œé€Ÿåº¦å¿«12å€ï¼Œå‚æ•°é‡åªæœ‰1/13ï¼ŒFlopsä»…æœ‰1/38ã€‚ä»£ç å°†å¼€æºã€‚
- [Track, Check, Repeat: An EM Approach to Unsupervised Tracking](https://arxiv.org/abs/2104.03424)<br>:house:[project](http://www.cs.cmu.edu/~aharley/em_cvpr21/):tv:[video](https://youtu.be/Jg2f5fkgxZo)
* å¤šç›®æ ‡è·Ÿè¸ª
  * [Probabilistic Tracklet Scoring and Inpainting for Multiple Object Tracking](https://arxiv.org/abs/2012.02337)<br>
  * [Track to Detect and Segment: An Online Multi-Object Tracker](https://arxiv.org/abs/2103.08808)<br>:star:[code](https://github.com/JialianW/TraDeS):house:[project](https://jialianwu.com/projects/TraDeS.html):tv:[video](https://youtu.be/oGNtSFHRZJA)<br>TraDeS ï¼šCVPR 2021å¤šç›®æ ‡è·Ÿè¸ªç®—æ³•ï¼Œæ”¹è¿›äº†ç›®å‰è”åˆæ£€æµ‹ä¸è·Ÿè¸ªçš„åœ¨çº¿æ–¹æ³•ï¼Œä½¿ç”¨è·Ÿè¸ªçº¿ç´¢è¾…åŠ©æ£€æµ‹ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†å®ç°äº†å¤§å¹…ç²¾åº¦æå‡ï¼Œä½œè€…æ¥è‡ªçº½çº¦å·ç«‹å¤§å­¦ã€‚ä»£ç å·²å¼€æºã€‚
  * Multiple Object Tracking with Correlation Learning
  * [Learning a Proposal Classifier for Multiple Object Tracking](https://arxiv.org/abs/2103.07889)<br>:star:[code](https://github.com/daip13/LPC_MOT)
  * [Learnable Graph Matching: Incorporating Graph Partitioning with Deep Feature Learning for Multiple Object Tracking](https://arxiv.org/abs/2103.16178)<br>:star:[code](https://github.com/jiaweihe1996/GMTracker)
  * [Online Multiple Object Tracking with Cross-Task Synergy](https://arxiv.org/abs/2104.00380)<br>:star:[code](https://github.com/songguocode/TADAM)
  * [SiamMOT: Siamese Multi-Object Tracking](https://arxiv.org/abs/2105.11595)<br>:star:[code](https://github.com/amazon-research/siam-mot)
* è§†è§‰ç›®æ ‡è·Ÿè¸ª
  * [IoU Attack: Towards Temporally Coherent Black-Box Adversarial Attack for Visual Object Tracking](https://arxiv.org/abs/2103.14938)<br>:star:[code](https://github.com/VISION-SJTU/IoUattack)
  * [Learning to Track Instances without Video Annotations](https://arxiv.org/abs/2104.00287)<br>:open_mouth:oral:house:[project](https://oasisyang.github.io/projects/semi-track/index.html):tv:[video](https://youtu.be/-S7xtk-7pGk)
* å•ç›®æ ‡è·Ÿè¸ª
  * [Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark](https://arxiv.org/abs/2103.16746)<br>:house:[project](https://sites.google.com/view/langtrackbenchmark/):tv:[video](https://www.youtube.com/watch?v=7lvVDlkkff0)
  * [SiamGAT: Graph Attention Tracking](https://arxiv.org/abs/2011.11204)<br>:star:[code](https://github.com/ohhhyeahhh/SiamGAT)
 
<a name="8"/> 

## 8.Image Segmentation(å›¾åƒåˆ†å‰²)

- [Information-Theoretic Segmentation by Inpainting Error Maximization](https://arxiv.org/abs/2012.07287)<br>
- [Simultaneously Localize, Segment and Rank the Camouflaged Objects](https://arxiv.org/abs/2103.04011)<br>:star:[code](https://github.com/JingZhang617/COD-Rank-Localize-and-Segment)
- [Capturing Omni-Range Context for Omnidirectional Segmentation](https://arxiv.org/abs/2103.05687)<br>:star:[code](https://github.com/elnino9ykl/WildPASS)
- [Boundary IoU: Improving Object-Centric Image Segmentation Evaluation](https://arxiv.org/abs/2103.16562)<br>:star:[code](https://github.com/bowenc0221/boundary-iou-api):house:[project](https://bowenc0221.github.io/boundary-iou/)
* [Locate then Segment: A Strong Pipeline for Referring Image Segmentation](https://arxiv.org/abs/2103.16284)
* [InverseForm: A Loss Function for Structured Boundary-Aware Segmentation](https://arxiv.org/abs/2104.02745)<br>:open_mouth:oral
* [Omnimatte: Associating Objects and Their Effects in Video](https://arxiv.org/abs/2105.06993)<br>:open_mouth:oral:house:[project](https://omnimatte.github.io/)
* å®ä¾‹åˆ†å‰²
  * [Zero-Shot Instance Segmentation](https://arxiv.org/abs/2104.06601)<br>:star:[code](https://github.com/zhengye1995/Zero-shot-Instance-Segmentation)<br>åˆ›æ–°å¥‡æ™ºé¦–æ¬¡æå‡ºé›¶æ ·æœ¬å®ä¾‹åˆ†å‰²ï¼ŒåŠ©åŠ›è§£å†³å·¥ä¸šåœºæ™¯æ•°æ®ç“¶é¢ˆéš¾é¢˜
  * [Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers](https://arxiv.org/abs/2103.12340)<br>:star:[code](https://github.com/lkeab/BCNet)
  * [Weakly Supervised Instance Segmentation for Videos with Temporal Mask Consistency](https://arxiv.org/abs/2103.12886)
  * [FAPIS: A Few-shot Anchor-free Part-based Instance Segmenter](https://arxiv.org/abs/2104.00073)
  * [Weakly-supervised Instance Segmentation via Class-agnostic Learning with Salient Images](https://arxiv.org/abs/2104.01526)<br>:star:[code](https://github.com/hustvl/BoxCaseg)
  * [Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation](https://arxiv.org/abs/2104.05239)<br>:star:[code](https://github.com/tinyalpha/BPR)
  * [RefineMask: Towards High-Quality Instance Segmentation with Fine-Grained Features](https://arxiv.org/abs/2104.08569)<br>:star:[code](https://github.com/zhanggang001/RefineMask)
  * [A^2-FPN: Attention Aggregation based Feature Pyramid Network for Instance Segmentation](https://arxiv.org/abs/2105.03186)
  * [Incremental Few-Shot Instance Segmentation](https://arxiv.org/abs/2105.05312)<br>:star:[code](https://github.com/danganea/iMTFA)
* å…¨æ™¯åˆ†å‰²
  * [4D Panoptic LiDAR Segmentation](https://arxiv.org/abs/2102.12472)<br>
  * [Cross-View Regularization for Domain Adaptive Panoptic Segmentation](https://arxiv.org/abs/2103.02584)<br>:open_mouth:oral<br>ç”¨äºåŸŸè‡ªé€‚åº”å…¨æ™¯åˆ†å‰²çš„è·¨è§†å›¾æ­£åˆ™åŒ–æ–¹æ³•<br>
  * Part-aware Panoptic Segmentation<br>
  * Toward Joint Thing-and-Stuff Mining for Weakly Supervised Panoptic Segmentation<br>è”åˆç‰©ä½“å’Œç‰©è´¨æŒ–æ˜çš„å¼±ç›‘ç£å…¨æ™¯åˆ†å‰²<br>è§£è¯»ï¼š[15](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
  * [Panoptic-PolarNet: Proposal-free LiDAR Point Cloud Panoptic Segmentation](https://arxiv.org/abs/2103.14962)<br>:star:[code](https://github.com/edwardzhou130/Panoptic-PolarNet)
  * [Fully Convolutional Networks for Panoptic Segmentation](https://arxiv.org/abs/2012.00720)<br>:open_mouth:oral:star:[code](https://github.com/yanwei-li/PanopticFCN)<br>ç²—è§£ï¼š[11](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
  * [Panoptic Segmentation Forecasting](https://arxiv.org/abs/2104.03962)
  * [Exemplar-Based Open-Set Panoptic Segmentation Network](https://arxiv.org/abs/2105.08336)<br>:star:[code](https://github.com/jd730/EOPSN):house:[project](https://cv.snu.ac.kr/research/EOPSN/)
* è¯­ä¹‰åˆ†å‰²
  * [PLOP: Learning without Forgetting for Continual Semantic Segmentation](https://arxiv.org/abs/2011.11390)<br>:star:[code](https://github.com/arthurdouillard/CVPR2021_PLOP)
  * [Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges](https://arxiv.org/abs/2009.03137)<br>:sunflower:[dataset](https://github.com/QingyongHu/SensatUrban):tv:[video](https://www.youtube.com/watch?v=IG0tTdqB3L8)<br>
  * [Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation](https://arxiv.org/abs/2103.04717)
  * [Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation](https://arxiv.org/abs/2103.04705)
  * [Differentiable Multi-Granularity Human Representation Learning for Instance-Aware Human Semantic Parsing](https://arxiv.org/abs/2103.04570)<br>:open_mouth:oral:star:[code](https://github.com/tfzhou/MG-HumanParsing)
  * [Learning Statistical Texture for Semantic Segmentation](https://arxiv.org/abs/2103.04133)
  * [MetaCorrection: Domain-aware Meta Loss Correction for Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2103.05254)<br>:star:[code](https://github.com/cyang-cityu/MetaCorrection)<br>è¯­ä¹‰åˆ†å‰²ä¸­çš„æ— ç›‘ç£åŸŸé€‚åº”çš„åŸŸæ„ŸçŸ¥å…ƒæŸå¤±æ ¡æ­£
  * [Continual Semantic Segmentation via Repulsion-Attraction of Sparse and Disentangled Latent Representations](https://arxiv.org/abs/2103.06342)
  * [Semantic Segmentation for Real Point Cloud Scenes via Bilateral Augmentation and Adaptive Fusion](https://arxiv.org/abs/2103.07074)<br>:star:[code](https://github.com/ShiQiu0419/BAAF-Net)
  * [Rethinking BiSeNet For Real-time Semantic Segmentation](https://arxiv.org/abs/2104.13188)<br>:star:[code](https://github.com/MichaelFan01/STDC-Seg)
  * [BBAM: Bounding Box Attribution Map for Weakly Supervised Semantic and Instance Segmentation](https://arxiv.org/abs/2103.08907)<br>:star:[code](https://github.com/jbeomlee93/BBAM)
  * [Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2103.08896)<br>:star:[code](https://github.com/jbeomlee93/AdvCAM)
  * [Cross-Dataset Collaborative Learning for Semantic Segmentation](https://arxiv.org/abs/2103.11351)
  * [Coarse-to-Fine Domain Adaptive Semantic Segmentation with Photometric Alignment and Category-Center Regularization](https://arxiv.org/abs/2103.13041)
  * [Non-Salient Region Object Mining for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2103.14581)<br>:star:[code](https://github.com/NUST-Machine-Intelligence-Laboratory/nsrom)
  * [Source-Free Domain Adaptation for Semantic Segmentation](https://arxiv.org/abs/2103.16372)
  * [PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering](https://arxiv.org/abs/2103.17070)
  * [Background-Aware Pooling and Noise-Aware Loss for Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2104.00905)<br>:house:[project](https://cvlab.yonsei.ac.kr/projects/BANA/)
  * [Progressive Semantic Segmentation](https://arxiv.org/abs/2104.03778)
  * [Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization](https://arxiv.org/abs/2104.05833)<br>:house:[project](https://nv-tlabs.github.io/semanticGAN/)
  * [DANNet: A One-Stage Domain Adaptation Network for Unsupervised Nighttime Semantic Segmentation](https://arxiv.org/abs/2104.10834)<br>:open_mouth:oral:star:[code](https://github.com/W-zx-Y/DANNet)<br>å®ç°å¤œé—´è¯­ä¹‰åˆ†å‰²æœ€å…ˆè¿›æ€§èƒ½ï¼Œå·²å¼€æºã€‚
  * [Self-supervised Augmentation Consistency for Adapting Semantic Segmentation](https://arxiv.org/abs/2105.00097)(https://github.com/visinf/da-sac)
  * [Railroad is not a Train: Saliency as Pseudo-pixel Supervision for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2105.08965)<br>:star:[code](https://github.com/halbielee/EPS)
* åœºæ™¯ç†è§£/åœºæ™¯è§£æ
  * [Exploring Data Efficient 3D Scene Understanding with Contrastive Scene Contexts](https://arxiv.org/abs/2012.09165)<br>:open_mouth:oral:house:[project](https://sekunde.github.io/project_efficient/):tv:[video](https://youtu.be/E70xToZLgs4)
  * [Monte Carlo Scene Search for 3D Scene Understanding](https://arxiv.org/abs/2103.07969)
  * [Bidirectional Projection Network for Cross Dimension Scene Understanding](https://arxiv.org/abs/2103.14326)<br>:open_mouth:oral:star:[code](https://github.com/wbhu/BPNet)
  * [RobustNet: Improving Domain Generalization in Urban-Scene Segmentation via Instance Selective Whitening](https://arxiv.org/abs/2103.15597)<br>:open_mouth:oral:star:[code](https://github.com/shachoi/RobustNet)
  * [CoCoNets: Continuous Contrastive 3D Scene Representations](https://arxiv.org/abs/2104.03851)<br>:house:[project](https://mihirp1998.github.io/project_pages/coconets/):tv:[video](https://youtu.be/n_own_d7Fh8)<br>æ¥è‡ªCMUçš„å­¦è€…æå‡ºä¸€ç§3Dåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œåˆ©ç”¨è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ å’Œè¾“å…¥çš„RGBä¸RGBDåœºæ™¯æ•°æ®å­¦ä¹ è€Œæ¥ï¼Œè¿™ç§ç‰¹å¾è¡¨ç¤ºæ–¹æ³•åœ¨ç›®æ ‡è·Ÿè¸ªã€æ£€æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚
  * åœºæ™¯å›¾åˆæˆ/åˆ†æ
    * [SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences](https://arxiv.org/abs/2103.14898)<br>:house:[project](https://shunchengwu.github.io/SceneGraphFusion)
    * [Probabilistic Modeling of Semantic Ambiguity for Scene Graph Generation](https://arxiv.org/abs/2103.05271)<br>åœºæ™¯å›¾ç”Ÿæˆ---åœºæ™¯è§£æ
    * [Exploiting Edge-Oriented Reasoning for 3D Point-based Scene Graph Analysis](https://arxiv.org/abs/2103.05558)<br>:house:[project](https://sggpoint.github.io/)<br>åˆ©ç”¨é¢å‘è¾¹ç¼˜çš„æ¨ç†è¿›è¡ŒåŸºäº3Dç‚¹çš„åœºæ™¯å›¾åˆ†æ---åœºæ™¯ç†è§£
    * [Fully Convolutional Scene Graph Generation](https://arxiv.org/abs/2103.16083)<br>:open_mouth:oral
    * [Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene Graph Generation](https://arxiv.org/abs/2104.00308)<br>:star:[code](https://github.com/Scarecrow0/BGNN-SGG)
* æŠ å›¾
  * [Real-Time High Resolution Background Matting](https://arxiv.org/abs/2012.07810)<br>:star:[code](https://github.com/PeterL1n/BackgroundMattingV2):house:[project](https://grail.cs.washington.edu/projects/background-matting-v2/):tv:[video](https://youtu.be/oMfPTeYDF9g)<br>æœ€æ–°å¼€æºæŠ å›¾æŠ€æœ¯ï¼Œå®æ—¶å¿«é€Ÿé«˜åˆ†è¾¨ç‡ï¼Œ4k(30fps)ã€ç°ä»£GPUï¼ˆ60fpsï¼‰<br>è§£è¯»ï¼š[å•å—GPUå®ç°4Kåˆ†è¾¨ç‡æ¯ç§’30å¸§ï¼Œåç››é¡¿å¤§å­¦å®æ—¶è§†é¢‘æŠ å›¾å†å‡çº§ï¼Œæ¯›å‘ç»†èŠ‚åˆ°ä½](https://mp.weixin.qq.com/s/0OJR3Y5cPfeHhdTdI3BgEA)<br>[æœ€æ–°å¼€æºæŠ å›¾æŠ€æœ¯ï¼Œå®æ—¶å¿«é€Ÿé«˜åˆ†è¾¨ç‡ï¼Œ4k(30fps)ã€ç°ä»£GPUï¼ˆ60fpsï¼‰](https://zhuanlan.zhihu.com/p/337028483)
* åŠ¨ä½œåˆ†å‰²
  * æ—¶åºåŠ¨ä½œåˆ†å‰²
    * [Temporal Action Segmentation from Timestamp Supervision](https://arxiv.org/abs/2103.06669)<br>:star:[code](https://github.com/ZheLi2020/TimestampActionSeg)
    * [Temporally-Weighted Hierarchical Clustering for Unsupervised Action Segmentation](https://arxiv.org/abs/2103.11264)<br>:star:[code](https://github.com/ssarfraz/FINCH-Clustering/tree/master/TW-FINCH)
  * æ— ç›‘ç£åŠ¨ä½œåˆ†å‰²
    * [Action Shuffle Alternating Learning for Unsupervised Action Segmentation](https://arxiv.org/abs/2104.02116)
  * ç›‘ç£åŠ¨ä½œåˆ†å‰²
   * [Anchor-Constrained Viterbi for Set-Supervised Action Segmentation](https://arxiv.org/abs/2104.02113)
  * è§†é¢‘åŠ¨ä½œåˆ†å‰²
    * [Global2Local: Efficient Structure Search for Video Action Segmentation](https://arxiv.org/abs/2101.00910)<br>ä»å…¨å±€åˆ°å±€éƒ¨ï¼šé¢å‘è§†é¢‘åŠ¨ä½œåˆ†å‰²çš„é«˜æ•ˆç½‘ç»œç»“æ„æœç´¢<br>è§£è¯»ï¼š[19](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* é›·è¾¾åˆ†å‰²
  * [Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR Segmentation](https://arxiv.org/abs/2011.10033)<br>:open_mouth:oral:star:[code](https://github.com/xinge008/Cylinder3D)<br>åœ¨ SemanticKITTI æ¦œå•æ’åç¬¬ä¸€ï¼ˆuntil CVPR DDLï¼‰ï¼Œåœ¨ nuScenes ä¸­è·å¾— SOTAï¼Œå¹¶å¯¹å…¶ä»–åŸºäºæ¿€å…‰é›·è¾¾çš„ä»»åŠ¡ä¿æŒäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¿€å…‰é›·è¾¾å…¨æ™¯åˆ†å‰²å’Œæ¿€å…‰é›·è¾¾ä¸‰ç»´æ£€æµ‹ï¼Œå…¶ä¸­å°±åŸºäºæ­¤å·¥ä½œï¼Œåœ¨ SemanticKITTI å…¨æ™¯åˆ†å‰²æ¦œå•ä¹Ÿæ’åç¬¬ä¸€ã€‚
* è§†é¢‘ç›®æ ‡åˆ†å‰²
  * [Modular Interactive Video Object Segmentation:Interaction-to-Mask, Propagation and Difference-Aware Fusion](https://arxiv.org/abs/2103.07941)<br>:open_mouth:oral:star:[code](https://github.com/hkchengrex/MiVOS):house:[project](https://hkchengrex.github.io/MiVOS/):tv:[video](https://hkchengrex.github.io/MiVOS/video.html)
  * [Learning to Recommend Frame for Interactive Video Object Segmentation in the Wild](https://arxiv.org/abs/2103.10391)<br>:star:[code](https://github.com/svip-lab/IVOS-W)
  * [Efficient Regional Memory Network for Video Object Segmentation](https://arxiv.org/abs/2103.12934)<br>:star:[code](https://github.com/hzxie/RMNet):house:[project](https://infinitescript.com/project/rmnet)
  * [Learning Position and Target Consistency for Memory-based Video Object Segmentation](https://arxiv.org/abs/2104.04329)<br>åœ¨ DAVIS å’Œ Youtube-VOS åŸºå‡†ä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ DAVIS 2020 æŒ‘æˆ˜åŠç›‘ç£ VOS ä»»åŠ¡ä¸­æ’åç¬¬ä¸€ã€‚
  * [Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps](https://arxiv.org/abs/2104.10386)<br>:open_mouth:oral:star:[code](https://github.com/yuk6heo/GIS-RAmap)
  * è§†é¢‘å¤šç›®æ ‡åˆ†å‰²
    * [Target-Aware Object Discovery and Association for Unsupervised Video Multi-Object Segmentation](https://arxiv.org/abs/2104.04782)
* è§†é¢‘å®ä¾‹åˆ†å‰²
  * [SG-Net: Spatial Granularity Network for One-Stage Video Instance Segmentation](https://arxiv.org/abs/2103.10284)<br>:star:[code](https://github.com/goodproj13/SG-Net):tv:[video](https://www.youtube.com/watch?v=zft0T3YUgpM)<br>æ–‡ç« ä»‹ç»ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å•é˜¶æ®µæ¡†æ¶ï¼šSG-Netï¼Œä¸ä¼ ç»Ÿçš„ä¸¤é˜¶æ®µæ¡†æ¶ç›¸æ¯”ï¼Œå¯ä»¥æœ‰æ•ˆæé«˜æ©ç è´¨é‡å’Œæ¨ç†é€Ÿåº¦ã€‚
  * [Spatial Feature Calibration and Temporal Fusion for Effective One-stage Video Instance Segmentation](https://arxiv.org/abs/2104.05606)<br>:star:[code](https://github.com/MinghanLi/STMask)
* å°æ ·æœ¬åˆ†å‰²
  * [Self-Guided and Cross-Guided Learning for Few-Shot Segmentation](https://arxiv.org/abs/2103.16129)<br>:star:[code](https://github.com/zbf1991/SCL)
  * [Adaptive Prototype Learning and Allocation for Few-Shot Segmentation](https://arxiv.org/abs/2104.01893)<br>:star:[code](https://github.com/Reagan1311/ASGNet)
* ä¼ªè£…ç›®æ ‡åˆ†å‰²
  * [Camouflaged Object Segmentation with Distraction Mining](https://arxiv.org/abs/2104.10475)<br>:house:[project](https://mhaiyang.github.io/CVPR2021_PFNet/index)
* è§†é¢‘æŠ å›¾
  * [Deep Video Matting via Spatio-Temporal Alignment and Aggregation](https://arxiv.org/abs/2104.11208)<br>:sunflower:[dataset](https://github.com/nowsyn/DVM)
* ç‚¹äº‘åˆ†å‰²
  * [Omni-supervised Point Cloud Segmentation via Gradual Receptive Field Component Reasoning](https://arxiv.org/abs/2105.10203)

<a name="7"/> 

## 7.Object Detection(ç›®æ ‡æ£€æµ‹)

- [Multiple Instance Active Learning for Object Detection](https://arxiv.org/pdf/2104.02324.pdf)<br>:star:[code](https://github.com/yuantn/MIAL)<br>
- Positive-Unlabeled Data Purification in the Wild for Object Detection<br>
- [Depth from Camera Motion and Object Detection](https://arxiv.org/abs/2103.01468)<br>:star:[github](https://github.com/griffbr/ODMD):tv:[video](https://www.youtube.com/watch?v=GruhbdJ2l7k)<br>é€šè¿‡ä½¿ç”¨â€œæ™®é€šæ‰‹æœºæ‘„åƒå¤´è¿åŠ¨+ç›®æ ‡æ£€æµ‹çš„åŒ…å›´æ¡†â€æ•°æ®ï¼Œè®¾è®¡RNNç½‘ç»œå®ç°äº†è¾¾åˆ°æœ€å…ˆè¿›ç²¾åº¦çš„ç›®æ ‡æ·±åº¦ä¼°è®¡ã€‚<br>
- [Towards Open World Object Detection](https://arxiv.org/abs/2103.02603)<br>:open_mouth:oral:star:[code](https://github.com/JosephKJ/OWOD)<br>
- [General Instance Distillation for Object Detection](https://arxiv.org/abs/2103.02340)<br>è¿‘å¹´æ¥ï¼ŒçŸ¥è¯†è’¸é¦å·²è¢«è¯æ˜æ˜¯æ¨¡å‹å‹ç¼©çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚å¯ä»¥ä½¿è½»é‡çº§çš„å­¦ç”Ÿæ¨¡å‹è·å¾—ä»ç¹ççš„æ•™å¸ˆæ¨¡å‹ä¸­æå–çš„çŸ¥è¯†ï¼Œä½†ä»¥å¾€çš„æ£€æµ‹è’¸é¦æ–¹æ³•å¯¹äºä¸åŒçš„æ£€æµ‹æ¡†æ¶çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ï¼Œè€Œä¸”ä¸¥é‡ä¾èµ–ground truthï¼ˆGTï¼‰ï¼Œå¿½ç•¥äº†å®ä¾‹ä¹‹é—´æœ‰ä»·å€¼çš„å…³ç³»ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œä½œè€…åœ¨æœ¬æ–‡ä¸­æå‡ºæ–°çš„åŸºäºåˆ¤åˆ«æ€§å®ä¾‹çš„æ£€æµ‹ä»»åŠ¡è’¸é¦æ–¹æ³•ï¼Œä¸è€ƒè™‘ GT åŒºåˆ†çš„æ­£è´Ÿï¼Œå‘½åä¸ºé€šç”¨å®ä¾‹è’¸é¦ï¼ˆGIDï¼‰ã€‚è¯¥æ–¹æ³•åŒ…å«ä¸€ä¸ªé€šç”¨å®ä¾‹é€‰æ‹©æ¨¡å—(GISM)ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨åŸºäºç‰¹å¾ã€åŸºäºå…³ç³»å’ŒåŸºäºå“åº”çš„çŸ¥è¯†è¿›è¡Œè’¸é¦ã€‚å®éªŒéªŒè¯ï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨å„ç§æ£€æµ‹æ¡†æ¶ä¸­å¯ä»¥å®ç°æ˜¾è‘—çš„ AP æ”¹è¿›ï¼Œç”šè‡³ä¼˜äºæ•™å¸ˆæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒRetinaNet ä¸ ResNet-50 åœ¨ COCO æ•°æ®é›†ä¸Šç”¨ GID å®ç°äº†39.1% çš„ mAPï¼Œæ¯”åŸºçº¿ 36.2% è¶…å‡ºäº† 2.9%ï¼Œç”šè‡³ä¼˜äºåŸºäº ResNet-101 çš„æ•™å¸ˆæ¨¡å‹ 38.1% çš„ APã€‚<br>
- Distilling Object Detectors via Decoupled Features<br>
- [MeGA-CDA: Memory Guided Attention for Category-Aware Unsupervised Domain Adaptive Object Detection](https://arxiv.org/abs/2103.04224)<br>
- Informative and Consistent Correspondence Mining for Cross-Domain Weakly Supervised Object Detection<br>:open_mouth:oral
* [You Only Look One-level Feature](https://arxiv.org/abs/2103.09460)<br>:star:[code](https://github.com/megvii-model/YOLOF)<br>[å¼€æº YOLOFï¼Œæ— éœ€ FPNï¼Œé€Ÿåº¦æ¯” YOLOv4 å¿«13%](https://zhuanlan.zhihu.com/p/357986047)<br>è§£è¯»ï¼š[ç›®æ ‡æ£€æµ‹ç®—æ³•YOLOFï¼šYou Only Look One-level Feature](https://mp.weixin.qq.com/s/0ChHOljrqrXk8yWi_S6ITg)
- [Sparse R-CNN: End-to-End Object Detection with Learnable Proposals](https://arxiv.org/abs/2011.12450)<br>:star:[code](https://github.com/PeizeSun/SparseR-CNN)
- [End-to-End Object Detection with Fully Convolutional Network](https://arxiv.org/abs/2012.03544)<br>:star:[code](https://github.com/Megvii-BaseDetection/DeFCN)<br>è§£è¯»ï¼š[ä¸¢å¼ƒTransformerï¼ŒFCNä¹Ÿå¯ä»¥å®ç°E2Eæ£€æµ‹](https://zhuanlan.zhihu.com/p/332281368)
- [Robust and Accurate Object Detection via Adversarial Learning](https://arxiv.org/abs/2103.13886)
* [I^3Net: Implicit Instance-Invariant Network for Adapting One-Stage Object Detectors](https://arxiv.org/abs/2103.13757)
* [Distilling Object Detectors via Decoupled Features](https://arxiv.org/abs/2103.14475)<br>:star:[code](https://github.com/ggjy/DeFeat.pytorch)
* [OTA: Optimal Transport Assignment for Object Detection](https://arxiv.org/abs/2103.14259)<br>:star:[code](https://github.com/Megvii-BaseDetection/OTA)
* [Scale-aware Automatic Augmentation for Object Detection](https://arxiv.org/abs/2103.17220)<br>:star:[code](https://github.com/Jia-Research-Lab/SA-AutoAug) 
* [A Closer Look at Fourier Spectrum Discrepancies for CNN-generated Images Detection](https://arxiv.org/abs/2103.17195)<br>:open_mouth:oral:house:[project](https://keshik6.github.io/Fourier-Discrepancies-CNN-Detection/)
* [Group Collaborative Learning for Co-Salient Object Detection](https://arxiv.org/abs/2104.01108)<br>:star:[code](https://github.com/fanq15/GCoNet)
* [IQDet: Instance-wise Quality Distribution Sampling for Object Detection](https://arxiv.org/abs/2104.06936)<br>ç²—è§£ï¼š[20](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* [Domain-Specific Suppression for Adaptive Object Detection](https://arxiv.org/abs/2105.03570)
* å°æ ·æœ¬ç›®æ ‡æ£€æµ‹
  * [Semantic Relation Reasoning for Shot-Stable Few-Shot Object Detection](https://arxiv.org/abs/2103.01903)<br>é¦–ä¸ªç ”ç©¶å°‘æ ·æœ¬æ£€æµ‹ä»»åŠ¡çš„è¯­ä¹‰å…³ç³»æ¨ç†ï¼Œå¹¶è¯æ˜å®ƒå¯æå‡å¼ºåŸºçº¿çš„æ½œã€‚ <br> 
  * [Dense Relation Distillation with Context-aware Aggregation for Few-Shot Object Detection](https://arxiv.org/abs/2103.17115)<br>:star:[code](https://github.com/hzhupku/DCNet)<br>åŒ—äº¬å¤§å­¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æœºå™¨å­¦ä¹ ç ”ç©¶ä¸­å¿ƒ<br>
  * [FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding](https://arxiv.org/abs/2103.05950)<br>:star:[code](https://github.com/bsun0802/FSCE)
  * [Generalized Few-Shot Object Detection without Forgetting](https://arxiv.org/abs/2105.09491)<br>ç²—è§£ï¼š[16](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* å¤šç›®æ ‡æ£€æµ‹
  * [There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge](https://arxiv.org/abs/2103.01353)<br>:house:[project](http://multimodal-distill.cs.uni-freiburg.de/)<br>
* 3Dç›®æ ‡æ£€æµ‹
  * [Categorical Depth Distribution Network for Monocular 3D Object Detection](https://arxiv.org/abs/2103.01100)<br>:open_mouth:oral<br>
  * [3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection](https://arxiv.org/abs/2012.04355)<br>:star:[code](https://github.com/thu17cyz/3DIoUMatch):house:[project](https://thu17cyz.github.io/3DIoUMatch/):tv:[video](https://youtu.be/nuARjhkQN2U)<br>æ›´å¤šï¼š[CVPR 2021|åˆ©ç”¨IoUé¢„æµ‹è¿›è¡ŒåŠç›‘ç£å¼3Dç›®æ ‡æ£€æµ‹](https://zhuanlan.zhihu.com/p/354618636)
  * [ST3D: Self-training for Unsupervised Domain Adaptation on 3D ObjectDetection](https://arxiv.org/abs/2103.05346)<br>:star:[code](https://github.com/CVMI-Lab/ST3D)
  * [Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection](https://arxiv.org/abs/2103.16470)<br>:star:[code](https://github.com/fudan-zvg/DDMP)
  * [MonoRUn: Monocular 3D Object Detection by Self-Supervised Reconstruction and Uncertainty Propagation](https://arxiv.org/abs/2103.12605)<br>:star:[code](https://github.com/tjiiv-cprg/MonoRUn)
  * [M3DSSD: Monocular 3D Single Stage Object Detector](https://arxiv.org/abs/2103.13164)<br>:star:[code](https://github.com/mumianyuxin/M3DSSD)
  * [GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection](https://arxiv.org/abs/2103.17202)<br>:star:[code](https://github.com/abhi1kumar/groomed_nms):tv:[video](https://www.youtube.com/watch?v=PWctKkyWrno)<br>ä½œè€…æå‡ºå¹¶é›†æˆGrooMeD-NMS--ä¸€ç§æ–°é¢–çš„åˆ†ç»„æ•°å­¦å¯åŒºåˆ†çš„NMSï¼Œç”¨äºå•çœ¼3Dç‰©ä½“æ£€æµ‹ï¼Œåœ¨KITTIåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å•çœ¼3Dç‰©ä½“æ£€æµ‹ç»“æœï¼Œè¡¨ç°ä¸åŸºäºå•çœ¼è§†é¢‘çš„æ–¹æ³•ç›¸å½“ã€‚
  * [LiDAR R-CNN: An Efficient and Universal 3D Object Detector](https://arxiv.org/abs/2103.15297)<br>:star:[code](https://github.com/tusimple/LiDAR_RCNN)
  * [Exploring intermediate representation for monocular vehicle pose estimation](https://arxiv.org/abs/2011.08464)<br>:star:[code](https://github.com/Nicholasli1995/EgoNet)
  * [Delving into Localization Errors for Monocular 3D Object Detection](https://arxiv.org/abs/2103.16237)<br>:star:[code](https://github.com/xinzhuma/monodle)
  * [HVPR: Hybrid Voxel-Point Representation for Single-stage 3D Object Detection](https://arxiv.org/abs/2104.00902)
  * [Objects are Different: Flexible Monocular 3D Object Detection](https://arxiv.org/abs/2104.02323)<br>:star:[code](https://github.com/zhangyp15/MonoFlex)
  * [Back-tracing Representative Points for Voting-based 3D Object Detection in Point Clouds](https://arxiv.org/abs/2104.06114)<br>:star:[code](https://github.com/cheng052)
  * [PointAugmenting: Cross-Modal Augmentation for 3D Object Detection](https://vision.sjtu.edu.cn/files/cvpr21_pointaugmenting.pdf)<br>åˆ†äº«ä¼š
  * [SE-SSD: Self-Ensembling Single-Stage Object Detector From Point Cloud](https://arxiv.org/abs/2104.09804)<br>:star:[code](https://github.com/Vegeta2020/SE-SSD)<br>æå‡º Self-Ensembling Single-Stage object Detectorï¼ˆSE-SSDï¼‰ï¼Œç”¨äºåœ¨å®¤å¤–ç‚¹äº‘ä¸­è¿›è¡Œå‡†ç¡®å’Œæœ‰æ•ˆçš„ 3D ç›®æ ‡æ£€æµ‹ã€‚å…³é”®åœ¨äºåˆ©ç”¨ soft å’Œ hard targets ä¸æ‰€åˆ¶å®šçš„çº¦æŸæ¡ä»¶æ¥å…±åŒä¼˜åŒ–æ¨¡å‹ï¼Œè€Œä¸åœ¨æ¨ç†ä¸­å¼•å…¥é¢å¤–çš„è®¡ç®—ã€‚ä¸ä¹‹å‰çš„æ‰€æœ‰ä½œå“ç›¸æ¯”ï¼ŒSE-SSD è¾¾åˆ°äº†é¡¶çº§æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ KITTI åŸºå‡†ä¸­çš„æ±½è½¦æ£€æµ‹ä¸­è·å¾—äº†æœ€é«˜çš„ç²¾åº¦ï¼ˆåˆ†åˆ«åœ¨ BEV å’Œ 3D æ’è¡Œæ¦œä¸Šæ’åç¬¬ä¸€å’Œç¬¬äºŒï¼‰ï¼Œå¹¶å…·æœ‰è¶…é«˜çš„æ¨ç†é€Ÿåº¦ã€‚
* æ—‹è½¬ç›®æ ‡æ£€æµ‹
  * [Dense Label Encoding for Boundary Discontinuity Free Rotation Detection](https://arxiv.org/abs/2011.09670)<br>:star:[code](https://github.com/Thinklab-SJTU/DCL_RetinaNet_Tensorflow)
* ç›®æ ‡å®šä½
  * [Unveiling the Potential of Structure-Preserving for Weakly Supervised Object Localization](https://arxiv.org/abs/2103.04523v1)<br>åŸºäºç»“æ„ä¿¡æ¯ä¿æŒçš„å¼±ç›‘ç£ç›®æ ‡å®šä½<br>è§£è¯»ï¼š[13](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* å¯†é›†ç›®æ ‡æ£€æµ‹
  * [Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection]()<br>:star:[code](https://github.com/implus/GFocalV2)<br>è§£è¯»ï¼š[ç›®æ ‡æ£€æµ‹æ— ç—›æ¶¨ç‚¹ä¹‹ Generalized Focal Loss V2](https://mp.weixin.qq.com/s/H3LuCuqKCUNFldzqiPWQXg)
* æ˜¾è‘—ç›®æ ‡æ£€æµ‹
  * [Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion](https://arxiv.org/abs/2103.11832)<br>:open_mouth:oral
  * [Weakly Supervised Video Salient Object Detection](https://arxiv.org/abs/2104.02391)<br>:star:[code](https://github.com/wangbo-zhao/WSVSOD)
  * [Uncertainty-aware Joint Salient Object and Camouflaged Object Detection](https://arxiv.org/abs/2104.02628)<br>:star:[code](https://github.com/JingZhang617/Joint_COD_SOD)
* åŠç›‘ç£ç›®æ ‡æ£€æµ‹
 * [Data-Uncertainty Guided Multi-Phase Learning for Semi-Supervised Object Detection](https://arxiv.org/abs/2103.16368)
 * [Points as Queries: Weakly Semi-supervised Object Detection by Points]<br>ç²—è§£ï¼š[6](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* å¼±ç›‘ç£ç›®æ ‡æ£€æµ‹
  * [DAP: Detection-Aware Pre-training with Weak Supervision](https://arxiv.org/abs/2103.16651)
* é•¿å°¾ç›®æ ‡æ£€æµ‹ 
  * [Adaptive Class Suppression Loss for Long-Tail Object Detection](https://arxiv.org/abs/2104.00885)<br>:star:[code](https://github.com/CASIA-IVA-Lab/ACSL)
* OODæ£€æµ‹
  * [MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space](https://arxiv.org/abs/2105.01879)<br>:star:[code](https://github.com/deeplearning-wisc/large_scale_ood)
  * [MOOD: Multi-level Out-of-distribution Detection](https://arxiv.org/abs/2104.14726)<br>:star:[code](https://github.com/deeplearning-wisc/MOOD)

<a name="6"/> 

## 6.Data Augmentation(æ•°æ®å¢å¹¿)

- [KeepAugment: A Simple Information-Preserving Data Augmentation](https://arxiv.org/pdf/2011.11778.pdf)<br>

<a name="5"/> 

## 5.Anomaly Detection(å¼‚å¸¸æ£€æµ‹)

- [Multiresolution Knowledge Distillation for Anomaly Detection](https://arxiv.org/abs/2011.11108)<br>:star:[code](https://github.com/Niousha12/Knowledge_Distillation_AD)

<a name="4"/> 

## 4.Weakly Supervised/Semi-Supervised/Self-supervised/Unsupervised Learning(è‡ª/åŠ/å¼±ç›‘ç£å­¦ä¹ )

* å¼±ç›‘ç£
  * [Weakly Supervised Learning of Rigid 3D Scene Flow](https://arxiv.org/pdf/2102.08945.pdf)<br>:open_mouth:oral:star:[code](https://github.com/zgojcic/Rigid3DSceneFlow):house:[project](https://3dsceneflow.github.io/)<br>
  * [Relation-aware Instance Refinement for Weakly Supervised Visual Grounding](https://arxiv.org/abs/2103.12989)<br>:star:[code](https://github.com/youngfly11/ReIR-WeaklyGrounding.pytorch)
* åŠç›‘ç£
  * [Adaptive Consistency Regularization for Semi-Supervised Transfer Learning](https://arxiv.org/abs/2103.02193)<br>:star:[code](https://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning)<br>
  * [SSLayout360: Semi-Supervised Indoor Layout Estimation from 360âˆ˜ Panorama](https://arxiv.org/abs/2103.13696)
* è‡ªç›‘ç£
  * [Self-supervised Geometric Perception](https://arxiv.org/abs/2103.03114)<br>:open_mouth:oral:star:[code](https://github.com/theNded/SGP)<br>ä½œè€…ç§° SGP æ˜¯ç¬¬ä¸€ä¸ªåœ¨å‡ ä½•æ„ŸçŸ¥ä¸­è¿›è¡Œç‰¹å¾å­¦ä¹ çš„é€šç”¨æ¡†æ¶ï¼Œä¸éœ€è¦ä»»ä½•æ¥è‡ª ground-truth å‡ ä½•æ ‡ç­¾çš„ç›‘ç£ã€‚SGPä»¥EMæ–¹å¼è¿è¡Œï¼Œå®ƒè¿­ä»£æ‰§è¡Œå‡ ä½•æ¨¡å‹çš„é²æ£’ä¼°è®¡ä»¥ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶åœ¨å™ªå£°ä¼ªæ ‡ç­¾çš„ç›‘ç£ä¸‹è¿›è¡Œç‰¹å¾å­¦ä¹ ã€‚å°† SGP åº”ç”¨äºç›¸æœºå§¿åŠ¿ä¼°è®¡å’Œç‚¹äº‘é…å‡†ï¼Œå¹¶è¯æ˜åœ¨å¤§è§„æ¨¡çœŸå®æ•°æ®é›†ä¸­ï¼ŒSGP çš„æ€§èƒ½ç­‰åŒäºç”šè‡³ä¼˜äºç›‘ç£çš„æƒå¨ã€‚
  * [Vectorization and Rasterization: Self-Supervised Learning for Sketch and Handwriting](https://arxiv.org/abs/2103.13716)<br>:star:[code](https://github.com/AyanKumarBhunia/Self-Supervised-Learning-for-Sketch)
  * [Self-supervised Motion Learning from Static Images](https://arxiv.org/abs/2104.00240)
  * [SOLD2: Self-supervised Occlusion-aware Line Description and Detection](https://arxiv.org/abs/2104.03362)<br>:open_mouth:oral:star:[code](https://github.com/cvg/SOLD2)
  * [All Labels Are Not Created Equal: Enhancing Semi-supervision via Label Grouping and Co-training](https://arxiv.org/abs/2104.05248)<br>:star:[code](https://github.com/islam-nassar/semco)
  * [Global Transport for Fluid Reconstruction with Learned Self-Supervision](https://arxiv.org/abs/2104.06031)<br>:open_mouth:oral:star:[code](https://github.com/tum-pbs/Global-Flow-Transport)
* æ— ç›‘ç£
  * [A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning](https://arxiv.org/abs/2104.14558)<br>:star:[code](https://github.com/facebookresearch/SlowFast)
* [Unsupervised Visual Representation Learning by Tracking Patches in Video](https://arxiv.org/abs/2105.02545)<br>:star:[code](https://github.com/microsoft/CtP)
* [SMURF: Self-Teaching Multi-Frame Unsupervised RAFT with Full-Image Warping](https://arxiv.org/abs/2105.07014)<br>:star:[code](https://github.com/google-research/google-research/tree/master/smurf)

<a name="3"/> 

## 3.Point Cloud(ç‚¹äº‘)

- [Style-based Point Generator with Adversarial Rendering for Point Cloud Completion](https://arxiv.org/abs/2103.02535)<br>
- [MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan Synchronization](https://arxiv.org/abs/2101.06605)<br>:open_mouth:oral:star:[code](https://github.com/huangjh-pub/multibody-sync)
- [TPCN: Temporal Point Cloud Networks for Motion Forecasting](https://arxiv.org/abs/2103.03067)<br>ç”¨äºè¿åŠ¨é¢„æµ‹çš„æ—¶ç©ºç‚¹äº‘ç½‘ç»œ<br>
- [How Privacy-Preserving are Line Clouds? Recovering Scene Details from 3D Lines](https://arxiv.org/abs/2103.05086)<br>:star:[code](https://github.com/kunalchelani/Line2Point)
- [PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds](https://arxiv.org/abs/2103.14635)<br>:star:[code](https://github.com/CVMI-Lab/PAConv)
- [Point2Skeleton: Learning Skeletal Representations from Point Clouds](https://arxiv.org/abs/2012.00230)<br>:open_mouth:oral:star:[code](https://github.com/clinplayer/Point2Skeleton):house:[project](https://enigma-li.github.io/projects/point2skeleton/point2skeleton.html)
- [FESTA: Flow Estimation via Spatial-Temporal Attention for Scene Point Clouds](https://arxiv.org/abs/2104.00798)
- [RPSRNet: End-to-End Trainable Rigid Point Set Registration Network using Barnes-Hut 2D-Tree Representation](https://arxiv.org/abs/2104.05328) 
* ç‚¹äº‘é…å‡†
  * [PREDATOR: Registration of 3D Point Clouds with Low Overlap](https://arxiv.org/pdf/2011.13005.pdf)<br>:open_mouth:oral:star:[code](https://github.com/ShengyuH/OverlapPredator):house:[project](https://overlappredator.github.io/)<br>
  * [SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration](https://arxiv.org/abs/2011.12149)<br>:star:[code](https://github.com/QingyongHu/SpinNet)
  * [Robust Point Cloud Registration Framework Based on Deep Graph Matching](https://arxiv.org/abs/2103.04256)<br>:star:[code](https://github.com/fukexue/RGM)
  * [PointDSC: Robust Point Cloud Registration using Deep Spatial Consistency](https://arxiv.org/abs/2103.05465)<br>:star:[code](https://github.com/XuyangBai/PointDSC)
  * [ReAgent: Point Cloud Registration using Imitation and Reinforcement Learning](https://arxiv.org/abs/2103.15231)<br>:star:[code](https://github.com/dornik/reagent)
  * [DeepI2P: Image-to-Point Cloud Registration via Deep Classification](https://arxiv.org/abs/2104.03501)<br>:star:[code](https://github.com/lijx10/DeepI2P)
* ç‚¹äº‘è¡¥å…¨
  * [Cycle4Completion: Unpaired Point Cloud Completion using Cycle Transformation with Missing Region Coding](https://arxiv.org/abs/2103.07838)
  * [Denoise and Contrast for Category Agnostic Shape Completion](https://arxiv.org/abs/2103.16671)<br>:star:[code](https://github.com/antoalli/Deco)
  * [Variational Relational Point Completion Network](https://arxiv.org/abs/2104.10154)<br>:open_mouth:oral:star:[code](https://github.com/paul007pl/VRCNet):house:[project](https://paul007pl.github.io/projects/VRCNet.html)
* ç‚¹äº‘å…³é”®ç‚¹æ£€æµ‹
  * [Skeleton Merger: an Unsupervised Aligned Keypoint Detector](https://arxiv.org/abs/2103.10814)<br>:star:[code](https://github.com/eliphatfs/SkeletonMerger)
* 3Dç‚¹äº‘
  * [Diffusion Probabilistic Models for 3D Point Cloud Generation](https://arxiv.org/abs/2103.01458)<br>:open_mouth:oral:star:[code](https://github.com/luost26/diffusion-point-cloud)<br>
  * [PointGuard: Provably Robust 3D Point Cloud Classification](https://arxiv.org/abs/2103.03046)
  * [Equivariant Point Network for 3D Point Cloud Analysis](https://arxiv.org/abs/2103.14147)<br>:star:[code](https://github.com/nintendops/EPN_PointCloud)
* ç‚¹äº‘å‹ç¼©
  * [VoxelContext-Net: An Octree based Framework for Point Cloud Compression](https://arxiv.org/abs/2105.02158)
* ç‚¹äº‘è¯†åˆ«
  * [3D Spatial Recognition without Spatially Labeled 3D](https://arxiv.org/abs/2105.06461)<br>:house:[project](https://facebookresearch.github.io/WyPR/)

<a name="2"/> 

## 2.Graph Neural Networks(å›¾å·ç§¯ç½‘ç»œGNN)

- [Sequential Graph Convolutional Network for Active Learning](https://arxiv.org/pdf/2006.10219.pdf)<br>
- [Quantifying Explainers of Graph Neural Networks in Computational Pathology](https://arxiv.org/abs/2011.12646)<br>
- [Binary Graph Neural Networks](https://arxiv.org/abs/2012.15823)

<a name="1"/> 

## 1.Unkown(æœªåˆ†ç±»)

- Inverting the Inherence of Convolution for Visual Recognition<br>
- Representative Batch Normalization with Feature Calibration<br>
- UC2: Universal Cross-lingual Cross-modal Vision-and-Language Pretraining<br>
- Reconsidering Representation Alignment for Multi-view Clustering<br>
- Self-supervised Simultaneous Multi-Step Prediction of Road Dynamics and Cost Map<br>
- [Instance Localization for Self-supervised Detection Pretraining](https://arxiv.org/pdf/2102.08318.pdf)<br>:star:[code](https://github.com/limbo0000/InstanceLoc)<br>
- Model-Contrastive Federated Learning<br>æå‡ºæ¨¡å‹å¯¹æ¯”å­¦ä¹ æ¥è§£å†³è”åˆå­¦ä¹ ä¸­çš„éIIDæ•°æ®é—®é¢˜<br>
- [Neural Geometric Level of Detail:Real-time Rendering with Implicit 3D Surfaces](https://arxiv.org/abs/2101.10994)<br>:open_mouth:Oral:star:[code](https://github.com/nv-tlabs/nglod):house:[project](https://nv-tlabs.github.io/nglod/)<br>
- [Data-Free Model Extraction](https://arxiv.org/abs/2011.14779)<br>:star:[code](https://github.com/cake-lab/datafree-model-extraction)<br>
- Single-Stage Instance Shadow Detection with Bidirectional Relation Learning<br>:open_mouth:oral:star:[code](https://github.com/stevewongv/SSIS)
- [Continual Adaptation of Visual Representations via Domain Randomization and Meta-learning](https://arxiv.org/abs/2012.04324)<br>:open_mouth:oral
- [PatchmatchNet: Learned Multi-View Patchmatch Stereo](https://arxiv.org/abs/2012.01411)<br>:open_mouth:oral:star:[code](https://github.com/FangjinhuaWang/PatchmatchNet)
- [Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning](https://arxiv.org/abs/2012.11552)<br>:star:[code](https://github.com/valeoai/obow):house:[project](https://valeoai.github.io/blog/publications/obow/)<br>
- [Semantic Palette: Guiding Scene Generation with Class Proportions]
- POSEFusion:Pose-guided Selective Fusion for Single-view Human Volumetric Capture<br>:open_mouth:oral
- [Multi-Objective Interpolation Training for Robustness to Label Noise](https://arxiv.org/abs/2012.04462)<br>:star:[code](https://github.com/DiegoOrtego/LabelNoiseMOIT)
- [Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations](https://arxiv.org/abs/2011.12854)<br>:star:[code](https://github.com/ml-research/NeSyXIL)
- Simpler Certified Radius Maximization by Propagating Covariances<br>:open_mouth:oral:tv:[video](https://www.youtube.com/watch?v=1V9sBzlfuwY)
- [Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food](https://arxiv.org/abs/2103.03375)
- [Discovering Hidden Physics Behind Transport Dynamics](https://arxiv.org/abs/2011.12222)<br>:open_mouth:oral
- [Soft-IntroVAE: Analyzing and Improving the Introspective Variational Autoencoder](https://arxiv.org/abs/2012.13253)<br>:open_mouth:oral:star:[code](https://github.com/taldatech/soft-intro-vae-pytorch):house:[project](https://taldatech.github.io/soft-intro-vae-web/)
- [Deep Gradient Projection Networks for Pan-sharpening](https://arxiv.org/abs/2103.04584)<br>:star:[code](https://github.com/xsxjtu/GPPNN)
- [Consensus Maximisation Using Influences of Monotone Boolean Functions](https://arxiv.org/abs/2103.04200)<br>:open_mouth:oral
* Forecasting Irreversible Disease via Progression Learning
* Causal Hidden Markov Model for Time Series Disease Forecasting
* Towards Unified Surgical Skill Assessment
- [Knowledge Evolution in Neural Networks](https://arxiv.org/abs/2103.05152)<br>:open_mouth:oral:star:[code](https://github.com/ahmdtaha/knowledge_evolution)
* RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words<br>RSTNet: åŸºäºå¯åŒºåˆ†è§†è§‰è¯å’Œéè§†è§‰è¯çš„è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶çš„å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹<br>è§£è¯»ï¼š[14](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* [Removing the Background by Adding the Background: Towards a Background Robust Self-supervised Video Representation Learning](https://arxiv.org/abs/2009.05769)<br>é€šè¿‡æ·»åŠ èƒŒæ™¯æ¥å»é™¤èƒŒæ™¯å½±å“ï¼šèƒŒæ™¯é²æ£’çš„è‡ªç›‘ç£è§†é¢‘è¡¨å¾å­¦ä¹ <br>è§£è¯»ï¼š[11](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* Representative Batch Normalization with Feature Calibration<br>:open_mouth:oral<br>[ä½œè€…ä¸»é¡µ](https://duoli.org/)<br>åŸºäºç‰¹å¾æ ¡å‡†çš„è¡¨å¾æ‰¹è§„èŒƒåŒ–æ–¹æ³•è§£è¯»ï¼š[4](https://mp.weixin.qq.com/s/yNDkHMhOIb76b4KcEhx4XQ)
* [Learning Compositional Representation for 4D Captures with Neural ODE](https://arxiv.org/abs/2103.08271)
* [Involution: Inverting the Inherence of Convolution for Visual Recognition](https://arxiv.org/abs/2103.06255)<br>:star:[code](https://github.com/d-li14/involution)<br>è§£è¯»ï¼š[CVPR'21 | involutionï¼šè¶…è¶Šconvolutionå’Œself-attentionçš„ç¥ç»ç½‘ç»œæ–°ç®—å­](https://mp.weixin.qq.com/s/Kn7QJdldLhyBfYS1KiCdcA)
* [Spatially Consistent Representation Learning](https://arxiv.org/abs/2103.06122)
* [Limitations of Post-Hoc Feature Alignment for Robustness](https://arxiv.org/abs/2103.05898)
* [AutoDO: Robust AutoAugment for Biased Data with Label Noise via Scalable Probabilistic Implicit Differentiation](https://arxiv.org/abs/2103.05863)<br>:star:[code](https://github.com/gudovskiy/autodo)
* [Augmentation Strategies for Learning with Noisy Labels](https://arxiv.org/abs/2103.02130)<br>:star:[code](https://github.com/KentoNishi/Augmentation-for-LNL)
* CFNet: Cascade and Fused Cost Volume for Robust Stereo Matching<br>:star:[code](https://github.com/gallenszl/CFNet)
* [Augmentation Strategies for Learning with Noisy Labels](https://arxiv.org/abs/2103.02130)<br>:star:[code](https://github.com/KentoNishi/Augmentation-for-LNL)
* [PGT: A Progressive Method for Training Models on Long Videos](https://arxiv.org/abs/2103.11313)<br>:open_mouth:oral:star:[code](https://github.com/BoPang1996/PGT)
* [Generic Perceptual Loss for Modeling Structured Output Dependencies](https://arxiv.org/abs/2103.10571)
* [Masksembles for Uncertainty Estimation](https://arxiv.org/abs/2012.08334)<br>:star:[code](https://github.com/nikitadurasov/masksembles):house:[project](https://nikitadurasov.github.io/projects/masksembles/)
* [Student-Teacher Learning from Clean Inputs to Noisy Inputs](https://arxiv.org/abs/2103.07600)
* [Scene-Intuitive Agent for Remote Embodied Visual Grounding](https://arxiv.org/abs/2103.12944)
* [Meta-Mining Discriminative Samples for Kinship Verification](https://arxiv.org/abs/2103.15108)<br>
* [Learning Probabilistic Ordinal Embeddings for Uncertainty-Aware Regression](https://arxiv.org/abs/2103.13629)<br>:star:[code](https://github.com/Li-Wanhua/POEs):tv:[video](https://www.youtube.com/watch?v=zCTPRxxlZsI&t=427s)<br>è®ºæ–‡å…¬å¼€
* [Diverse Branch Block: Building a Convolution as an Inception-like Unit](https://arxiv.org/abs/2103.13425)<br>:star:[code](https://github.com/DingXiaoH/DiverseBranchBlock)
* [OTCE: A Transferability Metric for Cross-Domain Cross-Task Representations](https://arxiv.org/abs/2103.13843)
* [Disentangled Cycle Consistency for Highly-realistic Virtual Try-On](https://arxiv.org/abs/2103.09479)<br>:star:[code](https://github.com/ChongjianGE/DCTON)
* [Stylized Neural Painting](https://arxiv.org/abs/2011.08114)<br>:star:[code](https://github.com/jiupinjia/stylized-neural-painting):house:[project](https://jiupinjia.github.io/neuralpainter/):tv:[video](https://youtu.be/oerb-nwrXhk)<br>é£æ ¼åŒ–çš„ç¥ç»ç»˜ç”»,Stylized Neural Painting,æå‡º image-to-painting ç¿»è¯‘æ–¹æ³•ï¼Œç”Ÿæˆç”ŸåŠ¨é€¼çœŸã€é£æ ¼å¯æ§çš„ç»˜ç”»è‰ºæœ¯ä½œå“ 
* [Confluent Vessel Trees with Accurate Bifurcations](https://arxiv.org/abs/2103.14268)<br>:star:[code](https://vision.cs.uwaterloo.ca/code/)
* [Repopulating Street Scenes](https://arxiv.org/abs/2103.16183)
* Extreme Rotation Estimation using Dense Correlation Volumes
* Can We Characterize Tasks Without Labels or Features?
* [Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding](https://arxiv.org/abs/2103.16848)
* [Online Learning of a Probabilistic and Adaptive Scene Representation](https://arxiv.org/abs/2103.16832)
* [Generative Modelling of BRDF Textures from Flash Images](https://arxiv.org/abs/2102.11861)<br>:star:[code](https://github.com/henzler/neuralmaterial):house:[project](https://henzler.github.io/publication/neuralmaterial/)
* [PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting](https://arxiv.org/abs/2104.00674)<br>:house:[project](https://kai-46.github.io/PhySG-website/)<br>ä½œè€…å‘æ˜çš„é€†å‘æ¸²æŸ“ç®—æ³•PhySGï¼Œå¯ä»¥ä»ä¸€ç»„RGBè¾“å…¥å›¾åƒä¸­é‡å»ºç‰©ä½“å‡ ä½•å›¾å½¢ã€æè´¨å’Œå…‰ç…§ï¼Œå…¨ç¨‹ç«¯åˆ°ç«¯è¿è¡Œã€‚
* [Self-supervised Video Representation Learning by Context and Motion Decoupling](https://arxiv.org/abs/2104.00862)
* [Dynamic Region-Aware Convolution](https://arxiv.org/abs/2003.12243)<br>ç²—è§£ï¼š[14](https://mp.weixin.qq.com/s/lL1cz_L523TSdYJFfHA2lQ)
* [Meta Pseudo Labels](https://arxiv.org/pdf/2003.10580.pdf)<br>:star:[code](https://github.com/google-research/google-research/tree/master/meta_pseudo_labels):tv:[video](https://www.youtube.com/watch?v=yhItocvAaq0)
* [PQA: Perceptual Question Answering](https://arxiv.org/abs/2104.03589)
* [CondenseNet V2: Sparse Feature Reactivation for Deep Networks](https://arxiv.org/abs/2104.04382)<br>:star:[code](https://github.com/jianghaojun/CondenseNetV2) 
* [CFNet: Cascade and Fused Cost Volume for Robust Stereo Matching](https://arxiv.org/abs/2104.04314)<br>:star:[code](https://github.com/gallenszl/CFNet)
* [Neural Camera Simulators](https://arxiv.org/abs/2104.05237)
* [Simpler Certified Radius Maximization by Propagating Covariances](https://arxiv.org/abs/2104.05888)<br>:open_mouth:oral:star:[code](https://github.com/zhenxingjian/Propagating_Covariance):tv:[video](https://www.youtube.com/watch?v=m1ya2oNf5iE)
* [Lighting, Reflectance and Geometry Estimation from 360âˆ˜ Panoramic Stereo](https://arxiv.org/abs/2104.09886)<br>:star:[code](https://github.com/junxuan-li/LRG_360Panoramic)
* [MetricOpt: Learning to Optimize Black-Box Evaluation Metrics](https://arxiv.org/abs/2104.10631)<br>:open_mouth:oral
* [Deep Stable Learning for Out-Of-Distribution](http://pengcui.thumedialab.com/papers/DeepStableLearning.pdf)<br>åˆ†äº«ä¼š
* [Learning a Self-Expressive Network for Subspace Clustering](http://www.pris.net.cn/wp-content/uploads/2021/04/SENet-CVPR-2021.pdf)<br>åˆ†äº«ä¼š
* [Heterogeneous Grid Convolution for Adaptive, Efficient, and Controllable Computation](https://arxiv.org/abs/2104.11176)
* [Extreme Rotation Estimation using Dense Correlation Volumes](https://arxiv.org/abs/2104.13530)<br>:house:[project](https://ruojincai.github.io/ExtremeRotation/)
* [Decoupled Dynamic Filter Networks](https://arxiv.org/abs/2104.14107)<br>:house:[project](https://thefoxofsky.github.io/project_pages/ddf):tv:[video](https://youtu.be/QecJD5HUF7U)
* [MongeNet: Efficient Sampler for Geometric Deep Learning](https://arxiv.org/abs/2104.14554)<br>:star:[code](https://github.com/lebrat/MongeNet):house:[project](https://lebrat.github.io/MongeNet/):tv:[video](https://youtu.be/RfmZBbSEiz4)
* [Multi-Perspective LSTM for Joint Visual Representation Learning](https://arxiv.org/abs/2105.02802)<br>:star:[code](https://github.com/arsm/MPLSTM)
* [Quantum Permutation Synchronization](https://arxiv.org/pdf/2101.07755.pdf)
* [A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts](https://arxiv.org/abs/2105.00290)
* [DriveGAN: Towards a Controllable High-Quality Neural Simulation](https://arxiv.org/abs/2104.15060)<br>:open_mouth:oral
* [Faster Meta Update Strategy for Noise-Robust Deep Learning](https://arxiv.org/abs/2104.15092)<br>:star:[code](https://github.com/youjiangxu/FaMUS)
* [NeRD: Neural 3D Reflection Symmetry Detector](https://arxiv.org/abs/2105.03211)<br>:star:[code](https://github.com/zhou13/nerd)


<a name="*"/>

## Workshop å¾ç¨¿ing

 * [Transport Challenge with ThreeDWorld](http://tdw-transport.csail.mit.edu/)<br>:tv:[video](https://youtu.be/WRx41xsB_wo):warning:6æœˆ1æˆªæ­¢
   * è®ºæ–‡ï¼š[The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion Planning Benchmark for Physically Realistic Embodied AI](https://arxiv.org/abs/2103.14025)

- [Visual Perception for Navigation in Human Environments](https://jrdb.stanford.edu/workshops/jrdb-cvpr21)<br>ç¬¬äºŒå±Šäººç±»ç¯å¢ƒå¯¼èˆªè§†è§‰æ„ŸçŸ¥å¾ç¨¿ :warning:4æœˆ15æˆªæ­¢
- [UG 2 + Challenge](http://cvpr2021.ug2challenge.org/index.html)<br>æ—¨åœ¨é€šè¿‡åº”ç”¨å›¾åƒæ¢å¤å’Œå¢å¼ºç®—æ³•æé«˜åˆ†ææ€§èƒ½ï¼Œæ¨åŠ¨å¯¹ "difficult"å›¾åƒçš„åˆ†æã€‚å‚ä¸è€…ä»»åŠ¡æ˜¯å¼€å‘æ–°çš„ç®—æ³•ï¼Œä»¥æ”¹è¿›å¯¹åœ¨é—®é¢˜æ¡ä»¶ä¸‹æ‹æ‘„çš„å›¾åƒåˆ†æã€‚<br>:crown:10Kç¾å…ƒå¥–é‡‘<br>
   * [ä½èƒ½è§åº¦ç¯å¢ƒä¸‹çš„ç›®æ ‡æ£€æµ‹](https://www.deepl.com/translator#en/zh/OBJECT%20DETECTION%20IN%20POOR%20VISIBILITY%20ENVIRONMENTS)
      * é›¾éœ¾æ¡ä»¶ä¸‹çš„(åŠ)ç›‘ç£ç›®æ ‡æ£€æµ‹
      * (åŠ)ä½å…‰æ¡ä»¶ä¸‹çš„äººè„¸æ£€æµ‹
   * [é»‘æš—è§†é¢‘ä¸­çš„åŠ¨ä½œè¯†åˆ«](http://cvpr2021.ug2challenge.org/track2.html)
      * é»‘æš—ä¸­è¿›è¡Œå®Œå…¨ç›‘ç£åŠ¨ä½œè¯†åˆ«
      * é»‘æš—ä¸­è¿›è¡ŒåŠç›‘ç£åŠ¨ä½œè¯†åˆ«

- [Continual Learning in Computer Vision å¾ç¨¿ä¸­](https://sites.google.com/view/clvision2021/overview?authuser=0)<br>æ—¨åœ¨èšé›†å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆï¼Œè®¨è®ºæŒç»­å­¦ä¹ çš„æœ€æ–°è¿›å±•ã€‚<br>
  * Best paper award: 					500 USD + 500 USD worth of Huawei cloud credits (HUAWEI)
  * Overall Challenge winner: 				1,000 USD  + 500 USD worth of Huawei cloud credits (HUAWEI)
  * Supervised-Learning track winner: 		500 USD (HUAWEI)
  * Reinforcement-Learning track winner: 	500 USD (ServiceNow)

- [ç¬¬å››å±ŠUG2ç ”è®¨ä¼šå’Œç«èµ›ï¼šå¼¥åˆè®¡ç®—æˆåƒä¸è§†è§‰è¯†åˆ«ä¹‹é—´çš„é¸¿æ²Ÿ](https://mp.weixin.qq.com/s/u7YDi7tAsDn77P0hb-dmFg)

- [10ä¸‡ç¾å…ƒå¥–é‡‘ï¼CVPR 2021 é‡ç£…èµ›äº‹ï¼Œå®‰å…¨AIæŒ‘æˆ˜è€…è®¡åˆ’](https://mp.weixin.qq.com/s/1d8RL9_YqKp9stVf2M1N-Q)
  * [CVPR 2021å¤§èµ›ï¼Œ å®‰å…¨AI ä¹‹é˜²å¾¡æ¨¡å‹çš„ã€Œç™½ç›’å¯¹æŠ—æ”»å‡»ã€è§£æ](https://mp.weixin.qq.com/s/OJ9_xaQ6GyuCUrph8boxYA)
  * [è¿˜åœ¨åˆ·æ¦œImageNetï¼Ÿæ‰¾å‡ºæ¨¡å‹çš„è„†å¼±ä¹‹å¤„æ›´æœ‰ä»·å€¼ï¼](https://mp.weixin.qq.com/s/nbAudiGJX_l69zaSEiWJFA)

- [Responsible Computer Vision](https://sites.google.com/view/rcv-cvpr2021/home)<br>:warning:3æœˆ25æ—¥æˆªæ­¢<br>æœ¬æ¬¡ç ”è®¨ä¼šå°†å¹¿æ³›è®¨è®ºè®¡ç®—æœºè§†è§‰èƒŒæ™¯ä¸‹è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½çš„ä¸‰ä¸ªä¸»è¦æ–¹é¢ï¼šå…¬å¹³æ€§ï¼›å¯è§£é‡Šæ€§å’Œé€æ˜åº¦ï¼›ä»¥åŠéšç§ã€‚
- [Holistic Video Understanding](https://holistic-video-understanding.github.io/workshops/cvpr2021.html)<br>ç›®çš„æ˜¯å»ºç«‹ä¸€ä¸ªæ•´åˆæ‰€æœ‰è¯­ä¹‰æ¦‚å¿µè”åˆè¯†åˆ«çš„è§†é¢‘åŸºå‡†ï¼Œå› ä¸ºæ¯ä¸ªä»»åŠ¡çš„å•ä¸€ç±»æ ‡ç­¾å¾€å¾€ä¸è¶³ä»¥æè¿°è§†é¢‘çš„æ•´ä½“å†…å®¹ã€‚
- [ThreeDWorld Transport Challenge](http://tdw-transport.csail.mit.edu/)<br>:warning:6æœˆ1æˆªæ­¢<br>:tv:[video](https://www.youtube.com/watch?v=WcJTpiRC4Zo)
- [FGVC 8](https://sites.google.com/view/fgvc8)<br>ç¬¬å…«å±Šç»†ç²’åº¦è§†è§‰åˆ†ç±»ç ”è®¨ä¼šï¼ˆFGVC8ï¼‰å°†é€šè¿‡ç»†ç²’åº¦è§†è§‰ç†è§£çš„è§†è§’ï¼Œæ¢è®¨ç»†ç²’åº¦å­¦ä¹ ã€è‡ªç›‘ç£å­¦ä¹ ã€åŠç›‘ç£å­¦ä¹ ã€matching(åŒ¹é…)ã€localization(å®šä½)ã€åŸŸé€‚åº”ã€è¿ç§»å­¦ä¹ ã€å°æ ·æœ¬å­¦ä¹ ã€æœºå™¨æ•™å­¦ã€å¤šæ¨¡æ€å­¦ä¹ ï¼ˆå¦‚éŸ³é¢‘å’Œè§†é¢‘ï¼‰ã€ä¼—åŒ…å’Œåˆ†ç±»å­¦é¢„æµ‹ç­‰ç›¸å…³è¯é¢˜ã€‚<br>:warning:[è®ºæ–‡æˆªç¨¿æ—¥æœŸ](https://sites.google.com/view/fgvc8/submission)ä¸º4æœˆ2æ—¥<br>[å¾ç¨¿ä¸»é¢˜åŒ…å«ä»¥ä¸‹å‡ ä¸ªæ–¹é¢](https://sites.google.com/view/fgvc8/submission)
  * Fine-grained categorizationç»†ç²’åº¦åˆ†ç±»
    * Novel datasets and data collection strategies for fine-grained categorizationç”¨äºç»†ç²’åº¦åˆ†ç±»çš„æ–°å‹æ•°æ®é›†å’Œæ•°æ®æ”¶é›†ç­–ç•¥
    * Appropriate error metrics for fine-grained categorizationç»†ç²’åº¦åˆ†ç±»çš„é€‚å½“è¯¯å·®æŒ‡æ ‡
    * Low/few shot learningå°‘/å°æ ·æœ¬å­¦ä¹ 
    * Self-supervised learningè‡ªç›‘ç£å­¦ä¹ 
    * Semi-supervised learningåŠç›‘ç£å­¦ä¹ 
    * Transfer-learning from known to novel subcategories
    * Attribute and part based approaches
    * Taxonomic predictions
    * Addressing long-tailed distributions
  * Human-in-the-loop
    * Fine-grained categorization with humans in the loop
    * Embedding human expertsâ€™ knowledge into computational models
    * Machine teaching
    * Interpretable fine-grained models
  * Multi-modal learning
    * Using audio and video data
    * Using geographical priors
    * Learning shape
  * Fine-grained applications
    * Product recognition 
    * Animal biometrics and camera traps 
    * Museum collections
    * Agricultural 
    * Medical 
    * Fashion
  * ç›¸å…³æŒ‘æˆ˜èµ›å¦‚ä¸‹ï¼ˆéƒ¨åˆ†å·²åœ¨Kaggleç½‘ç«™å¼€å§‹ï¼‰
    * [GeoLifeCLEF2021](https://www.kaggle.com/c/geolifeclef-2021)<br>åˆ©ç”¨è§‚æµ‹ç»“æœä¸èˆªç©ºå›¾åƒå’Œç¯å¢ƒç‰¹å¾é…å¯¹ï¼Œé¢„æµ‹ç‰©ç§çš„å­˜åœ¨
    * [Semi-iNat2021](https://www.kaggle.com/c/semi-inat-2021)<br>ç”±iNaturalistçš„æ•°æ®ç»„æˆçš„åŠç›‘ç£ç»†ç²’åº¦å›¾åƒåˆ†ç±»
    * [iNatChallenge2021](https://www.kaggle.com/c/inaturalist-2021)<br>å¯¹1ä¸‡ç±»åŠ¨æ¤ç‰©è¿›è¡Œå›¾åƒåˆ†ç±»æŒ‘æˆ˜èµ›
    * [iMet2021](https://www.kaggle.com/c/imet-2021-fgvc8)<br>å¯¹è‰ºæœ¯å“è¿›è¡Œç»†ç²’åº¦å±æ€§åˆ†ç±»
    * [iMat-Fashion2021](æœªå¼€å§‹)æœªå¼€å§‹<br>æœè£…å®ä¾‹åˆ†å‰²å’Œç»†ç²’åº¦å±æ€§åˆ†ç±»
    * [Hotel-ID 2021](https://www.kaggle.com/c/hotel-id-2021-fgvc8)<br>ä»å›¾åƒä¸­è¯†åˆ«é…’åº—æˆ¿é—´
    * [HerbariumChallenge2021](https://www.kaggle.com/c/herbarium-2021-fgvc8)<br>ä»æ•°æ®é›†ä¸­è¯†åˆ«æ ‡æœ¬ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªç¾æ´²ã€å¤§æ´‹æ´²å’Œå¤ªå¹³æ´‹åœ°åŒºçš„è¿‘66,000ç§ vascular plant speciesï¼ˆç»´ç®¡æŸæ¤ç‰©ï¼‰çš„ 2.5M å›¾åƒ
    * [iWildCam2021](https://www.kaggle.com/c/iwildcam2021-fgvc8)<br>å¯¹å›¾åƒåºåˆ—ä¸­æ¯ä¸ªç‰©ç§çš„åŠ¨ç‰©æ•°é‡è®¡æ•°
    * [PlantPathologyChallenge2021](æœªå¼€å§‹)æœªå¼€å§‹<br>å¯¹ç—…å®³æ¤ç‰©çš„å›¾åƒè¿›è¡Œåˆ†ç±»
    


## æ‰«ç CVå›å¾®ä¿¡ï¼ˆæ³¨æ˜ï¼šCVPRï¼‰å…¥å¾®ä¿¡äº¤æµç¾¤ï¼š

![image](https://user-images.githubusercontent.com/62801906/109789529-655e4380-7c4b-11eb-9f1a-58c5cb097428.png)
